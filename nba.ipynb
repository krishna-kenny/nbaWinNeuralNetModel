{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfpZ5fAwRxCQ4pObnnZsov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "ac722673-f350-4f24-ac3c-7206ad87eea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nba_api in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_info_common():\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_ids_dict = {team['full_name']: team['id'] for team in nba_teams}\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    for team_name, team_id in team_ids_dict.items():\n",
        "        team_info = TeamInfoCommon(team_id=team_id)\n",
        "        df_team = team_info.get_data_frames()[0]\n",
        "        df_team['TeamName'] = team_name\n",
        "        df_team['Season'] = '2023-24'\n",
        "        df = pd.concat([df, df_team], ignore_index=True)\n",
        "    return df\n",
        "\n",
        "def get_common_all_players():\n",
        "    common_all_players = CommonAllPlayers(\n",
        "        is_only_current_season=1,\n",
        "        league_id='00',\n",
        "        season='2023-24'\n",
        "    )\n",
        "    return common_all_players.get_data_frames()[0]\n",
        "\n",
        "def get_common_team_roster(team_id, season):\n",
        "    common_team_roster = CommonTeamRoster(\n",
        "        team_id=team_id,\n",
        "        league_id_nullable='00',\n",
        "        season=season\n",
        "    )\n",
        "    return common_team_roster.get_data_frames()[0]\n",
        "\n",
        "def get_league_leaders(season, per_mode48='PerGame'):\n",
        "    league_leaders = LeagueLeaders(\n",
        "        league_id='00',\n",
        "        season=season,\n",
        "        per_mode48=per_mode48\n",
        "    )\n",
        "    return league_leaders.get_data_frames()[0]\n",
        "\n",
        "def get_player_game_logs(season, season_type='Regular Season'):\n",
        "    player_game_logs = PlayerGameLogs(\n",
        "        season_nullable=season,\n",
        "        season_type_nullable=season_type\n",
        "    )\n",
        "    return player_game_logs.get_data_frames()[0]\n",
        "\n",
        "def get_team_game_logs(season, season_type='Regular Season'):\n",
        "    team_game_logs = TeamGameLogs(\n",
        "        league_id_nullable='00',\n",
        "        team_id_nullable=None,\n",
        "        season_nullable=season,\n",
        "        season_type_nullable=season_type\n",
        "    )\n",
        "    return team_game_logs.get_data_frames()[0]\n",
        "\n",
        "def get_play_by_play(game_id):\n",
        "    play_by_play = PlayByPlayV2(\n",
        "        game_id=game_id\n",
        "    )\n",
        "    return play_by_play.get_data_frames()[0]\n",
        "\n",
        "def get_shot_chart_detail(season, season_type='Regular Season'):\n",
        "    shot_chart = shotchartdetail.ShotChartDetail(\n",
        "        team_id=0,\n",
        "        player_id=0,\n",
        "        context_measure_simple='FGA',\n",
        "        season_nullable=season,\n",
        "        season_type_all_star=season_type\n",
        "    )\n",
        "    return shot_chart.get_data_frames()[0]"
      ],
      "metadata": {
        "id": "eGYKlU0d3u1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Preprocess TeamInfoCommon Table\n",
        "def preprocess_team_info_common(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# 2. Preprocess CommonAllPlayers Table\n",
        "def preprocess_common_all_players(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# 3. Preprocess CommonTeamRoster Table\n",
        "def preprocess_common_team_roster(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# 4. Preprocess LeagueLeaders Table\n",
        "def preprocess_league_leaders(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# 5. Preprocess PlayerGameLogs Table\n",
        "def preprocess_player_game_logs(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# 6. Preprocess TeamGameLogs Table\n",
        "def preprocess_team_game_logs(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# 7. Preprocess PlayByPlayV2 Table\n",
        "def preprocess_play_by_play_v2(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# 8. Preprocess ShotChartDetail Table\n",
        "def preprocess_shot_chart_detail(df):\n",
        "    df = df.dropna(axis=1)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "jynCKuF646Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODhh0g7xGfMv",
        "outputId": "430833a7-319e-42f3-81dc-6d9285b10cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Team information data stored.\n",
            "Fetching game logs...\n",
            "Game logs data stored.\n",
            "Fetching player game logs...\n",
            "Player game logs data stored.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "# Maximum number of retries for each API call\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "\n",
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt  # Exponential backoff\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_team_info(seasons):\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        for team in nba_teams:\n",
        "            team_info = fetch_with_retries(\n",
        "                TeamInfoCommon,\n",
        "                team_id=team[\"id\"],\n",
        "                season_type_nullable=\"Regular Season\",\n",
        "                timeout=60,\n",
        "            )\n",
        "            if team_info:\n",
        "                df_team = team_info.get_data_frames()[0]\n",
        "                team_data.append(df_team)\n",
        "                time.sleep(0.6)  # Delay to avoid API rate limits\n",
        "            else:\n",
        "                print(\n",
        "                    f\"Skipping team {team['full_name']} for season {season} after failed attempts.\"\n",
        "                )\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "\n",
        "def get_game_logs(seasons):\n",
        "    print(\"Fetching game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            game_log_data.append(df_game_logs)\n",
        "            time.sleep(0.6)\n",
        "        else:\n",
        "            print(f\"Skipping game logs for season {season} after failed attempts.\")\n",
        "\n",
        "    if game_log_data:\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "        df_all_game_logs.to_csv(\"nba_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "\n",
        "def get_player_game_logs(seasons):\n",
        "    print(\"Fetching player game logs...\")\n",
        "    player_game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        player_game_logs = fetch_with_retries(\n",
        "            PlayerGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if player_game_logs:\n",
        "            df_player_game_logs = player_game_logs.get_data_frames()[0]\n",
        "            player_game_log_data.append(df_player_game_logs)\n",
        "            time.sleep(0.6)\n",
        "        else:\n",
        "            print(\n",
        "                f\"Skipping player game logs for season {season} after failed attempts.\"\n",
        "            )\n",
        "\n",
        "    if player_game_log_data:\n",
        "        df_all_player_game_logs = pd.concat(player_game_log_data, ignore_index=True)\n",
        "        df_all_player_game_logs.to_csv(\"nba_player_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player game log data fetched.\")\n",
        "\n",
        "\n",
        "# Define the list of seasons\n",
        "seasons = [\"2023-24\", \"2024-25\"]\n",
        "\n",
        "# Run functions to save data to CSV files\n",
        "get_team_info(seasons[-1])\n",
        "print(\"Team information data stored.\")\n",
        "\n",
        "get_game_logs(seasons)\n",
        "print(\"Game logs data stored.\")\n",
        "\n",
        "get_player_game_logs(seasons[-2:])  # Fetching for the most recent two seasons only\n",
        "print(\"Player game logs data stored.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to preprocess team data\n",
        "def preprocess_team_data(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save team data by removing unnecessary columns.\"\"\"\n",
        "    team_data = pd.read_csv(input_file).dropna()\n",
        "    columns_to_drop = [\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_CITY\",\n",
        "        \"SEASON_YEAR\",\n",
        "        \"TEAM_CODE\",\n",
        "        \"TEAM_DIVISION\",\n",
        "        \"MIN_YEAR\",\n",
        "        \"MAX_YEAR\",\n",
        "    ]\n",
        "    team_data = team_data.drop(columns=columns_to_drop)\n",
        "    team_data = team_data.drop_duplicates()\n",
        "    team_data.to_csv(output_file, index=False)\n",
        "    print(f\"Team data preprocessed and saved. rows: {team_data.shape}\")\n",
        "\n",
        "\n",
        "# Function to preprocess game logs\n",
        "def preprocess_game_logs(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save game log data with specific transformations.\"\"\"\n",
        "    game_logs = pd.read_csv(input_file).dropna()\n",
        "\n",
        "    # Convert 'GAME_DATE' to datetime and sort by date\n",
        "    game_logs[\"GAME_DATE\"] = pd.to_datetime(game_logs[\"GAME_DATE\"])\n",
        "    game_logs_sorted = game_logs.sort_values(by=\"GAME_DATE\", ascending=True)\n",
        "\n",
        "    # Extract 'TEAM1' and 'TEAM2' from 'MATCHUP' column\n",
        "    game_logs_sorted[\"TEAM1\"] = game_logs_sorted[\"MATCHUP\"].str.split().str[0]\n",
        "    game_logs_sorted[\"TEAM2\"] = game_logs_sorted[\"MATCHUP\"].str.split().str[2]\n",
        "\n",
        "    # Drop columns that are not required\n",
        "    columns_to_drop = [\n",
        "        \"MATCHUP\",\n",
        "        \"AVAILABLE_FLAG\",\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_ABBREVIATION\",\n",
        "        \"GAME_ID\",\n",
        "    ]\n",
        "    game_logs_cleaned = game_logs_sorted.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Convert 'WL' to binary format: 'W' becomes 1, 'L' becomes 0\n",
        "    game_logs_cleaned[\"WL\"] = game_logs_cleaned[\"WL\"].apply(\n",
        "        lambda result: 1 if result == \"W\" else 0\n",
        "    )\n",
        "\n",
        "    # Convert 'SEASON_YEAR' to integer format, using only the starting year\n",
        "    game_logs_cleaned[\"SEASON_YEAR\"] = game_logs_cleaned[\"SEASON_YEAR\"].apply(\n",
        "        lambda year: int(year[:4])\n",
        "    )\n",
        "\n",
        "    game_logs_cleaned.to_csv(output_file, index=False)\n",
        "    print(f\"Game logs preprocessed and saved. rows: {game_logs_cleaned.shape}\")\n",
        "\n",
        "\n",
        "# Function to preprocess player game logs\n",
        "def preprocess_player_game_logs(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save player game log data with specific transformations.\"\"\"\n",
        "    player_game_logs = pd.read_csv(input_file).dropna()\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = [\n",
        "        \"PLAYER_NAME\",\n",
        "        \"NICKNAME\",\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_ABBREVIATION\",\n",
        "        \"MATCHUP\",\n",
        "        \"GAME_ID\",\n",
        "        \"MIN_SEC\",\n",
        "    ]\n",
        "    player_game_logs = player_game_logs.drop(columns=columns_to_drop)\n",
        "    player_game_logs[\"WL\"] = player_game_logs[\"WL\"].apply(\n",
        "        lambda x: 1 if x == \"W\" else 0\n",
        "    )\n",
        "    player_game_logs[\"SEASON_YEAR\"] = player_game_logs[\"SEASON_YEAR\"].apply(\n",
        "        lambda x: x[:4]\n",
        "    )\n",
        "\n",
        "    player_game_logs.to_csv(output_file, index=False)\n",
        "    print(f\"Player game logs preprocessed and saved. rows: {player_game_logs.shape}\")\n",
        "\n",
        "\n",
        "# Function to compute weighted averages\n",
        "def compute_weighted_avg(player_id, df):\n",
        "    \"\"\"Compute weighted averages for a given player.\"\"\"\n",
        "    player_rows = df[df[\"PLAYER_ID\"] == player_id].copy()\n",
        "\n",
        "    # Retain TEAM_ID\n",
        "    team_id = player_rows[\"TEAM_ID\"].iloc[0]\n",
        "\n",
        "    # Convert GAME_DATE to a timestamp for weighting\n",
        "    player_rows[\"GAME_TIMESTAMP\"] = pd.to_datetime(player_rows[\"GAME_DATE\"]).apply(\n",
        "        lambda x: x.timestamp()\n",
        "    )\n",
        "    max_timestamp = player_rows[\"GAME_TIMESTAMP\"].max()\n",
        "\n",
        "    # Calculate weights based on recency\n",
        "    player_rows[\"WEIGHTS\"] = np.exp(\n",
        "        (player_rows[\"GAME_TIMESTAMP\"] - max_timestamp) / 1e7\n",
        "    )\n",
        "    player_rows[\"WEIGHTS\"] /= player_rows[\"WEIGHTS\"].sum()\n",
        "\n",
        "    # Compute weighted average for all columns after WL\n",
        "    weighted_avg = (\n",
        "        player_rows.iloc[:, df.columns.get_loc(\"WL\") + 1 :]  # Select columns after WL\n",
        "        .mul(player_rows[\"WEIGHTS\"], axis=0)  # Multiply each column by weights\n",
        "        .sum()  # Sum the weighted values for each column\n",
        "    )\n",
        "    weighted_avg[\"TEAM_ID\"] = team_id  # Include TEAM_ID in the output\n",
        "    return weighted_avg\n",
        "\n",
        "\n",
        "# Function to create feature data\n",
        "def create_feature_data(input_csv, output_csv):\n",
        "    \"\"\"Generate feature data where each player is represented by a single row.\"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_csv)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure GAME_DATE is parsed correctly\n",
        "    df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"])\n",
        "\n",
        "    # Get unique players\n",
        "    unique_players = df[\"PLAYER_ID\"].unique()\n",
        "\n",
        "    # Create a new dataframe to store the features\n",
        "    feature_data = []\n",
        "\n",
        "    for player_id in unique_players:\n",
        "        weighted_avg = compute_weighted_avg(player_id, df)\n",
        "        weighted_avg[\"PLAYER_ID\"] = player_id  # Retain the player ID\n",
        "        feature_data.append(weighted_avg)\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    feature_df = pd.DataFrame(feature_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    feature_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Feature data saved to {output_csv}. rows: {feature_df.shape}\")\n",
        "\n",
        "\n",
        "# Function to compute team-level aggregated features\n",
        "def create_team_features(player_features_file, team_features_output):\n",
        "    \"\"\"\n",
        "    Generate aggregated features for each team by averaging player statistics.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load the player features data\n",
        "    player_data = pd.read_csv(player_features_file)\n",
        "\n",
        "    # Compute team features\n",
        "    team_features = player_data.groupby(\"TEAM_ID\").mean().reset_index()\n",
        "\n",
        "    # Load mapping of TEAM_ID to TEAM_ABBREVIATION and additional features\n",
        "    preprocessed_nba_team_data = pd.read_csv(\n",
        "        \"preprocessed_nba_team_data.csv\"\n",
        "    )\n",
        "    id_to_abbr_map = preprocessed_nba_team_data.set_index(\"TEAM_ID\")[\n",
        "        \"TEAM_ABBREVIATION\"\n",
        "    ].to_dict()\n",
        "\n",
        "    # Apply a function to map TEAM_ID to ABBR\n",
        "    team_features[\"TEAM_ID\"] = team_features[\"TEAM_ID\"].map(id_to_abbr_map)\n",
        "\n",
        "    # Rename the column\n",
        "    team_features.rename(columns={\"TEAM_ID\": \"TEAM_ABBREVIATION\"}, inplace=True)\n",
        "\n",
        "    # Merge additional team features\n",
        "    additional_features = preprocessed_nba_team_data.drop(\n",
        "        columns=[\"TEAM_ID\", \"TEAM_CONFERENCE\", \"TEAM_SLUG\"]\n",
        "    )\n",
        "    team_features = team_features.merge(\n",
        "        additional_features,\n",
        "        left_on=\"TEAM_ABBREVIATION\",\n",
        "        right_on=\"TEAM_ABBREVIATION\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # Save the resulting team features to a CSV file\n",
        "    team_features.to_csv(team_features_output, index=False)\n",
        "    print(\n",
        "        f\"Team feature data saved to {team_features_output}. rows: {team_features.shape}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# File paths for input and output data\n",
        "team_data_file = \"nba_team_data.csv\"\n",
        "team_data_output = \"preprocessed_nba_team_data.csv\"\n",
        "\n",
        "game_data_file = \"nba_game_logs.csv\"\n",
        "game_data_output = \"preprocessed_nba_game_logs.csv\"\n",
        "\n",
        "player_game_logs_file = \"nba_player_game_logs.csv\"\n",
        "player_game_logs_output = \"preprocessed_nba_player_game_logs.csv\"\n",
        "\n",
        "# Run the preprocessing functions\n",
        "preprocess_team_data(team_data_file, team_data_output)\n",
        "preprocess_game_logs(game_data_file, game_data_output)\n",
        "preprocess_player_game_logs(player_game_logs_file, player_game_logs_output)\n",
        "create_feature_data(player_game_logs_output, \"nba_player_features.csv\")\n",
        "\n",
        "# File paths for player features and team output\n",
        "player_features_file = \"nba_player_features.csv\"\n",
        "team_features_output = \"nba_team_features.csv\"\n",
        "\n",
        "# Run the team feature creation function\n",
        "create_team_features(player_features_file, team_features_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSorjKxGouB",
        "outputId": "892ead76-d3e9-44fc-d756-941c955d93ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team data preprocessed and saved. rows: (30, 9)\n",
            "Game logs preprocessed and saved. rows: (3238, 54)\n",
            "Player game logs preprocessed and saved. rows: (34803, 62)\n",
            "Feature data saved to nba_player_features.csv. rows: (660, 61)\n",
            "Team feature data saved to nba_team_features.csv. rows: (30, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # To save and load the scaler\n",
        "\n",
        "\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using team-specific features.\n",
        "\n",
        "    Args:\n",
        "        game_logs_file: CSV file containing game logs with TEAM1, TEAM2, and WL columns.\n",
        "        features_file: CSV file to save aggregated team features.\n",
        "\n",
        "    Returns:\n",
        "        X: Feature matrix for training.\n",
        "        y: Target vector (win/loss).\n",
        "    \"\"\"\n",
        "    # Load game logs\n",
        "    game_logs = pd.read_csv(game_logs_file)\n",
        "\n",
        "    # Aggregate features by team\n",
        "    team_features = (\n",
        "        game_logs.groupby(\"TEAM1\")\n",
        "        .mean(numeric_only=True)\n",
        "        .reset_index()\n",
        "        .rename(columns={\"TEAM1\": \"TEAM\"})\n",
        "    )\n",
        "\n",
        "    # Save the aggregated features to features_file\n",
        "    team_features.to_csv(features_file, index=False)\n",
        "\n",
        "    # Merge team features for TEAM1 and TEAM2\n",
        "    game_logs = game_logs.merge(\n",
        "        team_features,\n",
        "        how=\"left\",\n",
        "        left_on=\"TEAM1\",\n",
        "        right_on=\"TEAM\",\n",
        "        suffixes=(\"\", \"_TEAM1\"),\n",
        "    ).merge(\n",
        "        team_features,\n",
        "        how=\"left\",\n",
        "        left_on=\"TEAM2\",\n",
        "        right_on=\"TEAM\",\n",
        "        suffixes=(\"\", \"_TEAM2\"),\n",
        "    )\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = [\n",
        "        \"TEAM\",\n",
        "        \"TEAM_TEAM2\",\n",
        "        \"TEAM_CONFERENCE\",\n",
        "        \"TEAM_SLUG\",\n",
        "        \"TEAM_CONFERENCE_TEAM2\",\n",
        "        \"TEAM_SLUG_TEAM2\",\n",
        "        \"PLAYER_ID\",\n",
        "        \"PLAYER_ID_TEAM2\",\n",
        "        \"AVAILABLE_FLAG\",\n",
        "        \"AVAILABLE_FLAG_TEAM2\",\n",
        "        \"GAME_TIMESTAMP\",\n",
        "        \"GAME_TIMESTAMP_TEAM2\",\n",
        "    ]\n",
        "    game_logs.drop(\n",
        "        columns=[col for col in columns_to_drop if col in game_logs.columns],\n",
        "        inplace=True,\n",
        "    )\n",
        "\n",
        "    # Feature engineering: Create new features for differences and ratios\n",
        "    numeric_columns = game_logs.filter(regex=\"_TEAM1$\").columns\n",
        "    diff_features = {}\n",
        "    ratio_features = {}\n",
        "    for col in numeric_columns:\n",
        "        base_col = col.replace(\"_TEAM1\", \"\")\n",
        "        diff_features[f\"{base_col}_DIFF\"] = (\n",
        "            game_logs[f\"{base_col}_TEAM1\"] - game_logs[f\"{base_col}_TEAM2\"]\n",
        "        )\n",
        "        ratio_features[f\"{base_col}_RATIO\"] = game_logs[f\"{base_col}_TEAM1\"] / (\n",
        "            game_logs[f\"{base_col}_TEAM2\"] + 1e-5\n",
        "        )\n",
        "\n",
        "    # Add all new features at once to optimize performance\n",
        "    new_features = pd.concat(\n",
        "        [pd.DataFrame(diff_features), pd.DataFrame(ratio_features)], axis=1\n",
        "    )\n",
        "    game_logs = pd.concat([game_logs, new_features], axis=1)\n",
        "\n",
        "    # Handle missing values\n",
        "    game_logs.fillna(0, inplace=True)\n",
        "\n",
        "    # Extract features and target\n",
        "    feature_columns = game_logs.select_dtypes(include=np.number).columns.difference(\n",
        "        [\"WL\"]\n",
        "    )\n",
        "    X = game_logs[feature_columns].to_numpy()\n",
        "    y = game_logs[\"WL\"].astype(int).to_numpy()\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"\n",
        "    Train a neural network model on the given features and labels.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix for training.\n",
        "        y: Target vector (win/loss).\n",
        "\n",
        "    Returns:\n",
        "        model: Trained neural network model.\n",
        "        scaler: Fitted scaler for feature normalization.\n",
        "    \"\"\"\n",
        "    # Normalize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Handle class imbalance\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "    # Define the neural network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation=\"relu\", input_shape=(X_resampled.shape[1],)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation=\"relu\"))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_resampled, y_resampled, epochs=16, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "def load_trained_model(\n",
        "    model_path=\"model.h5\", scaler_path=\"scaler.pkl\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Load the pre-trained neural network model and the scaler.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model file.\n",
        "        scaler_path: Path to the saved scaler file.\n",
        "\n",
        "    Returns:\n",
        "        model: The pre-trained model.\n",
        "        scaler: The scaler used for feature normalization.\n",
        "    \"\"\"\n",
        "    # Load the trained model\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Load the scaler\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    game_logs_file = \"preprocessed_nba_game_logs.csv\"\n",
        "    features_file = \"features.csv\"\n",
        "    model_save_path = \"model.h5\"\n",
        "    scaler_save_path = \"scaler.pkl\"\n",
        "\n",
        "    # Prepare the dataset\n",
        "    X, y = prepare_dataset(game_logs_file, features_file)\n",
        "\n",
        "    # Check if data is valid for training\n",
        "    if X.size == 0 or y.size == 0:\n",
        "        print(\"No data available to train the model.\")\n",
        "    else:\n",
        "        # Split the dataset into training and test sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Train the neural network\n",
        "        model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "        # Save the trained model\n",
        "        model.save(model_save_path)\n",
        "        print(f\"Neural network model saved to {model_save_path}\")\n",
        "\n",
        "        # Save the scaler\n",
        "        joblib.dump(scaler, scaler_save_path)\n",
        "        print(f\"Scaler saved to {scaler_save_path}\")\n",
        "\n",
        "        # Evaluate the neural network\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "        print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "        # Train and evaluate a Random Forest for comparison\n",
        "        rf = RandomForestClassifier(random_state=42)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred_rf = rf.predict(X_test)\n",
        "        rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "        print(f\"Random Forest Accuracy: {rf_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cuhZRejGr7s",
        "outputId": "54e730d4-6b64-461d-f33f-62e8b78de4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7498 - loss: 0.4849 - val_accuracy: 0.9923 - val_loss: 0.0477\n",
            "Epoch 2/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9742 - loss: 0.0755 - val_accuracy: 0.9865 - val_loss: 0.0300\n",
            "Epoch 3/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9916 - loss: 0.0274 - val_accuracy: 0.9769 - val_loss: 0.0661\n",
            "Epoch 4/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9842 - loss: 0.0467 - val_accuracy: 0.9942 - val_loss: 0.0110\n",
            "Epoch 5/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9913 - loss: 0.0214 - val_accuracy: 0.9904 - val_loss: 0.0340\n",
            "Epoch 6/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9936 - loss: 0.0185 - val_accuracy: 0.9962 - val_loss: 0.0111\n",
            "Epoch 7/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9978 - loss: 0.0063 - val_accuracy: 0.9981 - val_loss: 0.0042\n",
            "Epoch 8/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9993 - loss: 0.0036 - val_accuracy: 0.9904 - val_loss: 0.0351\n",
            "Epoch 9/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 0.9962 - val_loss: 0.0101\n",
            "Epoch 10/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9964 - loss: 0.0143 - val_accuracy: 0.9981 - val_loss: 0.0060\n",
            "Epoch 11/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9945 - loss: 0.0220 - val_accuracy: 0.9942 - val_loss: 0.0092\n",
            "Epoch 12/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0063 - val_accuracy: 0.9981 - val_loss: 0.0162\n",
            "Epoch 13/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9974 - loss: 0.0108 - val_accuracy: 0.9769 - val_loss: 0.0686\n",
            "Epoch 14/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9970 - loss: 0.0065 - val_accuracy: 0.9942 - val_loss: 0.0164\n",
            "Epoch 15/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9964 - loss: 0.0097 - val_accuracy: 0.9962 - val_loss: 0.0087\n",
            "Epoch 16/16\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9988 - loss: 0.0032 - val_accuracy: 0.9981 - val_loss: 0.0075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network model saved to model.h5\n",
            "Scaler saved to scaler.pkl\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9963 - loss: 0.0073     \n",
            "Test Loss: 0.014677568338811398, Test Accuracy: 0.9938271641731262\n",
            "Random Forest Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "\n",
        "# Get Team ID based on abbreviation\n",
        "def get_team_id_by_abbreviation(team_abbreviation):\n",
        "    \"\"\"Retrieve the team ID by abbreviation.\"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        if team[\"abbreviation\"].lower() == team_abbreviation.lower():\n",
        "            return team[\"id\"]\n",
        "    raise ValueError(\n",
        "        f\"Team '{team_abbreviation}' not found! Please enter a valid abbreviation.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Fetch team-specific features\n",
        "def fetch_team_features(team_abbreviation, features_file):\n",
        "    \"\"\"\n",
        "    Retrieve the team-specific features for the given team abbreviation.\n",
        "    Args:\n",
        "        team_abbreviation: Abbreviation of the NBA team (e.g., 'LAL').\n",
        "        features_file: CSV file containing the aggregated team features.\n",
        "    Returns:\n",
        "        numpy array of the team's features.\n",
        "    \"\"\"\n",
        "    team_features = pd.read_csv(features_file)\n",
        "    team_row = team_features[team_features[\"TEAM\"] == team_abbreviation.upper()]\n",
        "    if team_row.empty:\n",
        "        raise ValueError(\n",
        "            f\"Features for team '{team_abbreviation}' not found in {features_file}.\"\n",
        "        )\n",
        "    return team_row.drop(columns=[\"TEAM\"]).to_numpy().flatten()\n",
        "\n",
        "\n",
        "# Predict win probability\n",
        "def predict_matchup_win_probability(\n",
        "    team1_abbreviation, team2_abbreviation, features_file\n",
        "):\n",
        "    \"\"\"\n",
        "    Predict the win probability of Team 1 beating Team 2.\n",
        "    Args:\n",
        "        team1_abbreviation: Abbreviation of Team 1 (e.g., 'LAL').\n",
        "        team2_abbreviation: Abbreviation of Team 2 (e.g., 'BOS').\n",
        "        features_file: Path to the CSV file containing aggregated team features.\n",
        "    \"\"\"\n",
        "    # Load the trained model and scaler\n",
        "    model, scaler = load_trained_model()\n",
        "\n",
        "    # Fetch features for both teams\n",
        "    team1_features = fetch_team_features(team1_abbreviation, features_file)\n",
        "    team2_features = fetch_team_features(team2_abbreviation, features_file)\n",
        "\n",
        "    # Calculate difference and ratio features\n",
        "    diff_features = team1_features - team2_features\n",
        "    ratio_features = team1_features / (team2_features + 1e-5)\n",
        "\n",
        "    # Combine features for the model\n",
        "    matchup_features = np.concatenate([diff_features, ratio_features]).reshape(1, -1)\n",
        "\n",
        "    # Debug: Check input shape\n",
        "    print(f\"Matchup features shape: {matchup_features.shape}\")\n",
        "    print(f\"Scaler expects: {scaler.n_features_in_}\")\n",
        "\n",
        "    # Ensure consistent feature count\n",
        "    if matchup_features.shape[1] != scaler.n_features_in_:\n",
        "        raise ValueError(\n",
        "            f\"Feature count mismatch. Got {matchup_features.shape[1]} features, \"\n",
        "            f\"but scaler expects {scaler.n_features_in_}. Check feature engineering consistency.\"\n",
        "        )\n",
        "\n",
        "    # Scale the features\n",
        "    scaled_features = scaler.transform(matchup_features)\n",
        "\n",
        "    # Predict win probability for Team 1\n",
        "    win_probability = model.predict(scaled_features)[0][0]\n",
        "    print(\n",
        "        f\"Predicted probability of {team1_abbreviation} beating {team2_abbreviation}: {win_probability * 100:.2f}%\"\n",
        "    )\n",
        "    return win_probability\n",
        "\n",
        "\n",
        "def display_team_data():\n",
        "    \"\"\"Display team names, abbreviations, and IDs in a 2D array.\"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = np.array(\n",
        "        [[team[\"full_name\"], team[\"abbreviation\"], team[\"id\"]] for team in nba_teams]\n",
        "    )\n",
        "    print(\"\\nAvailable Teams:\")\n",
        "    print(pd.DataFrame(team_data, columns=[\"Team Name\", \"Abbreviation\", \"Team ID\"]))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to handle user input and prediction.\"\"\"\n",
        "    features_file = \"features.csv\"  # Path to the features file\n",
        "\n",
        "    # Display team data before taking user input\n",
        "    display_team_data()\n",
        "\n",
        "    team1_abbreviation = input(\n",
        "        \"Enter Team 1 abbreviation (e.g., 'LAL' for Los Angeles Lakers): \"\n",
        "    ).strip()\n",
        "    team2_abbreviation = input(\n",
        "        \"Enter Team 2 abbreviation (e.g., 'BOS' for Boston Celtics): \"\n",
        "    ).strip()\n",
        "\n",
        "    try:\n",
        "        predict_matchup_win_probability(\n",
        "            team1_abbreviation, team2_abbreviation, features_file\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heGJjGPVGuLX",
        "outputId": "936e4423-b204-4723-bb90-0fbc62815f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available Teams:\n",
            "                 Team Name Abbreviation     Team ID\n",
            "0            Atlanta Hawks          ATL  1610612737\n",
            "1           Boston Celtics          BOS  1610612738\n",
            "2      Cleveland Cavaliers          CLE  1610612739\n",
            "3     New Orleans Pelicans          NOP  1610612740\n",
            "4            Chicago Bulls          CHI  1610612741\n",
            "5         Dallas Mavericks          DAL  1610612742\n",
            "6           Denver Nuggets          DEN  1610612743\n",
            "7    Golden State Warriors          GSW  1610612744\n",
            "8          Houston Rockets          HOU  1610612745\n",
            "9     Los Angeles Clippers          LAC  1610612746\n",
            "10      Los Angeles Lakers          LAL  1610612747\n",
            "11              Miami Heat          MIA  1610612748\n",
            "12         Milwaukee Bucks          MIL  1610612749\n",
            "13  Minnesota Timberwolves          MIN  1610612750\n",
            "14           Brooklyn Nets          BKN  1610612751\n",
            "15         New York Knicks          NYK  1610612752\n",
            "16           Orlando Magic          ORL  1610612753\n",
            "17          Indiana Pacers          IND  1610612754\n",
            "18      Philadelphia 76ers          PHI  1610612755\n",
            "19            Phoenix Suns          PHX  1610612756\n",
            "20  Portland Trail Blazers          POR  1610612757\n",
            "21        Sacramento Kings          SAC  1610612758\n",
            "22       San Antonio Spurs          SAS  1610612759\n",
            "23   Oklahoma City Thunder          OKC  1610612760\n",
            "24         Toronto Raptors          TOR  1610612761\n",
            "25               Utah Jazz          UTA  1610612762\n",
            "26       Memphis Grizzlies          MEM  1610612763\n",
            "27      Washington Wizards          WAS  1610612764\n",
            "28         Detroit Pistons          DET  1610612765\n",
            "29       Charlotte Hornets          CHA  1610612766\n",
            "Enter Team 1 abbreviation (e.g., 'LAL' for Los Angeles Lakers): POR\n",
            "Enter Team 2 abbreviation (e.g., 'BOS' for Boston Celtics): SAS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matchup features shape: (1, 102)\n",
            "Scaler expects: 254\n",
            "Feature count mismatch. Got 102 features, but scaler expects 254. Check feature engineering consistency.\n"
          ]
        }
      ]
    }
  ]
}