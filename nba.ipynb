{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt11YkmCoe5ScLy6Ji8dyD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "795d3e75-e980-4376-f133-7cdb22e9fb11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nba_api\n",
            "  Downloading nba_api-1.6.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2024.12.14)\n",
            "Downloading nba_api-1.6.1-py3-none-any.whl (279 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/279.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.4/279.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nba_api\n",
            "Successfully installed nba_api-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODhh0g7xGfMv",
        "outputId": "8d889b71-2086-4481-ec01-29883c387c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Team information data stored.\n",
            "Fetching game logs...\n",
            "Game logs data stored.\n",
            "Fetching player game logs...\n",
            "Player game logs data stored.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "# Maximum number of retries for each API call\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "\n",
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt  # Exponential backoff\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_team_info(seasons):\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        for team in nba_teams:\n",
        "            team_info = fetch_with_retries(\n",
        "                TeamInfoCommon,\n",
        "                team_id=team[\"id\"],\n",
        "                season_type_nullable=\"Regular Season\",\n",
        "                timeout=60,\n",
        "            )\n",
        "            if team_info:\n",
        "                df_team = team_info.get_data_frames()[0]\n",
        "                team_data.append(df_team)\n",
        "                time.sleep(0.6)  # Delay to avoid API rate limits\n",
        "            else:\n",
        "                print(\n",
        "                    f\"Skipping team {team['full_name']} for season {season} after failed attempts.\"\n",
        "                )\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "\n",
        "def get_game_logs(seasons):\n",
        "    print(\"Fetching game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            game_log_data.append(df_game_logs)\n",
        "            time.sleep(0.6)\n",
        "        else:\n",
        "            print(f\"Skipping game logs for season {season} after failed attempts.\")\n",
        "\n",
        "    if game_log_data:\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "        df_all_game_logs.to_csv(\"nba_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "\n",
        "def get_player_game_logs(seasons):\n",
        "    print(\"Fetching player game logs...\")\n",
        "    player_game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        player_game_logs = fetch_with_retries(\n",
        "            PlayerGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if player_game_logs:\n",
        "            df_player_game_logs = player_game_logs.get_data_frames()[0]\n",
        "            player_game_log_data.append(df_player_game_logs)\n",
        "            time.sleep(0.6)\n",
        "        else:\n",
        "            print(\n",
        "                f\"Skipping player game logs for season {season} after failed attempts.\"\n",
        "            )\n",
        "\n",
        "    if player_game_log_data:\n",
        "        df_all_player_game_logs = pd.concat(player_game_log_data, ignore_index=True)\n",
        "        df_all_player_game_logs.to_csv(\"nba_player_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player game log data fetched.\")\n",
        "\n",
        "\n",
        "# Define the list of seasons\n",
        "seasons = [\"2023-24\", \"2024-25\"]\n",
        "\n",
        "# Run functions to save data to CSV files\n",
        "get_team_info(seasons[-1])\n",
        "print(\"Team information data stored.\")\n",
        "\n",
        "get_game_logs(seasons)\n",
        "print(\"Game logs data stored.\")\n",
        "\n",
        "get_player_game_logs(seasons[-2:])  # Fetching for the most recent two seasons only\n",
        "print(\"Player game logs data stored.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to preprocess team data\n",
        "def preprocess_team_data(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save team data by removing unnecessary columns.\"\"\"\n",
        "    team_data = pd.read_csv(input_file).dropna()\n",
        "    columns_to_drop = [\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_CITY\",\n",
        "        \"SEASON_YEAR\",\n",
        "        \"TEAM_CODE\",\n",
        "        \"TEAM_DIVISION\",\n",
        "        \"MIN_YEAR\",\n",
        "        \"MAX_YEAR\",\n",
        "    ]\n",
        "    team_data = team_data.drop(columns=columns_to_drop)\n",
        "    team_data = team_data.drop_duplicates()\n",
        "    team_data.to_csv(output_file, index=False)\n",
        "    print(f\"Team data preprocessed and saved. rows: {team_data.shape}\")\n",
        "\n",
        "\n",
        "# Function to preprocess game logs\n",
        "def preprocess_game_logs(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save game log data with specific transformations.\"\"\"\n",
        "    game_logs = pd.read_csv(input_file).dropna()\n",
        "\n",
        "    # Convert 'GAME_DATE' to datetime and sort by date\n",
        "    game_logs[\"GAME_DATE\"] = pd.to_datetime(game_logs[\"GAME_DATE\"])\n",
        "    game_logs_sorted = game_logs.sort_values(by=\"GAME_DATE\", ascending=True)\n",
        "\n",
        "    # Extract 'TEAM1' and 'TEAM2' from 'MATCHUP' column\n",
        "    game_logs_sorted[\"TEAM1\"] = game_logs_sorted[\"MATCHUP\"].str.split().str[0]\n",
        "    game_logs_sorted[\"TEAM2\"] = game_logs_sorted[\"MATCHUP\"].str.split().str[2]\n",
        "\n",
        "    # Drop columns that are not required\n",
        "    columns_to_drop = [\n",
        "        \"MATCHUP\",\n",
        "        \"AVAILABLE_FLAG\",\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_ABBREVIATION\",\n",
        "        \"GAME_ID\",\n",
        "    ]\n",
        "    game_logs_cleaned = game_logs_sorted.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Convert 'WL' to binary format: 'W' becomes 1, 'L' becomes 0\n",
        "    game_logs_cleaned[\"WL\"] = game_logs_cleaned[\"WL\"].apply(\n",
        "        lambda result: 1 if result == \"W\" else 0\n",
        "    )\n",
        "\n",
        "    # Convert 'SEASON_YEAR' to integer format, using only the starting year\n",
        "    game_logs_cleaned[\"SEASON_YEAR\"] = game_logs_cleaned[\"SEASON_YEAR\"].apply(\n",
        "        lambda year: int(year[:4])\n",
        "    )\n",
        "\n",
        "    game_logs_cleaned.to_csv(output_file, index=False)\n",
        "    print(f\"Game logs preprocessed and saved. rows: {game_logs_cleaned.shape}\")\n",
        "\n",
        "\n",
        "# Function to preprocess player game logs\n",
        "def preprocess_player_game_logs(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save player game log data with specific transformations.\"\"\"\n",
        "    player_game_logs = pd.read_csv(input_file).dropna()\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = [\n",
        "        \"PLAYER_NAME\",\n",
        "        \"NICKNAME\",\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_ABBREVIATION\",\n",
        "        \"MATCHUP\",\n",
        "        \"GAME_ID\",\n",
        "        \"MIN_SEC\",\n",
        "    ]\n",
        "    player_game_logs = player_game_logs.drop(columns=columns_to_drop)\n",
        "    player_game_logs[\"WL\"] = player_game_logs[\"WL\"].apply(\n",
        "        lambda x: 1 if x == \"W\" else 0\n",
        "    )\n",
        "    player_game_logs[\"SEASON_YEAR\"] = player_game_logs[\"SEASON_YEAR\"].apply(\n",
        "        lambda x: x[:4]\n",
        "    )\n",
        "\n",
        "    player_game_logs.to_csv(output_file, index=False)\n",
        "    print(f\"Player game logs preprocessed and saved. rows: {player_game_logs.shape}\")\n",
        "\n",
        "\n",
        "# Function to compute weighted averages\n",
        "def compute_weighted_avg(player_id, df):\n",
        "    \"\"\"Compute weighted averages for a given player.\"\"\"\n",
        "    player_rows = df[df[\"PLAYER_ID\"] == player_id].copy()\n",
        "\n",
        "    # Retain TEAM_ID\n",
        "    team_id = player_rows[\"TEAM_ID\"].iloc[0]\n",
        "\n",
        "    # Convert GAME_DATE to a timestamp for weighting\n",
        "    player_rows[\"GAME_TIMESTAMP\"] = pd.to_datetime(player_rows[\"GAME_DATE\"]).apply(\n",
        "        lambda x: x.timestamp()\n",
        "    )\n",
        "    max_timestamp = player_rows[\"GAME_TIMESTAMP\"].max()\n",
        "\n",
        "    # Calculate weights based on recency\n",
        "    player_rows[\"WEIGHTS\"] = np.exp(\n",
        "        (player_rows[\"GAME_TIMESTAMP\"] - max_timestamp) / 1e7\n",
        "    )\n",
        "    player_rows[\"WEIGHTS\"] /= player_rows[\"WEIGHTS\"].sum()\n",
        "\n",
        "    # Compute weighted average for all columns after WL\n",
        "    weighted_avg = (\n",
        "        player_rows.iloc[:, df.columns.get_loc(\"WL\") + 1 :]  # Select columns after WL\n",
        "        .mul(player_rows[\"WEIGHTS\"], axis=0)  # Multiply each column by weights\n",
        "        .sum()  # Sum the weighted values for each column\n",
        "    )\n",
        "    weighted_avg[\"TEAM_ID\"] = team_id  # Include TEAM_ID in the output\n",
        "    return weighted_avg\n",
        "\n",
        "\n",
        "# Function to create feature data\n",
        "def create_feature_data(input_csv, output_csv):\n",
        "    \"\"\"Generate feature data where each player is represented by a single row.\"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_csv)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure GAME_DATE is parsed correctly\n",
        "    df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"])\n",
        "\n",
        "    # Get unique players\n",
        "    unique_players = df[\"PLAYER_ID\"].unique()\n",
        "\n",
        "    # Create a new dataframe to store the features\n",
        "    feature_data = []\n",
        "\n",
        "    for player_id in unique_players:\n",
        "        weighted_avg = compute_weighted_avg(player_id, df)\n",
        "        weighted_avg[\"PLAYER_ID\"] = player_id  # Retain the player ID\n",
        "        feature_data.append(weighted_avg)\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    feature_df = pd.DataFrame(feature_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    feature_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Feature data saved to {output_csv}. rows: {feature_df.shape}\")\n",
        "\n",
        "\n",
        "# Function to compute team-level aggregated features\n",
        "def create_team_features(player_features_file, team_features_output):\n",
        "    \"\"\"\n",
        "    Generate aggregated features for each team by averaging player statistics.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load the player features data\n",
        "    player_data = pd.read_csv(player_features_file)\n",
        "\n",
        "    # Compute team features\n",
        "    team_features = player_data.groupby(\"TEAM_ID\").mean().reset_index()\n",
        "\n",
        "    # Load mapping of TEAM_ID to TEAM_ABBREVIATION and additional features\n",
        "    preprocessed_nba_team_data = pd.read_csv(\n",
        "        \"preprocessed_nba_team_data.csv\"\n",
        "    )\n",
        "    id_to_abbr_map = preprocessed_nba_team_data.set_index(\"TEAM_ID\")[\n",
        "        \"TEAM_ABBREVIATION\"\n",
        "    ].to_dict()\n",
        "\n",
        "    # Apply a function to map TEAM_ID to ABBR\n",
        "    team_features[\"TEAM_ID\"] = team_features[\"TEAM_ID\"].map(id_to_abbr_map)\n",
        "\n",
        "    # Rename the column\n",
        "    team_features.rename(columns={\"TEAM_ID\": \"TEAM_ABBREVIATION\"}, inplace=True)\n",
        "\n",
        "    # Merge additional team features\n",
        "    additional_features = preprocessed_nba_team_data.drop(\n",
        "        columns=[\"TEAM_ID\", \"TEAM_CONFERENCE\", \"TEAM_SLUG\"]\n",
        "    )\n",
        "    team_features = team_features.merge(\n",
        "        additional_features,\n",
        "        left_on=\"TEAM_ABBREVIATION\",\n",
        "        right_on=\"TEAM_ABBREVIATION\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # Save the resulting team features to a CSV file\n",
        "    team_features.to_csv(team_features_output, index=False)\n",
        "    print(\n",
        "        f\"Team feature data saved to {team_features_output}. rows: {team_features.shape}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# File paths for input and output data\n",
        "team_data_file = \"nba_team_data.csv\"\n",
        "team_data_output = \"preprocessed_nba_team_data.csv\"\n",
        "\n",
        "game_data_file = \"nba_game_logs.csv\"\n",
        "game_data_output = \"preprocessed_nba_game_logs.csv\"\n",
        "\n",
        "player_game_logs_file = \"nba_player_game_logs.csv\"\n",
        "player_game_logs_output = \"preprocessed_nba_player_game_logs.csv\"\n",
        "\n",
        "# Run the preprocessing functions\n",
        "preprocess_team_data(team_data_file, team_data_output)\n",
        "preprocess_game_logs(game_data_file, game_data_output)\n",
        "preprocess_player_game_logs(player_game_logs_file, player_game_logs_output)\n",
        "create_feature_data(player_game_logs_output, \"nba_player_features.csv\")\n",
        "\n",
        "# File paths for player features and team output\n",
        "player_features_file = \"nba_player_features.csv\"\n",
        "team_features_output = \"nba_team_features.csv\"\n",
        "\n",
        "# Run the team feature creation function\n",
        "create_team_features(player_features_file, team_features_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSorjKxGouB",
        "outputId": "e159d070-881b-46e4-8907-6f34e26024c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team data preprocessed and saved. rows: (30, 9)\n",
            "Game logs preprocessed and saved. rows: (3300, 54)\n",
            "Player game logs preprocessed and saved. rows: (35471, 62)\n",
            "Feature data saved to nba_player_features.csv. rows: (660, 61)\n",
            "Team feature data saved to nba_team_features.csv. rows: (30, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.metrics import Metric\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib  # To save and load the scaler\n",
        "from tensorflow.keras.backend import round as kround\n",
        "\n",
        "\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using team-specific features.\n",
        "\n",
        "    Args:\n",
        "        game_logs_file: CSV file containing game logs with TEAM1, TEAM2, and WL columns.\n",
        "        features_file: CSV file to save aggregated team features.\n",
        "\n",
        "    Returns:\n",
        "        X: Feature matrix for training.\n",
        "        y: Target vector (win/loss).\n",
        "        feature_columns: List of feature names used in the dataset.\n",
        "    \"\"\"\n",
        "    # Load game logs\n",
        "    game_logs = pd.read_csv(game_logs_file)\n",
        "\n",
        "    # Aggregate features by team\n",
        "    team_features = (\n",
        "        game_logs.groupby(\"TEAM1\").mean(numeric_only=True).reset_index().rename(columns={\"TEAM1\": \"TEAM\"})\n",
        "    )\n",
        "    team_features.to_csv(features_file, index=False)\n",
        "\n",
        "    # Merge aggregated features for TEAM1 and TEAM2\n",
        "    game_logs = game_logs.merge(\n",
        "        team_features.add_suffix(\"_TEAM1\"), left_on=\"TEAM1\", right_on=\"TEAM_TEAM1\"\n",
        "    ).merge(\n",
        "        team_features.add_suffix(\"_TEAM2\"), left_on=\"TEAM2\", right_on=\"TEAM_TEAM2\"\n",
        "    )\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = [\"TEAM_TEAM1\", \"TEAM_TEAM2\"]\n",
        "    game_logs.drop(columns=[col for col in columns_to_drop if col in game_logs.columns], inplace=True)\n",
        "\n",
        "    # Create feature differences and ratios\n",
        "    numeric_columns = [col for col in game_logs.columns if col.endswith(\"_TEAM1\")]\n",
        "    for col in numeric_columns:\n",
        "        base_col = col.replace(\"_TEAM1\", \"\")\n",
        "        game_logs[f\"{base_col}_DIFF\"] = game_logs[f\"{base_col}_TEAM1\"] - game_logs[f\"{base_col}_TEAM2\"]\n",
        "        game_logs[f\"{base_col}_RATIO\"] = game_logs[f\"{base_col}_TEAM1\"] / (game_logs[f\"{base_col}_TEAM2\"] + 1e-5)\n",
        "\n",
        "    # Handle missing values\n",
        "    game_logs.fillna(0, inplace=True)\n",
        "\n",
        "    # Extract features and target\n",
        "    feature_columns = game_logs.select_dtypes(include=np.number).columns.difference([\"WL\"])\n",
        "\n",
        "    # Print the feature names\n",
        "    print(\"Feature Columns Used in the Model:\")\n",
        "    print(feature_columns.tolist())\n",
        "\n",
        "    X = game_logs[feature_columns].to_numpy()\n",
        "    y = game_logs[\"WL\"].astype(int).to_numpy()\n",
        "\n",
        "    return X, y, feature_columns\n",
        "\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom accuracy metric to evaluate the model based on given conditions.\n",
        "    If the prediction is in [0, 0.5) and true label is 0, it's correct.\n",
        "    If the prediction is in [0.5, 1] and true label is 1, it's correct.\n",
        "    \"\"\"\n",
        "    condition_1 = K.cast(y_pred < 0.5, dtype=\"float32\") * K.cast(y_true == 0, dtype=\"float32\")\n",
        "    condition_2 = K.cast(y_pred >= 0.5, dtype=\"float32\") * K.cast(y_true == 1, dtype=\"float32\")\n",
        "    return K.mean(condition_1 + condition_2)\n",
        "\n",
        "\n",
        "def build_neural_network(input_shape):\n",
        "    \"\"\"\n",
        "    Build a neural network model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Number of input features.\n",
        "\n",
        "    Returns:\n",
        "        model: Compiled neural network model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(256, activation=\"relu\", input_shape=(input_shape,)),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\", custom_accuracy])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"\n",
        "    Train a neural network model.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix for training.\n",
        "        y: Target vector (win/loss).\n",
        "\n",
        "    Returns:\n",
        "        model: Trained neural network model.\n",
        "        scaler: Fitted scaler for feature normalization.\n",
        "    \"\"\"\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Handle class imbalance\n",
        "    X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_scaled, y)\n",
        "\n",
        "    # Build and train the model\n",
        "    model = build_neural_network(X_resampled.shape[1])\n",
        "    model.fit(X_resampled, y_resampled, epochs=16, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "def save_model_and_scaler(model, scaler, model_path=\"model.h5\", scaler_path=\"scaler.pkl\"):\n",
        "    \"\"\"\n",
        "    Save trained model and scaler.\n",
        "    \"\"\"\n",
        "    model.save(model_path)\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "    print(f\"Model saved to {model_path}, Scaler saved to {scaler_path}\")\n",
        "\n",
        "\n",
        "def save_feature_names(feature_names, feature_names_file=\"feature_names.pkl\"):\n",
        "    \"\"\"\n",
        "    Save feature names for later use during prediction.\n",
        "    \"\"\"\n",
        "    joblib.dump(feature_names, feature_names_file)\n",
        "    print(f\"Feature names saved to {feature_names_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    game_logs_file = \"preprocessed_nba_game_logs.csv\"\n",
        "    features_file = \"features.csv\"\n",
        "    model_save_path = \"model.h5\"\n",
        "    scaler_save_path = \"scaler.pkl\"\n",
        "\n",
        "    # Prepare the dataset\n",
        "    X, y, feature_columns = prepare_dataset(game_logs_file, features_file)\n",
        "\n",
        "    if X.size == 0 or y.size == 0:\n",
        "        print(\"No data available to train the model.\")\n",
        "        return\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train neural network\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Save the model and scaler\n",
        "    save_model_and_scaler(model, scaler, model_save_path, scaler_save_path)\n",
        "\n",
        "    # Save feature names after preparing the dataset\n",
        "    save_feature_names(feature_columns.tolist())\n",
        "\n",
        "    # Evaluate the model\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    test_loss, test_accuracy, test_custom_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "    print(f\"Neural Network - Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Custom Accuracy: {test_custom_accuracy}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cuhZRejGr7s",
        "outputId": "1ea5d42e-9e45-462a-a57c-c440408a7d3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-e2d8c9755889>:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  game_logs[f\"{base_col}_DIFF\"] = game_logs[f\"{base_col}_TEAM1\"] - game_logs[f\"{base_col}_TEAM2\"]\n",
            "<ipython-input-4-e2d8c9755889>:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  game_logs[f\"{base_col}_RATIO\"] = game_logs[f\"{base_col}_TEAM1\"] / (game_logs[f\"{base_col}_TEAM2\"] + 1e-5)\n",
            "<ipython-input-4-e2d8c9755889>:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  game_logs[f\"{base_col}_DIFF\"] = game_logs[f\"{base_col}_TEAM1\"] - game_logs[f\"{base_col}_TEAM2\"]\n",
            "<ipython-input-4-e2d8c9755889>:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  game_logs[f\"{base_col}_RATIO\"] = game_logs[f\"{base_col}_TEAM1\"] / (game_logs[f\"{base_col}_TEAM2\"] + 1e-5)\n",
            "<ipython-input-4-e2d8c9755889>:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  game_logs[f\"{base_col}_DIFF\"] = game_logs[f\"{base_col}_TEAM1\"] - game_logs[f\"{base_col}_TEAM2\"]\n",
            "<ipython-input-4-e2d8c9755889>:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  game_logs[f\"{base_col}_RATIO\"] = game_logs[f\"{base_col}_TEAM1\"] / (game_logs[f\"{base_col}_TEAM2\"] + 1e-5)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Columns Used in the Model:\n",
            "['AST', 'AST_DIFF', 'AST_RANK', 'AST_RANK_DIFF', 'AST_RANK_RATIO', 'AST_RANK_TEAM1', 'AST_RANK_TEAM2', 'AST_RATIO', 'AST_TEAM1', 'AST_TEAM2', 'BLK', 'BLKA', 'BLKA_DIFF', 'BLKA_RANK', 'BLKA_RANK_DIFF', 'BLKA_RANK_RATIO', 'BLKA_RANK_TEAM1', 'BLKA_RANK_TEAM2', 'BLKA_RATIO', 'BLKA_TEAM1', 'BLKA_TEAM2', 'BLK_DIFF', 'BLK_RANK', 'BLK_RANK_DIFF', 'BLK_RANK_RATIO', 'BLK_RANK_TEAM1', 'BLK_RANK_TEAM2', 'BLK_RATIO', 'BLK_TEAM1', 'BLK_TEAM2', 'DREB', 'DREB_DIFF', 'DREB_RANK', 'DREB_RANK_DIFF', 'DREB_RANK_RATIO', 'DREB_RANK_TEAM1', 'DREB_RANK_TEAM2', 'DREB_RATIO', 'DREB_TEAM1', 'DREB_TEAM2', 'FG3A', 'FG3A_DIFF', 'FG3A_RANK', 'FG3A_RANK_DIFF', 'FG3A_RANK_RATIO', 'FG3A_RANK_TEAM1', 'FG3A_RANK_TEAM2', 'FG3A_RATIO', 'FG3A_TEAM1', 'FG3A_TEAM2', 'FG3M', 'FG3M_DIFF', 'FG3M_RANK', 'FG3M_RANK_DIFF', 'FG3M_RANK_RATIO', 'FG3M_RANK_TEAM1', 'FG3M_RANK_TEAM2', 'FG3M_RATIO', 'FG3M_TEAM1', 'FG3M_TEAM2', 'FG3_PCT', 'FG3_PCT_DIFF', 'FG3_PCT_RANK', 'FG3_PCT_RANK_DIFF', 'FG3_PCT_RANK_RATIO', 'FG3_PCT_RANK_TEAM1', 'FG3_PCT_RANK_TEAM2', 'FG3_PCT_RATIO', 'FG3_PCT_TEAM1', 'FG3_PCT_TEAM2', 'FGA', 'FGA_DIFF', 'FGA_RANK', 'FGA_RANK_DIFF', 'FGA_RANK_RATIO', 'FGA_RANK_TEAM1', 'FGA_RANK_TEAM2', 'FGA_RATIO', 'FGA_TEAM1', 'FGA_TEAM2', 'FGM', 'FGM_DIFF', 'FGM_RANK', 'FGM_RANK_DIFF', 'FGM_RANK_RATIO', 'FGM_RANK_TEAM1', 'FGM_RANK_TEAM2', 'FGM_RATIO', 'FGM_TEAM1', 'FGM_TEAM2', 'FG_PCT', 'FG_PCT_DIFF', 'FG_PCT_RANK', 'FG_PCT_RANK_DIFF', 'FG_PCT_RANK_RATIO', 'FG_PCT_RANK_TEAM1', 'FG_PCT_RANK_TEAM2', 'FG_PCT_RATIO', 'FG_PCT_TEAM1', 'FG_PCT_TEAM2', 'FTA', 'FTA_DIFF', 'FTA_RANK', 'FTA_RANK_DIFF', 'FTA_RANK_RATIO', 'FTA_RANK_TEAM1', 'FTA_RANK_TEAM2', 'FTA_RATIO', 'FTA_TEAM1', 'FTA_TEAM2', 'FTM', 'FTM_DIFF', 'FTM_RANK', 'FTM_RANK_DIFF', 'FTM_RANK_RATIO', 'FTM_RANK_TEAM1', 'FTM_RANK_TEAM2', 'FTM_RATIO', 'FTM_TEAM1', 'FTM_TEAM2', 'FT_PCT', 'FT_PCT_DIFF', 'FT_PCT_RANK', 'FT_PCT_RANK_DIFF', 'FT_PCT_RANK_RATIO', 'FT_PCT_RANK_TEAM1', 'FT_PCT_RANK_TEAM2', 'FT_PCT_RATIO', 'FT_PCT_TEAM1', 'FT_PCT_TEAM2', 'GP_RANK', 'GP_RANK_DIFF', 'GP_RANK_RATIO', 'GP_RANK_TEAM1', 'GP_RANK_TEAM2', 'L_RANK', 'L_RANK_DIFF', 'L_RANK_RATIO', 'L_RANK_TEAM1', 'L_RANK_TEAM2', 'MIN', 'MIN_DIFF', 'MIN_RANK', 'MIN_RANK_DIFF', 'MIN_RANK_RATIO', 'MIN_RANK_TEAM1', 'MIN_RANK_TEAM2', 'MIN_RATIO', 'MIN_TEAM1', 'MIN_TEAM2', 'OREB', 'OREB_DIFF', 'OREB_RANK', 'OREB_RANK_DIFF', 'OREB_RANK_RATIO', 'OREB_RANK_TEAM1', 'OREB_RANK_TEAM2', 'OREB_RATIO', 'OREB_TEAM1', 'OREB_TEAM2', 'PF', 'PFD', 'PFD_DIFF', 'PFD_RANK', 'PFD_RANK_DIFF', 'PFD_RANK_RATIO', 'PFD_RANK_TEAM1', 'PFD_RANK_TEAM2', 'PFD_RATIO', 'PFD_TEAM1', 'PFD_TEAM2', 'PF_DIFF', 'PF_RANK', 'PF_RANK_DIFF', 'PF_RANK_RATIO', 'PF_RANK_TEAM1', 'PF_RANK_TEAM2', 'PF_RATIO', 'PF_TEAM1', 'PF_TEAM2', 'PLUS_MINUS', 'PLUS_MINUS_DIFF', 'PLUS_MINUS_RANK', 'PLUS_MINUS_RANK_DIFF', 'PLUS_MINUS_RANK_RATIO', 'PLUS_MINUS_RANK_TEAM1', 'PLUS_MINUS_RANK_TEAM2', 'PLUS_MINUS_RATIO', 'PLUS_MINUS_TEAM1', 'PLUS_MINUS_TEAM2', 'PTS', 'PTS_DIFF', 'PTS_RANK', 'PTS_RANK_DIFF', 'PTS_RANK_RATIO', 'PTS_RANK_TEAM1', 'PTS_RANK_TEAM2', 'PTS_RATIO', 'PTS_TEAM1', 'PTS_TEAM2', 'REB', 'REB_DIFF', 'REB_RANK', 'REB_RANK_DIFF', 'REB_RANK_RATIO', 'REB_RANK_TEAM1', 'REB_RANK_TEAM2', 'REB_RATIO', 'REB_TEAM1', 'REB_TEAM2', 'SEASON_YEAR', 'SEASON_YEAR_DIFF', 'SEASON_YEAR_RATIO', 'SEASON_YEAR_TEAM1', 'SEASON_YEAR_TEAM2', 'STL', 'STL_DIFF', 'STL_RANK', 'STL_RANK_DIFF', 'STL_RANK_RATIO', 'STL_RANK_TEAM1', 'STL_RANK_TEAM2', 'STL_RATIO', 'STL_TEAM1', 'STL_TEAM2', 'TEAM_ID', 'TEAM_ID_DIFF', 'TEAM_ID_RATIO', 'TEAM_ID_TEAM1', 'TEAM_ID_TEAM2', 'TOV', 'TOV_DIFF', 'TOV_RANK', 'TOV_RANK_DIFF', 'TOV_RANK_RATIO', 'TOV_RANK_TEAM1', 'TOV_RANK_TEAM2', 'TOV_RATIO', 'TOV_TEAM1', 'TOV_TEAM2', 'WL_DIFF', 'WL_RATIO', 'WL_TEAM1', 'WL_TEAM2', 'W_PCT_RANK', 'W_PCT_RANK_DIFF', 'W_PCT_RANK_RATIO', 'W_PCT_RANK_TEAM1', 'W_PCT_RANK_TEAM2', 'W_RANK', 'W_RANK_DIFF', 'W_RANK_RATIO', 'W_RANK_TEAM1', 'W_RANK_TEAM2']\n",
            "Epoch 1/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7410 - custom_accuracy: 0.5076 - loss: 0.4858 - val_accuracy: 0.9602 - val_custom_accuracy: 0.5134 - val_loss: 0.1065\n",
            "Epoch 2/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9757 - custom_accuracy: 0.5160 - loss: 0.0657 - val_accuracy: 0.9924 - val_custom_accuracy: 0.5136 - val_loss: 0.0270\n",
            "Epoch 3/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9849 - custom_accuracy: 0.5146 - loss: 0.0386 - val_accuracy: 0.9924 - val_custom_accuracy: 0.5137 - val_loss: 0.0253\n",
            "Epoch 4/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9954 - custom_accuracy: 0.5129 - loss: 0.0155 - val_accuracy: 0.9962 - val_custom_accuracy: 0.5144 - val_loss: 0.0116\n",
            "Epoch 5/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9907 - custom_accuracy: 0.5114 - loss: 0.0195 - val_accuracy: 0.9924 - val_custom_accuracy: 0.5129 - val_loss: 0.0129\n",
            "Epoch 6/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9969 - custom_accuracy: 0.5146 - loss: 0.0095 - val_accuracy: 0.9943 - val_custom_accuracy: 0.5131 - val_loss: 0.0136\n",
            "Epoch 7/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9944 - custom_accuracy: 0.5127 - loss: 0.0166 - val_accuracy: 0.9924 - val_custom_accuracy: 0.5141 - val_loss: 0.0304\n",
            "Epoch 8/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9967 - custom_accuracy: 0.5184 - loss: 0.0052 - val_accuracy: 0.9886 - val_custom_accuracy: 0.5133 - val_loss: 0.0242\n",
            "Epoch 9/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9975 - custom_accuracy: 0.5087 - loss: 0.0059 - val_accuracy: 0.9886 - val_custom_accuracy: 0.5147 - val_loss: 0.0517\n",
            "Epoch 10/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9926 - custom_accuracy: 0.5132 - loss: 0.0426 - val_accuracy: 0.9924 - val_custom_accuracy: 0.5142 - val_loss: 0.0120\n",
            "Epoch 11/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9972 - custom_accuracy: 0.5165 - loss: 0.0064 - val_accuracy: 0.9943 - val_custom_accuracy: 0.5146 - val_loss: 0.0106\n",
            "Epoch 12/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9976 - custom_accuracy: 0.5154 - loss: 0.0048 - val_accuracy: 0.9962 - val_custom_accuracy: 0.5149 - val_loss: 0.0092\n",
            "Epoch 13/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9961 - custom_accuracy: 0.5183 - loss: 0.0105 - val_accuracy: 1.0000 - val_custom_accuracy: 0.5139 - val_loss: 0.0017\n",
            "Epoch 14/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9990 - custom_accuracy: 0.5220 - loss: 0.0019 - val_accuracy: 0.9981 - val_custom_accuracy: 0.5137 - val_loss: 0.0108\n",
            "Epoch 15/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9974 - custom_accuracy: 0.5173 - loss: 0.0070 - val_accuracy: 1.0000 - val_custom_accuracy: 0.5139 - val_loss: 0.0025\n",
            "Epoch 16/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - custom_accuracy: 0.5155 - loss: 4.7629e-04 - val_accuracy: 0.9981 - val_custom_accuracy: 0.5137 - val_loss: 0.0077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to model.h5, Scaler saved to scaler.pkl\n",
            "Feature names saved to feature_names.pkl\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - custom_accuracy: 0.5123 - loss: 5.9632e-04 \n",
            "Neural Network - Test Loss: 0.0007558608776889741, Test Accuracy: 1.0, Custom Accuracy: 0.5194345116615295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nba_api.stats.static import teams\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "def load_feature_names(feature_names_file=\"feature_names.pkl\"):\n",
        "    \"\"\"\n",
        "    Load saved feature names for feature alignment during prediction.\n",
        "    \"\"\"\n",
        "    feature_names = joblib.load(feature_names_file)\n",
        "\n",
        "    # Print feature names\n",
        "    print(\"Feature Names Used in the Model:\")\n",
        "    print(feature_names)\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "\n",
        "def get_team_id_by_abbreviation(team_abbreviation):\n",
        "    \"\"\"Retrieve the team ID by abbreviation.\"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        if team[\"abbreviation\"].lower() == team_abbreviation.lower():\n",
        "            return team[\"id\"]\n",
        "    raise ValueError(f\"Team '{team_abbreviation}' not found! Please enter a valid abbreviation.\")\n",
        "\n",
        "\n",
        "def fetch_team_features(team_abbreviation, features_file, feature_names):\n",
        "    \"\"\"\n",
        "    Retrieve the team-specific features for the given team abbreviation.\n",
        "\n",
        "    Args:\n",
        "        team_abbreviation: Abbreviation of the NBA team (e.g., 'LAL').\n",
        "        features_file: CSV file containing the aggregated team features.\n",
        "        feature_names: List of features expected by the model.\n",
        "\n",
        "    Returns:\n",
        "        numpy array of the team's features.\n",
        "    \"\"\"\n",
        "    team_features = pd.read_csv(features_file)\n",
        "    team_row = team_features[team_features[\"TEAM\"] == team_abbreviation.upper()]\n",
        "\n",
        "    if team_row.empty:\n",
        "        raise ValueError(f\"Features for team '{team_abbreviation}' not found in {features_file}.\")\n",
        "\n",
        "    # Ensure only numeric features are returned\n",
        "    numeric_features = team_row[feature_names].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    return numeric_features.to_numpy().flatten()\n",
        "\n",
        "\n",
        "def predict_matchup_win_probability(team1_abbreviation, team2_abbreviation, features_file, model_path=\"model.h5\", scaler_path=\"scaler.pkl\"):\n",
        "    \"\"\"\n",
        "    Predict the win probability for Team 1 in a matchup against Team 2.\n",
        "\n",
        "    Args:\n",
        "        team1_abbreviation: Abbreviation of Team 1 (e.g., 'LAL').\n",
        "        team2_abbreviation: Abbreviation of Team 2 (e.g., 'BOS').\n",
        "        features_file: CSV file containing aggregated team features.\n",
        "        model_path: Path to the trained neural network model file.\n",
        "        scaler_path: Path to the scaler file for feature normalization.\n",
        "    \"\"\"\n",
        "    # Load model, scaler, and feature names\n",
        "    model = load_model(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    feature_names = load_feature_names()\n",
        "\n",
        "    # Fetch features for both teams\n",
        "    team1_features = fetch_team_features(team1_abbreviation, features_file, feature_names)\n",
        "    team2_features = fetch_team_features(team2_abbreviation, features_file, feature_names)\n",
        "\n",
        "    # Create matchup feature differences and ratios\n",
        "    matchup_features = np.concatenate([\n",
        "        team1_features - team2_features,\n",
        "        team1_features / (team2_features + 1e-5)  # Avoid division by zero\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Scale the matchup features\n",
        "    matchup_features_scaled = scaler.transform(matchup_features)\n",
        "\n",
        "    # Predict win probability for Team 1\n",
        "    win_probability = model.predict(matchup_features_scaled)[0][0]\n",
        "\n",
        "    print(f\"\\nWin Probability for {team1_abbreviation} vs {team2_abbreviation}: {win_probability * 100:.2f}%\")\n",
        "\n",
        "\n",
        "def display_team_data():\n",
        "    \"\"\"\n",
        "    Display available team abbreviations and names for user reference.\n",
        "    \"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    print(\"Available NBA Teams:\")\n",
        "    for team in nba_teams:\n",
        "        print(f\"{team['abbreviation']} - {team['full_name']}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to handle user input and prediction.\"\"\"\n",
        "    features_file = \"features.csv\"  # Path to the features file\n",
        "\n",
        "    # Display team data before taking user input\n",
        "    display_team_data()\n",
        "\n",
        "    team1_abbreviation = input(\"Enter Team 1 abbreviation (e.g., 'LAL' for Los Angeles Lakers): \").strip()\n",
        "    team2_abbreviation = input(\"Enter Team 2 abbreviation (e.g., 'BOS' for Boston Celtics): \").strip()\n",
        "\n",
        "    try:\n",
        "        predict_matchup_win_probability(team1_abbreviation, team2_abbreviation, features_file)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "heGJjGPVGuLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9473f034-01db-4c2f-c0da-a64159d4ac2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available NBA Teams:\n",
            "ATL - Atlanta Hawks\n",
            "BOS - Boston Celtics\n",
            "CLE - Cleveland Cavaliers\n",
            "NOP - New Orleans Pelicans\n",
            "CHI - Chicago Bulls\n",
            "DAL - Dallas Mavericks\n",
            "DEN - Denver Nuggets\n",
            "GSW - Golden State Warriors\n",
            "HOU - Houston Rockets\n",
            "LAC - Los Angeles Clippers\n",
            "LAL - Los Angeles Lakers\n",
            "MIA - Miami Heat\n",
            "MIL - Milwaukee Bucks\n",
            "MIN - Minnesota Timberwolves\n",
            "BKN - Brooklyn Nets\n",
            "NYK - New York Knicks\n",
            "ORL - Orlando Magic\n",
            "IND - Indiana Pacers\n",
            "PHI - Philadelphia 76ers\n",
            "PHX - Phoenix Suns\n",
            "POR - Portland Trail Blazers\n",
            "SAC - Sacramento Kings\n",
            "SAS - San Antonio Spurs\n",
            "OKC - Oklahoma City Thunder\n",
            "TOR - Toronto Raptors\n",
            "UTA - Utah Jazz\n",
            "MEM - Memphis Grizzlies\n",
            "WAS - Washington Wizards\n",
            "DET - Detroit Pistons\n",
            "CHA - Charlotte Hornets\n",
            "Enter Team 1 abbreviation (e.g., 'LAL' for Los Angeles Lakers): POR\n",
            "Enter Team 2 abbreviation (e.g., 'BOS' for Boston Celtics): SAS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names Used in the Model:\n",
            "['AST', 'AST_DIFF', 'AST_RANK', 'AST_RANK_DIFF', 'AST_RANK_RATIO', 'AST_RANK_TEAM1', 'AST_RANK_TEAM2', 'AST_RATIO', 'AST_TEAM1', 'AST_TEAM2', 'BLK', 'BLKA', 'BLKA_DIFF', 'BLKA_RANK', 'BLKA_RANK_DIFF', 'BLKA_RANK_RATIO', 'BLKA_RANK_TEAM1', 'BLKA_RANK_TEAM2', 'BLKA_RATIO', 'BLKA_TEAM1', 'BLKA_TEAM2', 'BLK_DIFF', 'BLK_RANK', 'BLK_RANK_DIFF', 'BLK_RANK_RATIO', 'BLK_RANK_TEAM1', 'BLK_RANK_TEAM2', 'BLK_RATIO', 'BLK_TEAM1', 'BLK_TEAM2', 'DREB', 'DREB_DIFF', 'DREB_RANK', 'DREB_RANK_DIFF', 'DREB_RANK_RATIO', 'DREB_RANK_TEAM1', 'DREB_RANK_TEAM2', 'DREB_RATIO', 'DREB_TEAM1', 'DREB_TEAM2', 'FG3A', 'FG3A_DIFF', 'FG3A_RANK', 'FG3A_RANK_DIFF', 'FG3A_RANK_RATIO', 'FG3A_RANK_TEAM1', 'FG3A_RANK_TEAM2', 'FG3A_RATIO', 'FG3A_TEAM1', 'FG3A_TEAM2', 'FG3M', 'FG3M_DIFF', 'FG3M_RANK', 'FG3M_RANK_DIFF', 'FG3M_RANK_RATIO', 'FG3M_RANK_TEAM1', 'FG3M_RANK_TEAM2', 'FG3M_RATIO', 'FG3M_TEAM1', 'FG3M_TEAM2', 'FG3_PCT', 'FG3_PCT_DIFF', 'FG3_PCT_RANK', 'FG3_PCT_RANK_DIFF', 'FG3_PCT_RANK_RATIO', 'FG3_PCT_RANK_TEAM1', 'FG3_PCT_RANK_TEAM2', 'FG3_PCT_RATIO', 'FG3_PCT_TEAM1', 'FG3_PCT_TEAM2', 'FGA', 'FGA_DIFF', 'FGA_RANK', 'FGA_RANK_DIFF', 'FGA_RANK_RATIO', 'FGA_RANK_TEAM1', 'FGA_RANK_TEAM2', 'FGA_RATIO', 'FGA_TEAM1', 'FGA_TEAM2', 'FGM', 'FGM_DIFF', 'FGM_RANK', 'FGM_RANK_DIFF', 'FGM_RANK_RATIO', 'FGM_RANK_TEAM1', 'FGM_RANK_TEAM2', 'FGM_RATIO', 'FGM_TEAM1', 'FGM_TEAM2', 'FG_PCT', 'FG_PCT_DIFF', 'FG_PCT_RANK', 'FG_PCT_RANK_DIFF', 'FG_PCT_RANK_RATIO', 'FG_PCT_RANK_TEAM1', 'FG_PCT_RANK_TEAM2', 'FG_PCT_RATIO', 'FG_PCT_TEAM1', 'FG_PCT_TEAM2', 'FTA', 'FTA_DIFF', 'FTA_RANK', 'FTA_RANK_DIFF', 'FTA_RANK_RATIO', 'FTA_RANK_TEAM1', 'FTA_RANK_TEAM2', 'FTA_RATIO', 'FTA_TEAM1', 'FTA_TEAM2', 'FTM', 'FTM_DIFF', 'FTM_RANK', 'FTM_RANK_DIFF', 'FTM_RANK_RATIO', 'FTM_RANK_TEAM1', 'FTM_RANK_TEAM2', 'FTM_RATIO', 'FTM_TEAM1', 'FTM_TEAM2', 'FT_PCT', 'FT_PCT_DIFF', 'FT_PCT_RANK', 'FT_PCT_RANK_DIFF', 'FT_PCT_RANK_RATIO', 'FT_PCT_RANK_TEAM1', 'FT_PCT_RANK_TEAM2', 'FT_PCT_RATIO', 'FT_PCT_TEAM1', 'FT_PCT_TEAM2', 'GP_RANK', 'GP_RANK_DIFF', 'GP_RANK_RATIO', 'GP_RANK_TEAM1', 'GP_RANK_TEAM2', 'L_RANK', 'L_RANK_DIFF', 'L_RANK_RATIO', 'L_RANK_TEAM1', 'L_RANK_TEAM2', 'MIN', 'MIN_DIFF', 'MIN_RANK', 'MIN_RANK_DIFF', 'MIN_RANK_RATIO', 'MIN_RANK_TEAM1', 'MIN_RANK_TEAM2', 'MIN_RATIO', 'MIN_TEAM1', 'MIN_TEAM2', 'OREB', 'OREB_DIFF', 'OREB_RANK', 'OREB_RANK_DIFF', 'OREB_RANK_RATIO', 'OREB_RANK_TEAM1', 'OREB_RANK_TEAM2', 'OREB_RATIO', 'OREB_TEAM1', 'OREB_TEAM2', 'PF', 'PFD', 'PFD_DIFF', 'PFD_RANK', 'PFD_RANK_DIFF', 'PFD_RANK_RATIO', 'PFD_RANK_TEAM1', 'PFD_RANK_TEAM2', 'PFD_RATIO', 'PFD_TEAM1', 'PFD_TEAM2', 'PF_DIFF', 'PF_RANK', 'PF_RANK_DIFF', 'PF_RANK_RATIO', 'PF_RANK_TEAM1', 'PF_RANK_TEAM2', 'PF_RATIO', 'PF_TEAM1', 'PF_TEAM2', 'PLUS_MINUS', 'PLUS_MINUS_DIFF', 'PLUS_MINUS_RANK', 'PLUS_MINUS_RANK_DIFF', 'PLUS_MINUS_RANK_RATIO', 'PLUS_MINUS_RANK_TEAM1', 'PLUS_MINUS_RANK_TEAM2', 'PLUS_MINUS_RATIO', 'PLUS_MINUS_TEAM1', 'PLUS_MINUS_TEAM2', 'PTS', 'PTS_DIFF', 'PTS_RANK', 'PTS_RANK_DIFF', 'PTS_RANK_RATIO', 'PTS_RANK_TEAM1', 'PTS_RANK_TEAM2', 'PTS_RATIO', 'PTS_TEAM1', 'PTS_TEAM2', 'REB', 'REB_DIFF', 'REB_RANK', 'REB_RANK_DIFF', 'REB_RANK_RATIO', 'REB_RANK_TEAM1', 'REB_RANK_TEAM2', 'REB_RATIO', 'REB_TEAM1', 'REB_TEAM2', 'SEASON_YEAR', 'SEASON_YEAR_DIFF', 'SEASON_YEAR_RATIO', 'SEASON_YEAR_TEAM1', 'SEASON_YEAR_TEAM2', 'STL', 'STL_DIFF', 'STL_RANK', 'STL_RANK_DIFF', 'STL_RANK_RATIO', 'STL_RANK_TEAM1', 'STL_RANK_TEAM2', 'STL_RATIO', 'STL_TEAM1', 'STL_TEAM2', 'TEAM_ID', 'TEAM_ID_DIFF', 'TEAM_ID_RATIO', 'TEAM_ID_TEAM1', 'TEAM_ID_TEAM2', 'TOV', 'TOV_DIFF', 'TOV_RANK', 'TOV_RANK_DIFF', 'TOV_RANK_RATIO', 'TOV_RANK_TEAM1', 'TOV_RANK_TEAM2', 'TOV_RATIO', 'TOV_TEAM1', 'TOV_TEAM2', 'WL_DIFF', 'WL_RATIO', 'WL_TEAM1', 'WL_TEAM2', 'W_PCT_RANK', 'W_PCT_RANK_DIFF', 'W_PCT_RANK_RATIO', 'W_PCT_RANK_TEAM1', 'W_PCT_RANK_TEAM2', 'W_RANK', 'W_RANK_DIFF', 'W_RANK_RATIO', 'W_RANK_TEAM1', 'W_RANK_TEAM2']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['AST_DIFF', 'AST_RANK_DIFF', 'AST_RANK_RATIO', 'AST_RANK_TEAM1', 'AST_RANK_TEAM2', 'AST_RATIO', 'AST_TEAM1', 'AST_TEAM2', 'BLKA_DIFF', 'BLKA_RANK_DIFF', 'BLKA_RANK_RATIO', 'BLKA_RANK_TEAM1', 'BLKA_RANK_TEAM2', 'BLKA_RATIO', 'BLKA_TEAM1', 'BLKA_TEAM2', 'BLK_DIFF', 'BLK_RANK_DIFF', 'BLK_RANK_RATIO', 'BLK_RANK_TEAM1', 'BLK_RANK_TEAM2', 'BLK_RATIO', 'BLK_TEAM1', 'BLK_TEAM2', 'DREB_DIFF', 'DREB_RANK_DIFF', 'DREB_RANK_RATIO', 'DREB_RANK_TEAM1', 'DREB_RANK_TEAM2', 'DREB_RATIO', 'DREB_TEAM1', 'DREB_TEAM2', 'FG3A_DIFF', 'FG3A_RANK_DIFF', 'FG3A_RANK_RATIO', 'FG3A_RANK_TEAM1', 'FG3A_RANK_TEAM2', 'FG3A_RATIO', 'FG3A_TEAM1', 'FG3A_TEAM2', 'FG3M_DIFF', 'FG3M_RANK_DIFF', 'FG3M_RANK_RATIO', 'FG3M_RANK_TEAM1', 'FG3M_RANK_TEAM2', 'FG3M_RATIO', 'FG3M_TEAM1', 'FG3M_TEAM2', 'FG3_PCT_DIFF', 'FG3_PCT_RANK_DIFF', 'FG3_PCT_RANK_RATIO', 'FG3_PCT_RANK_TEAM1', 'FG3_PCT_RANK_TEAM2', 'FG3_PCT_RATIO', 'FG3_PCT_TEAM1', 'FG3_PCT_TEAM2', 'FGA_DIFF', 'FGA_RANK_DIFF', 'FGA_RANK_RATIO', 'FGA_RANK_TEAM1', 'FGA_RANK_TEAM2', 'FGA_RATIO', 'FGA_TEAM1', 'FGA_TEAM2', 'FGM_DIFF', 'FGM_RANK_DIFF', 'FGM_RANK_RATIO', 'FGM_RANK_TEAM1', 'FGM_RANK_TEAM2', 'FGM_RATIO', 'FGM_TEAM1', 'FGM_TEAM2', 'FG_PCT_DIFF', 'FG_PCT_RANK_DIFF', 'FG_PCT_RANK_RATIO', 'FG_PCT_RANK_TEAM1', 'FG_PCT_RANK_TEAM2', 'FG_PCT_RATIO', 'FG_PCT_TEAM1', 'FG_PCT_TEAM2', 'FTA_DIFF', 'FTA_RANK_DIFF', 'FTA_RANK_RATIO', 'FTA_RANK_TEAM1', 'FTA_RANK_TEAM2', 'FTA_RATIO', 'FTA_TEAM1', 'FTA_TEAM2', 'FTM_DIFF', 'FTM_RANK_DIFF', 'FTM_RANK_RATIO', 'FTM_RANK_TEAM1', 'FTM_RANK_TEAM2', 'FTM_RATIO', 'FTM_TEAM1', 'FTM_TEAM2', 'FT_PCT_DIFF', 'FT_PCT_RANK_DIFF', 'FT_PCT_RANK_RATIO', 'FT_PCT_RANK_TEAM1', 'FT_PCT_RANK_TEAM2', 'FT_PCT_RATIO', 'FT_PCT_TEAM1', 'FT_PCT_TEAM2', 'GP_RANK_DIFF', 'GP_RANK_RATIO', 'GP_RANK_TEAM1', 'GP_RANK_TEAM2', 'L_RANK_DIFF', 'L_RANK_RATIO', 'L_RANK_TEAM1', 'L_RANK_TEAM2', 'MIN_DIFF', 'MIN_RANK_DIFF', 'MIN_RANK_RATIO', 'MIN_RANK_TEAM1', 'MIN_RANK_TEAM2', 'MIN_RATIO', 'MIN_TEAM1', 'MIN_TEAM2', 'OREB_DIFF', 'OREB_RANK_DIFF', 'OREB_RANK_RATIO', 'OREB_RANK_TEAM1', 'OREB_RANK_TEAM2', 'OREB_RATIO', 'OREB_TEAM1', 'OREB_TEAM2', 'PFD_DIFF', 'PFD_RANK_DIFF', 'PFD_RANK_RATIO', 'PFD_RANK_TEAM1', 'PFD_RANK_TEAM2', 'PFD_RATIO', 'PFD_TEAM1', 'PFD_TEAM2', 'PF_DIFF', 'PF_RANK_DIFF', 'PF_RANK_RATIO', 'PF_RANK_TEAM1', 'PF_RANK_TEAM2', 'PF_RATIO', 'PF_TEAM1', 'PF_TEAM2', 'PLUS_MINUS_DIFF', 'PLUS_MINUS_RANK_DIFF', 'PLUS_MINUS_RANK_RATIO', 'PLUS_MINUS_RANK_TEAM1', 'PLUS_MINUS_RANK_TEAM2', 'PLUS_MINUS_RATIO', 'PLUS_MINUS_TEAM1', 'PLUS_MINUS_TEAM2', 'PTS_DIFF', 'PTS_RANK_DIFF', 'PTS_RANK_RATIO', 'PTS_RANK_TEAM1', 'PTS_RANK_TEAM2', 'PTS_RATIO', 'PTS_TEAM1', 'PTS_TEAM2', 'REB_DIFF', 'REB_RANK_DIFF', 'REB_RANK_RATIO', 'REB_RANK_TEAM1', 'REB_RANK_TEAM2', 'REB_RATIO', 'REB_TEAM1', 'REB_TEAM2', 'SEASON_YEAR_DIFF', 'SEASON_YEAR_RATIO', 'SEASON_YEAR_TEAM1', 'SEASON_YEAR_TEAM2', 'STL_DIFF', 'STL_RANK_DIFF', 'STL_RANK_RATIO', 'STL_RANK_TEAM1', 'STL_RANK_TEAM2', 'STL_RATIO', 'STL_TEAM1', 'STL_TEAM2', 'TEAM_ID_DIFF', 'TEAM_ID_RATIO', 'TEAM_ID_TEAM1', 'TEAM_ID_TEAM2', 'TOV_DIFF', 'TOV_RANK_DIFF', 'TOV_RANK_RATIO', 'TOV_RANK_TEAM1', 'TOV_RANK_TEAM2', 'TOV_RATIO', 'TOV_TEAM1', 'TOV_TEAM2', 'WL_DIFF', 'WL_RATIO', 'WL_TEAM1', 'WL_TEAM2', 'W_PCT_RANK_DIFF', 'W_PCT_RANK_RATIO', 'W_PCT_RANK_TEAM1', 'W_PCT_RANK_TEAM2', 'W_RANK_DIFF', 'W_RANK_RATIO', 'W_RANK_TEAM1', 'W_RANK_TEAM2'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8e1c78713532>\u001b[0m in \u001b[0;36m<cell line: 115>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-8e1c78713532>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mpredict_matchup_win_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteam1_abbreviation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam2_abbreviation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8e1c78713532>\u001b[0m in \u001b[0;36mpredict_matchup_win_probability\u001b[0;34m(team1_abbreviation, team2_abbreviation, features_file, model_path, scaler_path)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Fetch features for both teams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mteam1_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_team_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteam1_abbreviation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mteam2_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_team_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteam2_abbreviation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8e1c78713532>\u001b[0m in \u001b[0;36mfetch_team_features\u001b[0;34m(team_abbreviation, features_file, feature_names)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Ensure only numeric features are returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mnumeric_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteam_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumeric_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['AST_DIFF', 'AST_RANK_DIFF', 'AST_RANK_RATIO', 'AST_RANK_TEAM1', 'AST_RANK_TEAM2', 'AST_RATIO', 'AST_TEAM1', 'AST_TEAM2', 'BLKA_DIFF', 'BLKA_RANK_DIFF', 'BLKA_RANK_RATIO', 'BLKA_RANK_TEAM1', 'BLKA_RANK_TEAM2', 'BLKA_RATIO', 'BLKA_TEAM1', 'BLKA_TEAM2', 'BLK_DIFF', 'BLK_RANK_DIFF', 'BLK_RANK_RATIO', 'BLK_RANK_TEAM1', 'BLK_RANK_TEAM2', 'BLK_RATIO', 'BLK_TEAM1', 'BLK_TEAM2', 'DREB_DIFF', 'DREB_RANK_DIFF', 'DREB_RANK_RATIO', 'DREB_RANK_TEAM1', 'DREB_RANK_TEAM2', 'DREB_RATIO', 'DREB_TEAM1', 'DREB_TEAM2', 'FG3A_DIFF', 'FG3A_RANK_DIFF', 'FG3A_RANK_RATIO', 'FG3A_RANK_TEAM1', 'FG3A_RANK_TEAM2', 'FG3A_RATIO', 'FG3A_TEAM1', 'FG3A_TEAM2', 'FG3M_DIFF', 'FG3M_RANK_DIFF', 'FG3M_RANK_RATIO', 'FG3M_RANK_TEAM1', 'FG3M_RANK_TEAM2', 'FG3M_RATIO', 'FG3M_TEAM1', 'FG3M_TEAM2', 'FG3_PCT_DIFF', 'FG3_PCT_RANK_DIFF', 'FG3_PCT_RANK_RATIO', 'FG3_PCT_RANK_TEAM1', 'FG3_PCT_RANK_TEAM2', 'FG3_PCT_RATIO', 'FG3_PCT_TEAM1', 'FG3_PCT_TEAM2', 'FGA_DIFF', 'FGA_RANK_DIFF', 'FGA_RANK_RATIO', 'FGA_RANK_TEAM1', 'FGA_RANK_TEAM2', 'FGA_RATIO', 'FGA_TEAM1', 'FGA_TEAM2', 'FGM_DIFF', 'FGM_RANK_DIFF', 'FGM_RANK_RATIO', 'FGM_RANK_TEAM1', 'FGM_RANK_TEAM2', 'FGM_RATIO', 'FGM_TEAM1', 'FGM_TEAM2', 'FG_PCT_DIFF', 'FG_PCT_RANK_DIFF', 'FG_PCT_RANK_RATIO', 'FG_PCT_RANK_TEAM1', 'FG_PCT_RANK_TEAM2', 'FG_PCT_RATIO', 'FG_PCT_TEAM1', 'FG_PCT_TEAM2', 'FTA_DIFF', 'FTA_RANK_DIFF', 'FTA_RANK_RATIO', 'FTA_RANK_TEAM1', 'FTA_RANK_TEAM2', 'FTA_RATIO', 'FTA_TEAM1', 'FTA_TEAM2', 'FTM_DIFF', 'FTM_RANK_DIFF', 'FTM_RANK_RATIO..."
          ]
        }
      ]
    }
  ]
}