{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFwkNpSLupb/6FUjAcqPtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "d5a1ee7d-b39f-4877-fde9-590a6d5719df"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nba_api in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs, LeagueGameFinder, LeagueLeaders, PlayerCareerStats\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "# Maximum number of retries for each API call\n",
        "MAX_RETRIES = 3\n",
        "# Define the list of seasons\n",
        "# Generate all seasons from 2013 onwards\n",
        "start_year = 2024\n",
        "end_year = 2024  # Adjust to your desired year\n",
        "seasons = [f\"{year}-{(year + 1)%100}\" for year in range(start_year, end_year + 1)]\n",
        "\n",
        "# Printing seasons to verify\n",
        "print(seasons)\n"
      ],
      "metadata": {
        "id": "SJ2v4Yzgep72",
        "outputId": "ea836cbf-a43a-4a33-9902-bd72b2fae8c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2024-25']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt  # Exponential backoff\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "DV6BxvlTerjE"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_info(seasons):\n",
        "    \"\"\"Fetches relevant team information for the specified seasons.\"\"\"\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for team in nba_teams:\n",
        "        team_info = fetch_with_retries(\n",
        "            TeamInfoCommon,\n",
        "            team_id=team[\"id\"],\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if team_info:\n",
        "            df_team = team_info.get_data_frames()[0]\n",
        "            df_team = df_team[[\"TEAM_ID\", \"TEAM_ABBREVIATION\"]]  # Only keep relevant features\n",
        "            team_data.append(df_team)\n",
        "            time.sleep(0.6)  # Delay to avoid API rate limits\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "# Run functions to save data to CSV files\n",
        "get_team_info(seasons)\n",
        "print(\"Team information data stored.\")"
      ],
      "metadata": {
        "id": "uflUFIR0et8B",
        "outputId": "9a396c02-2e08-4145-bfcc-1b4fdc18bf68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Team information data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_game_logs(seasons):\n",
        "    \"\"\"Fetches team game logs for the specified seasons and processes the MATCHUP column.\"\"\"\n",
        "    print(\"Fetching team game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            # Keep only relevant columns\n",
        "            df_game_logs = df_game_logs[[\"GAME_ID\", \"GAME_DATE\", \"MATCHUP\", \"WL\"]]\n",
        "            game_log_data.append(df_game_logs)\n",
        "            time.sleep(0.6)  # Delay to respect rate limits\n",
        "\n",
        "    if game_log_data:\n",
        "        # Concatenate all game logs\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "\n",
        "        # Process MATCHUP column to create team1 and team2 columns\n",
        "        matchups_split = df_all_game_logs['MATCHUP'].str.split(' @ | vs. ', expand=True)\n",
        "        df_all_game_logs['TEAM1'] = matchups_split[0]\n",
        "        df_all_game_logs['TEAM2'] = matchups_split[1]\n",
        "\n",
        "        # Drop the original MATCHUP column if no longer needed\n",
        "        df_all_game_logs.drop(columns=['MATCHUP'], inplace=True)\n",
        "\n",
        "        # Extract and add SEASON_YEAR\n",
        "        df_all_game_logs['SEASON_YEAR'] = pd.to_datetime(df_all_game_logs['GAME_DATE']).dt.year.astype(str)\n",
        "\n",
        "        # Create combined TEAM_SEASON columns\n",
        "        df_all_game_logs['TEAM_SEASON1'] = df_all_game_logs['TEAM1'] + ':' + df_all_game_logs['SEASON_YEAR']\n",
        "        df_all_game_logs['TEAM_SEASON2'] = df_all_game_logs['TEAM2'] + ':' + df_all_game_logs['SEASON_YEAR']\n",
        "\n",
        "        # Drop the original TEAM1, TEAM2, and SEASON_YEAR columns if no longer needed\n",
        "        df_all_game_logs.drop(columns=['TEAM1', 'TEAM2', 'SEASON_YEAR'], inplace=True)\n",
        "\n",
        "        # Save the processed DataFrame to a CSV file\n",
        "        df_all_game_logs.to_csv(\"nba_game_logs.csv\", index=False)\n",
        "        print(\"Processed game logs saved to 'nba_game_logs.csv'.\")\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "get_team_game_logs(seasons)\n",
        "print(\"Team game logs data stored.\")\n"
      ],
      "metadata": {
        "id": "8r9Z1Q8Hev9-",
        "outputId": "d95baeaa-89e4-41af-fa22-ddcfab2893db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team game logs...\n",
            "Processed game logs saved to 'nba_game_logs.csv'.\n",
            "Team game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_player_game_logs(seasons):\n",
        "    \"\"\"Fetches player game logs for the specified seasons.\"\"\"\n",
        "    print(\"Fetching player game logs...\")\n",
        "    player_game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        player_game_logs = fetch_with_retries(\n",
        "            PlayerGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if player_game_logs:\n",
        "            df_player_game_logs = player_game_logs.get_data_frames()[0]\n",
        "            # Keep only relevant columns\n",
        "            df_player_game_logs = df_player_game_logs[[\n",
        "                \"SEASON_YEAR\", \"GAME_ID\", \"TEAM_ID\", \"PLAYER_ID\", \"PLAYER_NAME\", \"PTS\", \"REB\", \"AST\", \"STL\", \"BLK\",\n",
        "                \"MIN\", \"FG_PCT\", \"FG3_PCT\", \"FT_PCT\", \"TOV\", \"PF\"\n",
        "            ]]\n",
        "            # Modify SEASON_YEAR to keep only the first 4 characters\n",
        "            df_player_game_logs[\"SEASON_YEAR\"] = df_player_game_logs[\"SEASON_YEAR\"].str[:4]\n",
        "            player_game_log_data.append(df_player_game_logs)\n",
        "            time.sleep(0.6)\n",
        "\n",
        "    if player_game_log_data:\n",
        "        df_all_player_game_logs = pd.concat(player_game_log_data, ignore_index=True)\n",
        "        df_all_player_game_logs.to_csv(\"nba_player_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player game log data fetched.\")\n",
        "\n",
        "# Call the function with the specified seasons\n",
        "get_player_game_logs(seasons)\n",
        "print(\"Player game logs data stored.\")"
      ],
      "metadata": {
        "id": "mX-tQtyvex59",
        "outputId": "240074ca-ee89-46bc-b85b-0194557b08e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching player game logs...\n",
            "Player game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_league_game_data():\n",
        "    \"\"\"Fetches league-wide game data with relevant features for a neural network.\"\"\"\n",
        "    print(\"Fetching league game data for NN...\")\n",
        "    game_data = fetch_with_retries(LeagueGameFinder, timeout=60)\n",
        "    if game_data:\n",
        "        df_game_data = game_data.get_data_frames()[0]\n",
        "        # Relevant columns for neural network input\n",
        "        relevant_columns = [\n",
        "            \"SEASON_ID\", \"TEAM_ID\", \"TEAM_ABBREVIATION\", \"TEAM_NAME\", \"GAME_ID\",\n",
        "            \"GAME_DATE\", \"MATCHUP\", \"WL\", \"MIN\", \"PTS\", \"FGM\", \"FGA\", \"FG_PCT\",\n",
        "            \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"OREB\", \"DREB\",\n",
        "            \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PLUS_MINUS\"\n",
        "        ]\n",
        "        df_nn_data = df_game_data[relevant_columns]\n",
        "        df_nn_data.to_csv(\"nba_league_game.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No league game data fetched.\")\n",
        "\n",
        "get_league_game_data()\n",
        "print(\"League game data stored.\")"
      ],
      "metadata": {
        "id": "AQfHSeRyezmh",
        "outputId": "fb1beb76-6b41-442a-d37a-f15ff104eeb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching league game data for NN...\n",
            "League game data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_league_leaders():\n",
        "    \"\"\"Fetches league leaders data with relevant columns for analysis.\"\"\"\n",
        "    print(\"Fetching league leaders data...\")\n",
        "    leaders_data = fetch_with_retries(LeagueLeaders, timeout=60)\n",
        "    if leaders_data:\n",
        "        df_leaders = leaders_data.get_data_frames()[0]\n",
        "        # Select only relevant columns\n",
        "        relevant_columns = [\n",
        "            \"PLAYER_ID\", \"PLAYER\", \"TEAM_ID\", \"TEAM\", \"GP\", \"MIN\", \"FGM\", \"FGA\",\n",
        "            \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"OREB\",\n",
        "            \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PTS\", \"EFF\"\n",
        "        ]\n",
        "        df_relevant_leaders = df_leaders[relevant_columns]\n",
        "        df_relevant_leaders.to_csv(\"nba_league_leaders_relevant.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No league leaders data fetched.\")\n",
        "\n",
        "get_league_leaders()\n",
        "print(\"League leaders data stored.\")"
      ],
      "metadata": {
        "id": "nemrfltYe1I-",
        "outputId": "b9dc888e-a2a4-4633-ba76-e8c8a05daa31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching league leaders data...\n",
            "League leaders data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_player_career_stats():\n",
        "    \"\"\"Fetches career stats for players.\"\"\"\n",
        "    print(\"Fetching player career stats...\")\n",
        "    career_stats_data = []\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        players = team.get(\"players\", [])\n",
        "        for player in players:\n",
        "            career_stats = fetch_with_retries(PlayerCareerStats, player_id=player[\"id\"], timeout=60)\n",
        "            if career_stats:\n",
        "                df_career_stats = career_stats.get_data_frames()[0]\n",
        "                # Keep only relevant columns\n",
        "                df_career_stats = df_career_stats[[\n",
        "                    \"PLAYER_ID\", \"PLAYER_NAME\", \"GP\", \"PTS\", \"REB\", \"AST\", \"FG_PCT\", \"FG3_PCT\", \"FT_PCT\"\n",
        "                ]]\n",
        "                career_stats_data.append(df_career_stats)\n",
        "                time.sleep(0.6)\n",
        "\n",
        "    if career_stats_data:\n",
        "        df_all_career_stats = pd.concat(career_stats_data, ignore_index=True)\n",
        "        df_all_career_stats.to_csv(\"nba_player_career_stats.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player career stats data fetched.\")\n",
        "\n",
        "get_player_career_stats()\n",
        "print(\"Player career stats data stored.\")"
      ],
      "metadata": {
        "id": "MBwUaa-We2i8",
        "outputId": "aa243c55-54ac-428b-9059-83a0b00bd2e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching player career stats...\n",
            "No player career stats data fetched.\n",
            "Player career stats data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load player game logs from the CSV file\n",
        "file_path = \"nba_player_game_logs.csv\"  # Update with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Exclude non-numerical columns explicitly\n",
        "no_aggregate_columns = ['PLAYER_ID', 'SEASON_YEAR', 'PLAYER_NAME', 'TEAM_ID', 'GAME_ID']  # Adjust as necessary\n",
        "numerical_columns = [col for col in df.columns if col not in no_aggregate_columns]\n",
        "\n",
        "# Group by PLAYER_ID and SEASON_YEAR\n",
        "grouped = df.groupby(['PLAYER_ID', 'SEASON_YEAR'])\n",
        "\n",
        "# Aggregate numerical columns with mean, std, median, and variance\n",
        "aggregated_data = grouped[numerical_columns].agg(['mean', 'std', 'median', 'var']).reset_index()\n",
        "\n",
        "# Flatten multi-level columns\n",
        "aggregated_data.columns = ['_'.join(col).strip('_') for col in aggregated_data.columns]\n",
        "\n",
        "# Add coefficient of variation (CV) separately\n",
        "for col in numerical_columns:\n",
        "    col_mean = f\"{col}_mean\"\n",
        "    col_std = f\"{col}_std\"\n",
        "    col_cv = f\"{col}_cv\"\n",
        "    aggregated_data[col_cv] = aggregated_data[col_std] / aggregated_data[col_mean]\n",
        "    aggregated_data[col_cv] = aggregated_data[col_cv].replace([float('inf'), -float('inf')], None)  # Handle division by zero\n",
        "\n",
        "# Add non-numerical columns using the first value in the group (like TEAM_ID)\n",
        "aggregated_data['TEAM_ID'] = grouped['TEAM_ID'].first().values\n",
        "\n",
        "# Add games played as a new column\n",
        "aggregated_data['GAMES_PLAYED'] = grouped.size().values\n",
        "\n",
        "# Save the aggregated data for further use\n",
        "output_path = \"nba_player_aggregated_data.csv\"\n",
        "aggregated_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Aggregated data saved to '{output_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dTm6jXFeV36",
        "outputId": "6d387e0a-ea98-44e2-aaba-616004eebe79"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data saved to 'nba_player_aggregated_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load aggregated player data\n",
        "player_aggregated_file = \"nba_player_aggregated_data.csv\"  # Update with your actual file path\n",
        "team_abbreviation_file = \"nba_team_data.csv\"  # File containing TEAM_ID to TEAM_ABBREVIATION mapping\n",
        "\n",
        "# Load player data and team abbreviation mapping\n",
        "player_df = pd.read_csv(player_aggregated_file)\n",
        "team_data_df = pd.read_csv(team_abbreviation_file)\n",
        "\n",
        "# Ensure 'MIN_mean' column exists\n",
        "if 'MIN_mean' not in player_df.columns:\n",
        "    raise KeyError(\"The 'MIN_mean' column is missing from the player data. Please verify the input file.\")\n",
        "\n",
        "# Merge team abbreviations into player data\n",
        "player_df = player_df.merge(team_data_df[['TEAM_ID', 'TEAM_ABBREVIATION']], on=\"TEAM_ID\", how=\"left\")\n",
        "\n",
        "# Combine TEAM_ABBREVIATION and SEASON_YEAR into a new column\n",
        "player_df['TEAM_SEASON'] = player_df['TEAM_ABBREVIATION'] + \":\" + player_df['SEASON_YEAR'].astype(str)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['TEAM_ABBREVIATION', 'SEASON_YEAR']\n",
        "if 'GAME_ID' in player_df.columns:  # Check if 'GAME_ID' exists\n",
        "    columns_to_drop.append('GAME_ID')\n",
        "\n",
        "player_df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Define non-numerical columns to exclude\n",
        "no_aggregate_columns = ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_SEASON']\n",
        "numerical_columns = [col for col in player_df.columns if col not in no_aggregate_columns]\n",
        "\n",
        "# Weight each player's stats by their 'MIN_mean'\n",
        "for col in numerical_columns:\n",
        "    if col != 'MIN_mean':  # Avoid weighting the 'MIN_mean' column by itself\n",
        "        player_df[f\"{col}_WEIGHTED\"] = player_df[col] * player_df['MIN_mean']\n",
        "\n",
        "# Group by TEAM_SEASON\n",
        "grouped = player_df.groupby(['TEAM_SEASON'])\n",
        "\n",
        "# Compute team-level weighted averages\n",
        "weighted_stats = grouped[[f\"{col}_WEIGHTED\" for col in numerical_columns if col != 'MIN_mean']].sum()\n",
        "total_minutes = grouped['MIN_mean'].sum()\n",
        "\n",
        "# Calculate weighted averages by dividing the sum of weighted stats by total minutes\n",
        "team_weighted_avg = weighted_stats.div(total_minutes, axis=0)\n",
        "\n",
        "# Rename columns back to their original names\n",
        "team_weighted_avg.columns = [col.replace('_WEIGHTED', '') for col in team_weighted_avg.columns]\n",
        "\n",
        "# Add additional columns\n",
        "team_weighted_avg['TOTAL_MIN'] = total_minutes\n",
        "team_weighted_avg['TEAM_GAMES_PLAYED'] = grouped['GAMES_PLAYED'].sum()\n",
        "\n",
        "# Reset index to flatten the DataFrame\n",
        "team_weighted_avg.reset_index(inplace=True)\n",
        "\n",
        "# Save the aggregated data for further use\n",
        "output_path = \"nba_team_aggregated_data.csv\"\n",
        "team_weighted_avg.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Team aggregated data saved to '{output_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojGa1FcvBwsO",
        "outputId": "1b35f208-8366-42c5-f57b-9f67295ff890"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team aggregated data saved to 'nba_team_aggregated_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom accuracy metric to evaluate the model based on given conditions.\n",
        "    \"\"\"\n",
        "    condition_1 = K.cast(y_pred < 0.5, dtype=\"float32\") * K.cast(y_true == 0, dtype=\"float32\")\n",
        "    condition_2 = K.cast(y_pred >= 0.5, dtype=\"float32\") * K.cast(y_true == 1, dtype=\"float32\")\n",
        "    return K.mean(condition_1 + condition_2)\n",
        "\n",
        "\n",
        "def build_neural_network(input_shape):\n",
        "    \"\"\"\n",
        "    Build a neural network model with added regularization and improved architecture.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[custom_accuracy])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"\n",
        "    Train a neural network model.\n",
        "    \"\"\"\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2)\n",
        "\n",
        "    # Build the model\n",
        "    model = build_neural_network(X_train.shape[1])\n",
        "\n",
        "    # Early stopping and learning rate scheduler\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=80, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train,\n",
        "              epochs=150,\n",
        "              batch_size=4,  # Reduced batch size\n",
        "              validation_data=(X_val, y_val),\n",
        "              callbacks=[early_stopping],\n",
        "              verbose=2)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "# Modify prepare_dataset for improved feature scaling and transformations\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using transformed features (difference and ratio).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load game logs and team features\n",
        "        game_logs = pd.read_csv(game_logs_file, parse_dates=[\"GAME_DATE\"])\n",
        "        team_features = pd.read_csv(features_file)\n",
        "\n",
        "        # Normalize key columns\n",
        "        game_logs[\"TEAM_SEASON1\"] = game_logs[\"TEAM_SEASON1\"].str.strip().str.upper()\n",
        "        game_logs[\"TEAM_SEASON2\"] = game_logs[\"TEAM_SEASON2\"].str.strip().str.upper()\n",
        "\n",
        "        # Merge features for TEAM1 and TEAM2\n",
        "        game_logs = game_logs.merge(\n",
        "            team_features.add_suffix(\"_TEAM1\"),\n",
        "            left_on=[\"TEAM_SEASON1\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM1\"],\n",
        "            how=\"left\"\n",
        "        ).merge(\n",
        "            team_features.add_suffix(\"_TEAM2\"),\n",
        "            left_on=[\"TEAM_SEASON2\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM2\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = [\"TEAM_SEASON_TEAM1\", \"TEAM_SEASON_TEAM2\", \"GAME_DATE\"]\n",
        "        game_logs.drop(columns=[col for col in columns_to_drop if col in game_logs.columns], inplace=True)\n",
        "\n",
        "        # Handle missing values\n",
        "        game_logs.fillna(0, inplace=True)\n",
        "\n",
        "        # Extract features and target\n",
        "        feature_columns_team1 = [col for col in game_logs.columns if col.endswith(\"_TEAM1\")]\n",
        "        feature_columns_team2 = [col.replace(\"_TEAM1\", \"_TEAM2\") for col in feature_columns_team1]\n",
        "\n",
        "        # Ensure column alignment\n",
        "        feature_columns_team2 = [col for col in feature_columns_team2 if col in game_logs.columns]\n",
        "\n",
        "        # Standardize team features before transformations\n",
        "        scaler = StandardScaler()\n",
        "        team_features_team1 = scaler.fit_transform(game_logs[feature_columns_team1])\n",
        "        team_features_team2 = scaler.fit_transform(game_logs[feature_columns_team2])\n",
        "\n",
        "        # Compute transformed features\n",
        "        X = (team_features_team1 - team_features_team2)  # Simpler transformation\n",
        "        y = (game_logs[\"WL\"] == \"W\").astype(int).to_numpy()\n",
        "\n",
        "        return X, y, feature_columns_team1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in prepare_dataset:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "def main():\n",
        "    game_logs_file = \"nba_game_logs.csv\"\n",
        "    features_file = \"nba_team_aggregated_data.csv\"\n",
        "\n",
        "    # Prepare the dataset\n",
        "    X, y, feature_columns = prepare_dataset(game_logs_file, features_file)\n",
        "\n",
        "    # Train the model\n",
        "    model, scaler = train_model(X, y)\n",
        "\n",
        "    # Evaluate the model\n",
        "    X_scaled = scaler.transform(X)\n",
        "    loss, accuracy = model.evaluate(X_scaled, y)\n",
        "    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_cuhZRejGr7s",
        "outputId": "89861fc0-87d0-4764-e8e7-97296db84b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_34\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_34\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_108 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m7,424\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_49 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_109 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,553\u001b[0m (29.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,553</span> (29.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,553\u001b[0m (29.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,553</span> (29.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/150\n",
            "209/209 - 2s - 9ms/step - custom_accuracy: 0.5200 - loss: 0.7173 - val_custom_accuracy: 0.5448 - val_loss: 0.6546\n",
            "Epoch 2/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5297 - loss: 0.6422 - val_custom_accuracy: 0.5425 - val_loss: 0.6715\n",
            "Epoch 3/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5464 - loss: 0.6541 - val_custom_accuracy: 0.5259 - val_loss: 0.6641\n",
            "Epoch 4/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5375 - loss: 0.6548 - val_custom_accuracy: 0.5283 - val_loss: 0.6625\n",
            "Epoch 5/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5463 - loss: 0.6294 - val_custom_accuracy: 0.5425 - val_loss: 0.6663\n",
            "Epoch 6/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5458 - loss: 0.6208 - val_custom_accuracy: 0.5330 - val_loss: 0.6691\n",
            "Epoch 7/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5365 - loss: 0.6064 - val_custom_accuracy: 0.5354 - val_loss: 0.6735\n",
            "Epoch 8/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5361 - loss: 0.6161 - val_custom_accuracy: 0.5354 - val_loss: 0.6790\n",
            "Epoch 9/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5445 - loss: 0.6035 - val_custom_accuracy: 0.5307 - val_loss: 0.6786\n",
            "Epoch 10/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5469 - loss: 0.6041 - val_custom_accuracy: 0.5425 - val_loss: 0.6751\n",
            "Epoch 11/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5409 - loss: 0.6051 - val_custom_accuracy: 0.5377 - val_loss: 0.6667\n",
            "Epoch 12/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5463 - loss: 0.6101 - val_custom_accuracy: 0.5377 - val_loss: 0.6641\n",
            "Epoch 13/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5498 - loss: 0.5959 - val_custom_accuracy: 0.5377 - val_loss: 0.6676\n",
            "Epoch 14/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5380 - loss: 0.5998 - val_custom_accuracy: 0.5354 - val_loss: 0.6693\n",
            "Epoch 15/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5565 - loss: 0.5916 - val_custom_accuracy: 0.5142 - val_loss: 0.6691\n",
            "Epoch 16/150\n",
            "209/209 - 1s - 4ms/step - custom_accuracy: 0.5475 - loss: 0.5951 - val_custom_accuracy: 0.5377 - val_loss: 0.6689\n",
            "Epoch 17/150\n",
            "209/209 - 1s - 6ms/step - custom_accuracy: 0.5504 - loss: 0.5856 - val_custom_accuracy: 0.5448 - val_loss: 0.6715\n",
            "Epoch 18/150\n",
            "209/209 - 1s - 5ms/step - custom_accuracy: 0.5415 - loss: 0.5931 - val_custom_accuracy: 0.5330 - val_loss: 0.6746\n",
            "Epoch 19/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5461 - loss: 0.5841 - val_custom_accuracy: 0.5354 - val_loss: 0.6846\n",
            "Epoch 20/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5367 - loss: 0.5846 - val_custom_accuracy: 0.5259 - val_loss: 0.6807\n",
            "Epoch 21/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5392 - loss: 0.5889 - val_custom_accuracy: 0.5283 - val_loss: 0.6814\n",
            "Epoch 22/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5379 - loss: 0.5820 - val_custom_accuracy: 0.5165 - val_loss: 0.6791\n",
            "Epoch 23/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5487 - loss: 0.5811 - val_custom_accuracy: 0.5283 - val_loss: 0.6892\n",
            "Epoch 24/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5320 - loss: 0.5856 - val_custom_accuracy: 0.5047 - val_loss: 0.6752\n",
            "Epoch 25/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5397 - loss: 0.5764 - val_custom_accuracy: 0.5307 - val_loss: 0.6719\n",
            "Epoch 26/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5415 - loss: 0.5805 - val_custom_accuracy: 0.5283 - val_loss: 0.6808\n",
            "Epoch 27/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5415 - loss: 0.5723 - val_custom_accuracy: 0.5354 - val_loss: 0.6872\n",
            "Epoch 28/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5506 - loss: 0.5809 - val_custom_accuracy: 0.5354 - val_loss: 0.6899\n",
            "Epoch 29/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5584 - loss: 0.5710 - val_custom_accuracy: 0.5377 - val_loss: 0.6969\n",
            "Epoch 30/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5493 - loss: 0.5795 - val_custom_accuracy: 0.5401 - val_loss: 0.6868\n",
            "Epoch 31/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5530 - loss: 0.5797 - val_custom_accuracy: 0.5259 - val_loss: 0.6844\n",
            "Epoch 32/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5445 - loss: 0.5735 - val_custom_accuracy: 0.5283 - val_loss: 0.6892\n",
            "Epoch 33/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5506 - loss: 0.5851 - val_custom_accuracy: 0.5118 - val_loss: 0.6899\n",
            "Epoch 34/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5469 - loss: 0.5688 - val_custom_accuracy: 0.5259 - val_loss: 0.6896\n",
            "Epoch 35/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5446 - loss: 0.5618 - val_custom_accuracy: 0.5307 - val_loss: 0.6912\n",
            "Epoch 36/150\n",
            "209/209 - 1s - 4ms/step - custom_accuracy: 0.5461 - loss: 0.5597 - val_custom_accuracy: 0.5259 - val_loss: 0.6983\n",
            "Epoch 37/150\n",
            "209/209 - 1s - 6ms/step - custom_accuracy: 0.5610 - loss: 0.5667 - val_custom_accuracy: 0.5259 - val_loss: 0.6887\n",
            "Epoch 38/150\n",
            "209/209 - 1s - 5ms/step - custom_accuracy: 0.5469 - loss: 0.5681 - val_custom_accuracy: 0.5307 - val_loss: 0.6848\n",
            "Epoch 39/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5493 - loss: 0.5622 - val_custom_accuracy: 0.5307 - val_loss: 0.6931\n",
            "Epoch 40/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5546 - loss: 0.5556 - val_custom_accuracy: 0.5071 - val_loss: 0.6919\n",
            "Epoch 41/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5475 - loss: 0.5587 - val_custom_accuracy: 0.5283 - val_loss: 0.7003\n",
            "Epoch 42/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5490 - loss: 0.5566 - val_custom_accuracy: 0.5283 - val_loss: 0.6966\n",
            "Epoch 43/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5636 - loss: 0.5604 - val_custom_accuracy: 0.5283 - val_loss: 0.6957\n",
            "Epoch 44/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5481 - loss: 0.5596 - val_custom_accuracy: 0.5401 - val_loss: 0.7009\n",
            "Epoch 45/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5536 - loss: 0.5500 - val_custom_accuracy: 0.5094 - val_loss: 0.6978\n",
            "Epoch 46/150\n",
            "209/209 - 1s - 2ms/step - custom_accuracy: 0.5463 - loss: 0.5529 - val_custom_accuracy: 0.5118 - val_loss: 0.7040\n",
            "Epoch 47/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5553 - loss: 0.5534 - val_custom_accuracy: 0.5094 - val_loss: 0.7057\n",
            "Epoch 48/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5472 - loss: 0.5454 - val_custom_accuracy: 0.5212 - val_loss: 0.7147\n",
            "Epoch 49/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5548 - loss: 0.5498 - val_custom_accuracy: 0.5283 - val_loss: 0.7103\n",
            "Epoch 50/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5469 - loss: 0.5428 - val_custom_accuracy: 0.5236 - val_loss: 0.7066\n",
            "Epoch 51/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5431 - loss: 0.5537 - val_custom_accuracy: 0.5189 - val_loss: 0.7012\n",
            "Epoch 52/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5487 - loss: 0.5411 - val_custom_accuracy: 0.5354 - val_loss: 0.7195\n",
            "Epoch 53/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5548 - loss: 0.5371 - val_custom_accuracy: 0.5307 - val_loss: 0.7225\n",
            "Epoch 54/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5421 - loss: 0.5559 - val_custom_accuracy: 0.5024 - val_loss: 0.7089\n",
            "Epoch 55/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5350 - loss: 0.5461 - val_custom_accuracy: 0.5236 - val_loss: 0.7049\n",
            "Epoch 56/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5506 - loss: 0.5248 - val_custom_accuracy: 0.5094 - val_loss: 0.7200\n",
            "Epoch 57/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5422 - loss: 0.5339 - val_custom_accuracy: 0.5212 - val_loss: 0.7199\n",
            "Epoch 58/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5530 - loss: 0.5309 - val_custom_accuracy: 0.5189 - val_loss: 0.7143\n",
            "Epoch 59/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5720 - loss: 0.5328 - val_custom_accuracy: 0.5071 - val_loss: 0.7116\n",
            "Epoch 60/150\n",
            "209/209 - 1s - 5ms/step - custom_accuracy: 0.5631 - loss: 0.5288 - val_custom_accuracy: 0.5047 - val_loss: 0.7113\n",
            "Epoch 61/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5595 - loss: 0.5298 - val_custom_accuracy: 0.5071 - val_loss: 0.7179\n",
            "Epoch 62/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5467 - loss: 0.5235 - val_custom_accuracy: 0.5118 - val_loss: 0.7196\n",
            "Epoch 63/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5552 - loss: 0.5084 - val_custom_accuracy: 0.5142 - val_loss: 0.7328\n",
            "Epoch 64/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5494 - loss: 0.5225 - val_custom_accuracy: 0.5071 - val_loss: 0.7280\n",
            "Epoch 65/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5553 - loss: 0.5230 - val_custom_accuracy: 0.5118 - val_loss: 0.7289\n",
            "Epoch 66/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5571 - loss: 0.5170 - val_custom_accuracy: 0.5236 - val_loss: 0.7219\n",
            "Epoch 67/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5583 - loss: 0.5253 - val_custom_accuracy: 0.5259 - val_loss: 0.7241\n",
            "Epoch 68/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5506 - loss: 0.5265 - val_custom_accuracy: 0.5307 - val_loss: 0.7182\n",
            "Epoch 69/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5445 - loss: 0.5217 - val_custom_accuracy: 0.5283 - val_loss: 0.7241\n",
            "Epoch 70/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5795 - loss: 0.5180 - val_custom_accuracy: 0.5094 - val_loss: 0.7192\n",
            "Epoch 71/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5524 - loss: 0.5139 - val_custom_accuracy: 0.5377 - val_loss: 0.7293\n",
            "Epoch 72/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5727 - loss: 0.5148 - val_custom_accuracy: 0.5212 - val_loss: 0.7271\n",
            "Epoch 73/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5529 - loss: 0.5173 - val_custom_accuracy: 0.5283 - val_loss: 0.7257\n",
            "Epoch 74/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5552 - loss: 0.5036 - val_custom_accuracy: 0.5259 - val_loss: 0.7232\n",
            "Epoch 75/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5589 - loss: 0.5091 - val_custom_accuracy: 0.5259 - val_loss: 0.7243\n",
            "Epoch 76/150\n",
            "209/209 - 0s - 2ms/step - custom_accuracy: 0.5682 - loss: 0.5130 - val_custom_accuracy: 0.5283 - val_loss: 0.7366\n",
            "Epoch 77/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5476 - loss: 0.5017 - val_custom_accuracy: 0.5259 - val_loss: 0.7347\n",
            "Epoch 78/150\n",
            "209/209 - 1s - 4ms/step - custom_accuracy: 0.5643 - loss: 0.5177 - val_custom_accuracy: 0.5330 - val_loss: 0.7326\n",
            "Epoch 79/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5742 - loss: 0.5085 - val_custom_accuracy: 0.5165 - val_loss: 0.7215\n",
            "Epoch 80/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5481 - loss: 0.5124 - val_custom_accuracy: 0.5307 - val_loss: 0.7311\n",
            "Epoch 81/150\n",
            "209/209 - 1s - 3ms/step - custom_accuracy: 0.5523 - loss: 0.4933 - val_custom_accuracy: 0.5354 - val_loss: 0.7338\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - custom_accuracy: 0.5060 - loss: 0.5978 \n",
            "Test Loss: 0.6031123995780945, Test Accuracy: 0.5025449991226196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from nba_api.stats.static import teams\n",
        "from keras.models import load_model\n",
        "import os\n",
        "\n",
        "\n",
        "def prepare_features(team_season1, team_season2, features_file, scaler, feature_columns):\n",
        "    \"\"\"\n",
        "    Prepare input features for prediction by combining team-specific features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load team features\n",
        "        team_features = pd.read_csv(features_file)\n",
        "\n",
        "        # Normalize team names\n",
        "        team_features[\"TEAM_SEASON\"] = team_features[\"TEAM_SEASON\"].str.strip().str.upper()\n",
        "        team_season1 = team_season1.strip().upper()\n",
        "        team_season2 = team_season2.strip().upper()\n",
        "\n",
        "        # Extract features for the two teams\n",
        "        features_team1 = team_features[team_features[\"TEAM_SEASON\"] == team_season1].add_suffix(\"_TEAM1\")\n",
        "        features_team2 = team_features[team_features[\"TEAM_SEASON\"] == team_season2].add_suffix(\"_TEAM2\")\n",
        "\n",
        "        if features_team1.empty or features_team2.empty:\n",
        "            raise ValueError(f\"Features for {team_season1} or {team_season2} not found in the file.\")\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = pd.concat([features_team1.reset_index(drop=True),\n",
        "                                        features_team2.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        # Align with feature_columns and fill missing values with 0\n",
        "        combined_features = combined_features.reindex(columns=feature_columns, fill_value=0)\n",
        "\n",
        "        # Scale features\n",
        "        X = combined_features.to_numpy()\n",
        "        X_scaled = scaler.transform(X)\n",
        "\n",
        "        return X_scaled\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in prepare_features:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "def predict(team_season1, team_season2, model_path, scaler_path, features_file, feature_columns):\n",
        "    \"\"\"\n",
        "    Predict the probability of Team 1 beating Team 2.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate file paths\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n",
        "        if not os.path.exists(scaler_path):\n",
        "            raise FileNotFoundError(f\"Scaler file '{scaler_path}' not found.\")\n",
        "\n",
        "        # Load the model and scaler\n",
        "        model = load_model(model_path, custom_objects={\"custom_accuracy\": custom_accuracy})\n",
        "        scaler = joblib.load(scaler_path)\n",
        "\n",
        "        # Prepare the input features\n",
        "        X_scaled = prepare_features(team_season1, team_season2, features_file, scaler, feature_columns)\n",
        "\n",
        "        # Make predictions\n",
        "        probability = model.predict(X_scaled).flatten()[0]\n",
        "\n",
        "        print(f\"Probability of {team_season1} beating {team_season2}: {probability:.2%}\")\n",
        "        return probability\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in predict:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom accuracy metric to evaluate the model based on given conditions.\n",
        "    \"\"\"\n",
        "    import tensorflow.keras.backend as K\n",
        "    condition_1 = K.cast(y_pred < 0.5, dtype=\"float32\") * K.cast(y_true == 0, dtype=\"float32\")\n",
        "    condition_2 = K.cast(y_pred >= 0.5, dtype=\"float32\") * K.cast(y_true == 1, dtype=\"float32\")\n",
        "    return K.mean(condition_1 + condition_2)\n",
        "\n",
        "\n",
        "def display_team_data(features_file):\n",
        "    \"\"\"\n",
        "    Display available team-season combinations for user reference.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        team_features = pd.read_csv(features_file)\n",
        "        print(\"Available TEAM_SEASON combinations:\")\n",
        "        for team_season in team_features[\"TEAM_SEASON\"].unique():\n",
        "            print(team_season)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in display_team_data:\", e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    features_file = \"nba_team_aggregated_data.csv\"\n",
        "    model_path = \"model.keras\"\n",
        "    scaler_path = \"scaler.pkl\"\n",
        "\n",
        "    # Load feature columns from the training process\n",
        "    try:\n",
        "        feature_columns = pd.read_csv(\"train_data_X.csv\", nrows=0).columns.tolist()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Feature column file 'train_data_X.csv' not found.\")\n",
        "        exit(1)\n",
        "\n",
        "    display_team_data(features_file)\n",
        "\n",
        "    # Example usage\n",
        "    team_season1 = input(\"Enter TEAM_SEASON like team:season (e.g., GSW:2024): \")\n",
        "    team_season2 = input(\"Enter TEAM_SEASON like team:season (e.g., PHI:2024): \")\n",
        "\n",
        "    predict(team_season1, team_season2, model_path, scaler_path, features_file, feature_columns)\n"
      ],
      "metadata": {
        "id": "_ilaXJR5Ud-1",
        "outputId": "57d0f3ee-3bb4-4450-88e1-0c23932fa787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available TEAM_SEASON combinations:\n",
            "ATL:2024\n",
            "BKN:2024\n",
            "BOS:2024\n",
            "CHA:2024\n",
            "CHI:2024\n",
            "CLE:2024\n",
            "DAL:2024\n",
            "DEN:2024\n",
            "DET:2024\n",
            "GSW:2024\n",
            "HOU:2024\n",
            "IND:2024\n",
            "LAC:2024\n",
            "LAL:2024\n",
            "MEM:2024\n",
            "MIA:2024\n",
            "MIL:2024\n",
            "MIN:2024\n",
            "NOP:2024\n",
            "NYK:2024\n",
            "OKC:2024\n",
            "ORL:2024\n",
            "PHI:2024\n",
            "PHX:2024\n",
            "POR:2024\n",
            "SAC:2024\n",
            "SAS:2024\n",
            "TOR:2024\n",
            "UTA:2024\n",
            "WAS:2024\n",
            "Enter TEAM_SEASON like team:season (e.g., GSW:2024): GSW:2024\n",
            "Enter TEAM_SEASON like team:season (e.g., PHI:2024): WAS:2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e978fda8ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "Probability of GSW:2024 beating WAS:2024: 36.04%\n"
          ]
        }
      ]
    }
  ]
}