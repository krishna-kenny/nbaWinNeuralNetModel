{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMOptzGKRRa3HyoOZJyx9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "5c118370-b129-4515-adb5-108f19354753"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nba_api in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs, LeagueGameFinder, LeagueLeaders, PlayerCareerStats\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "# Maximum number of retries for each API call\n",
        "MAX_RETRIES = 3\n",
        "# Define the list of seasons\n",
        "seasons = [\"2024-25\"]"
      ],
      "metadata": {
        "id": "SJ2v4Yzgep72"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt  # Exponential backoff\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "DV6BxvlTerjE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_info(seasons):\n",
        "    \"\"\"Fetches relevant team information for the specified seasons.\"\"\"\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for team in nba_teams:\n",
        "        team_info = fetch_with_retries(\n",
        "            TeamInfoCommon,\n",
        "            team_id=team[\"id\"],\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if team_info:\n",
        "            df_team = team_info.get_data_frames()[0]\n",
        "            df_team = df_team[[\"TEAM_ID\", \"TEAM_ABBREVIATION\"]]  # Only keep relevant features\n",
        "            team_data.append(df_team)\n",
        "            time.sleep(0.6)  # Delay to avoid API rate limits\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "# Run functions to save data to CSV files\n",
        "get_team_info(seasons)\n",
        "print(\"Team information data stored.\")"
      ],
      "metadata": {
        "id": "uflUFIR0et8B",
        "outputId": "a1815506-b00f-4600-945d-668207997127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Team information data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_game_logs(seasons):\n",
        "    \"\"\"Fetches team game logs for the specified seasons and processes the MATCHUP column.\"\"\"\n",
        "    print(\"Fetching team game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            # Keep only relevant columns\n",
        "            df_game_logs = df_game_logs[[\"GAME_ID\", \"GAME_DATE\", \"MATCHUP\", \"WL\"]]\n",
        "            game_log_data.append(df_game_logs)\n",
        "            time.sleep(0.6)  # Delay to respect rate limits\n",
        "\n",
        "    if game_log_data:\n",
        "        # Concatenate all game logs\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "\n",
        "        # Process MATCHUP column to create team1 and team2 columns\n",
        "        matchups_split = df_all_game_logs['MATCHUP'].str.split(' @ | vs. ', expand=True)\n",
        "        df_all_game_logs['TEAM1'] = matchups_split[0]\n",
        "        df_all_game_logs['TEAM2'] = matchups_split[1]\n",
        "\n",
        "        # Drop the original MATCHUP column if no longer needed\n",
        "        df_all_game_logs.drop(columns=['MATCHUP'], inplace=True)\n",
        "\n",
        "        # Extract and add SEASON_YEAR\n",
        "        df_all_game_logs['SEASON_YEAR'] = pd.to_datetime(df_all_game_logs['GAME_DATE']).dt.year.astype(str)\n",
        "\n",
        "        # Create combined TEAM_SEASON columns\n",
        "        df_all_game_logs['TEAM_SEASON1'] = df_all_game_logs['TEAM1'] + ':' + df_all_game_logs['SEASON_YEAR']\n",
        "        df_all_game_logs['TEAM_SEASON2'] = df_all_game_logs['TEAM2'] + ':' + df_all_game_logs['SEASON_YEAR']\n",
        "\n",
        "        # Drop the original TEAM1, TEAM2, and SEASON_YEAR columns if no longer needed\n",
        "        df_all_game_logs.drop(columns=['TEAM1', 'TEAM2', 'SEASON_YEAR'], inplace=True)\n",
        "\n",
        "        # Save the processed DataFrame to a CSV file\n",
        "        df_all_game_logs.to_csv(\"nba_game_logs.csv\", index=False)\n",
        "        print(\"Processed game logs saved to 'nba_game_logs.csv'.\")\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "get_team_game_logs(seasons)\n",
        "print(\"Team game logs data stored.\")\n"
      ],
      "metadata": {
        "id": "8r9Z1Q8Hev9-",
        "outputId": "0367ca40-6c17-427a-f57d-771a13905a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team game logs...\n",
            "Processed game logs saved to 'nba_game_logs.csv'.\n",
            "Team game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_player_game_logs(seasons):\n",
        "    \"\"\"Fetches player game logs for the specified seasons.\"\"\"\n",
        "    print(\"Fetching player game logs...\")\n",
        "    player_game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        player_game_logs = fetch_with_retries(\n",
        "            PlayerGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if player_game_logs:\n",
        "            df_player_game_logs = player_game_logs.get_data_frames()[0]\n",
        "            # Keep only relevant columns\n",
        "            df_player_game_logs = df_player_game_logs[[\n",
        "                \"SEASON_YEAR\", \"GAME_ID\", \"TEAM_ID\", \"PLAYER_ID\", \"PLAYER_NAME\", \"PTS\", \"REB\", \"AST\", \"STL\", \"BLK\",\n",
        "                \"MIN\", \"FG_PCT\", \"FG3_PCT\", \"FT_PCT\", \"TOV\", \"PF\"\n",
        "            ]]\n",
        "            # Modify SEASON_YEAR to keep only the first 4 characters\n",
        "            df_player_game_logs[\"SEASON_YEAR\"] = df_player_game_logs[\"SEASON_YEAR\"].str[:4]\n",
        "            player_game_log_data.append(df_player_game_logs)\n",
        "            time.sleep(0.6)\n",
        "\n",
        "    if player_game_log_data:\n",
        "        df_all_player_game_logs = pd.concat(player_game_log_data, ignore_index=True)\n",
        "        df_all_player_game_logs.to_csv(\"nba_player_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player game log data fetched.\")\n",
        "\n",
        "# Call the function with the specified seasons\n",
        "get_player_game_logs(seasons)\n",
        "print(\"Player game logs data stored.\")"
      ],
      "metadata": {
        "id": "mX-tQtyvex59",
        "outputId": "de2e7676-d9dd-4b91-befe-fc9d1896b727",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching player game logs...\n",
            "Player game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_league_game_data():\n",
        "    \"\"\"Fetches league-wide game data with relevant features for a neural network.\"\"\"\n",
        "    print(\"Fetching league game data for NN...\")\n",
        "    game_data = fetch_with_retries(LeagueGameFinder, timeout=60)\n",
        "    if game_data:\n",
        "        df_game_data = game_data.get_data_frames()[0]\n",
        "        # Relevant columns for neural network input\n",
        "        relevant_columns = [\n",
        "            \"SEASON_ID\", \"TEAM_ID\", \"TEAM_ABBREVIATION\", \"TEAM_NAME\", \"GAME_ID\",\n",
        "            \"GAME_DATE\", \"MATCHUP\", \"WL\", \"MIN\", \"PTS\", \"FGM\", \"FGA\", \"FG_PCT\",\n",
        "            \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"OREB\", \"DREB\",\n",
        "            \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PLUS_MINUS\"\n",
        "        ]\n",
        "        df_nn_data = df_game_data[relevant_columns]\n",
        "        df_nn_data.to_csv(\"nba_league_game.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No league game data fetched.\")\n",
        "\n",
        "get_league_game_data()\n",
        "print(\"League game data stored.\")"
      ],
      "metadata": {
        "id": "AQfHSeRyezmh",
        "outputId": "6c5b54a0-6b08-4a6a-f4bc-2fc3a3642ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching league game data for NN...\n",
            "League game data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_league_leaders():\n",
        "    \"\"\"Fetches league leaders data with relevant columns for analysis.\"\"\"\n",
        "    print(\"Fetching league leaders data...\")\n",
        "    leaders_data = fetch_with_retries(LeagueLeaders, timeout=60)\n",
        "    if leaders_data:\n",
        "        df_leaders = leaders_data.get_data_frames()[0]\n",
        "        # Select only relevant columns\n",
        "        relevant_columns = [\n",
        "            \"PLAYER_ID\", \"PLAYER\", \"TEAM_ID\", \"TEAM\", \"GP\", \"MIN\", \"FGM\", \"FGA\",\n",
        "            \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"OREB\",\n",
        "            \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PTS\", \"EFF\"\n",
        "        ]\n",
        "        df_relevant_leaders = df_leaders[relevant_columns]\n",
        "        df_relevant_leaders.to_csv(\"nba_league_leaders_relevant.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No league leaders data fetched.\")\n",
        "\n",
        "get_league_leaders()\n",
        "print(\"League leaders data stored.\")"
      ],
      "metadata": {
        "id": "nemrfltYe1I-",
        "outputId": "3e3756e3-7f85-4c27-99be-6ca75e591dfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching league leaders data...\n",
            "League leaders data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_player_career_stats():\n",
        "    \"\"\"Fetches career stats for players.\"\"\"\n",
        "    print(\"Fetching player career stats...\")\n",
        "    career_stats_data = []\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        players = team.get(\"players\", [])\n",
        "        for player in players:\n",
        "            career_stats = fetch_with_retries(PlayerCareerStats, player_id=player[\"id\"], timeout=60)\n",
        "            if career_stats:\n",
        "                df_career_stats = career_stats.get_data_frames()[0]\n",
        "                # Keep only relevant columns\n",
        "                df_career_stats = df_career_stats[[\n",
        "                    \"PLAYER_ID\", \"PLAYER_NAME\", \"GP\", \"PTS\", \"REB\", \"AST\", \"FG_PCT\", \"FG3_PCT\", \"FT_PCT\"\n",
        "                ]]\n",
        "                career_stats_data.append(df_career_stats)\n",
        "                time.sleep(0.6)\n",
        "\n",
        "    if career_stats_data:\n",
        "        df_all_career_stats = pd.concat(career_stats_data, ignore_index=True)\n",
        "        df_all_career_stats.to_csv(\"nba_player_career_stats.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player career stats data fetched.\")\n",
        "\n",
        "get_player_career_stats()\n",
        "print(\"Player career stats data stored.\")"
      ],
      "metadata": {
        "id": "MBwUaa-We2i8",
        "outputId": "c56878f4-3eb1-47ad-ef19-17ada68c20f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching player career stats...\n",
            "No player career stats data fetched.\n",
            "Player career stats data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load player game logs from the CSV file\n",
        "file_path = \"nba_player_game_logs.csv\"  # Update with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Load league leaders data (assumed to have 'PLAYER_ID' column)\n",
        "league_leaders_path = \"nba_league_leaders_relevant.csv\"  # Update with your actual file path\n",
        "league_leaders_df = pd.read_csv(league_leaders_path)\n",
        "\n",
        "# Exclude non-numerical columns explicitly\n",
        "no_aggregate_columns = ['PLAYER_ID', 'SEASON_YEAR', 'PLAYER_NAME', 'TEAM_ID']  # Adjust as necessary\n",
        "numerical_columns = [col for col in df.columns if col not in no_aggregate_columns]\n",
        "\n",
        "# Group by PLAYER_ID and SEASON_YEAR\n",
        "grouped = df.groupby(['PLAYER_ID', 'SEASON_YEAR'])\n",
        "\n",
        "# Aggregate numerical columns using mean and count the number of games\n",
        "aggregated_data = grouped[numerical_columns].mean().reset_index()\n",
        "\n",
        "# Add non-numerical columns using the first value in the group (like TEAM_ID)\n",
        "aggregated_data['TEAM_ID'] = grouped['TEAM_ID'].first().values\n",
        "\n",
        "# Add games played as a new column\n",
        "aggregated_data['GAMES_PLAYED'] = grouped.size().values\n",
        "\n",
        "# Mark league leaders (ignoring SEASON_YEAR)\n",
        "league_leader_set = set(league_leaders_df['PLAYER_ID'])\n",
        "\n",
        "# Add a column to indicate whether the player is a league leader\n",
        "aggregated_data['LEAGUE_LEADER'] = aggregated_data['PLAYER_ID'].apply(\n",
        "    lambda player_id: 1 if player_id in league_leader_set else 0\n",
        ")\n",
        "\n",
        "# Save the aggregated data for further use\n",
        "output_path = \"nba_player_aggregated_data.csv\"\n",
        "aggregated_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Aggregated data saved to '{output_path}'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dTm6jXFeV36",
        "outputId": "1372332e-dbb2-499a-a69b-3889ece44ce5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data saved to 'nba_player_aggregated_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load aggregated player data\n",
        "player_aggregated_file = \"nba_player_aggregated_data.csv\"  # Update with your actual file path\n",
        "team_abbreviation_file = \"nba_team_data.csv\"  # File containing TEAM_ID to TEAM_ABBREVIATION mapping\n",
        "\n",
        "# Load player data and team abbreviation mapping\n",
        "player_df = pd.read_csv(player_aggregated_file)\n",
        "team_data_df = pd.read_csv(team_abbreviation_file)\n",
        "\n",
        "# Merge team abbreviations into player data\n",
        "player_df = player_df.merge(team_data_df, on=\"TEAM_ID\", how=\"left\")\n",
        "\n",
        "# Combine TEAM_ABBREVIATION and SEASON_YEAR into a new column\n",
        "player_df['TEAM_SEASON'] = player_df['TEAM_ABBREVIATION'] + \":\" + player_df['SEASON_YEAR'].astype(str)\n",
        "\n",
        "# Drop the original TEAM_ABBREVIATION and SEASON_YEAR columns\n",
        "player_df.drop(columns=['TEAM_ABBREVIATION', 'SEASON_YEAR'], inplace=True)\n",
        "\n",
        "# Define non-numerical columns to exclude\n",
        "no_aggregate_columns = ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_SEASON']\n",
        "numerical_columns = [col for col in player_df.columns if col not in no_aggregate_columns]\n",
        "\n",
        "# Multiply each player's stats by their 'MIN' to weight the statistics\n",
        "for col in numerical_columns:\n",
        "    player_df[f\"{col}_WEIGHTED\"] = player_df[col] * player_df['MIN']\n",
        "\n",
        "# Group by TEAM_SEASON\n",
        "grouped = player_df.groupby(['TEAM_SEASON'])\n",
        "\n",
        "# Compute team-level weighted stats as the sum of weighted stats divided by the total 'MIN'\n",
        "team_aggregated_data = grouped[[f\"{col}_WEIGHTED\" for col in numerical_columns]].sum()\n",
        "team_aggregated_data.columns = numerical_columns  # Rename back to original column names\n",
        "\n",
        "# Compute total minutes played by the team\n",
        "team_aggregated_data['TOTAL_MIN'] = grouped['MIN'].sum()\n",
        "\n",
        "# Normalize weighted stats by dividing by TOTAL_MIN\n",
        "for col in numerical_columns:\n",
        "    team_aggregated_data[col] = team_aggregated_data[col] / team_aggregated_data['TOTAL_MIN']\n",
        "\n",
        "# Add additional columns\n",
        "team_aggregated_data['TEAM_GAMES_PLAYED'] = grouped['GAMES_PLAYED'].sum()  # Total games played by players in the team\n",
        "\n",
        "# Reset index to flatten the DataFrame\n",
        "team_aggregated_data.reset_index(inplace=True)\n",
        "\n",
        "# Save the aggregated data for further use\n",
        "output_path = \"nba_team_aggregated_data.csv\"\n",
        "team_aggregated_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Team aggregated data saved to '{output_path}'.\")\n",
        "print(team_aggregated_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojGa1FcvBwsO",
        "outputId": "875d8125-b450-49a0-bc65-b52a57f99cb6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team aggregated data saved to 'nba_team_aggregated_data.csv'.\n",
            "  TEAM_SEASON       GAME_ID        PTS       REB       AST       STL  \\\n",
            "0    ATL:2024  2.240029e+07  12.517602  4.603445  3.263720  1.106791   \n",
            "1    BKN:2024  2.240029e+07  11.169127  3.884185  2.478942  0.692463   \n",
            "2    BOS:2024  2.240030e+07  13.978082  4.928553  2.887519  0.832307   \n",
            "3    CHA:2024  2.240030e+07  11.674453  4.483149  2.554467  0.774399   \n",
            "4    CHI:2024  2.240029e+07  11.855969  4.358064  3.096378  0.742207   \n",
            "\n",
            "        BLK        MIN    FG_PCT   FG3_PCT    FT_PCT       TOV        PF  \\\n",
            "0  0.523868  25.093177  0.449480  0.287977  0.482819  1.688143  1.883960   \n",
            "1  0.352566  23.417055  0.415455  0.249237  0.477443  1.456231  2.003369   \n",
            "2  0.593120  26.227137  0.422986  0.288981  0.449298  1.307108  1.733583   \n",
            "3  0.501070  24.043847  0.408512  0.255962  0.456263  1.621471  2.083410   \n",
            "4  0.404245  23.668120  0.434351  0.330585  0.453331  1.496984  1.688131   \n",
            "\n",
            "   GAMES_PLAYED  LEAGUE_LEADER   TOTAL_MIN  TEAM_GAMES_PLAYED  \n",
            "0     24.575455            1.0  334.073158                336  \n",
            "1     23.272073            1.0  297.515535                309  \n",
            "2     25.214425            1.0  317.080398                345  \n",
            "3     19.883753            1.0  390.016917                348  \n",
            "4     26.897446            1.0  304.303980                395  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "M8jLxkQKWOmv"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_season_year(game_date):\n",
        "    \"\"\"\n",
        "    Extracts the year from a game date in the format 'YYYY-MM-DDTHH:MM:SS'.\n",
        "\n",
        "    Args:\n",
        "        game_date (str): The game date string.\n",
        "\n",
        "    Returns:\n",
        "        str: The year as a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pd.to_datetime(game_date).year\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date '{game_date}': {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "cydqZx_RvyGo"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using team-specific features.\n",
        "\n",
        "    Args:\n",
        "        game_logs_file: CSV file containing game logs with TEAM1, TEAM2, GAME_DATE, and WL columns.\n",
        "        features_file: CSV file containing aggregated team features.\n",
        "\n",
        "    Returns:\n",
        "        X: Feature matrix for training.\n",
        "        y: Target vector (win/loss).\n",
        "        feature_columns: List of feature names used in the dataset.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load game logs\n",
        "        game_logs = pd.read_csv(game_logs_file, parse_dates=[\"GAME_DATE\"])  # Ensure GAME_DATE is parsed\n",
        "\n",
        "\n",
        "        # Load team features\n",
        "        team_features = pd.read_csv(features_file)\n",
        "\n",
        "        # Normalize key columns\n",
        "        game_logs[\"TEAM_SEASON1\"] = game_logs[\"TEAM_SEASON1\"].str.strip().str.upper()\n",
        "        game_logs[\"TEAM_SEASON2\"] = game_logs[\"TEAM_SEASON2\"].str.strip().str.upper()\n",
        "\n",
        "        # Merge features for TEAM1\n",
        "        game_logs = game_logs.merge(\n",
        "            team_features.add_suffix(\"_TEAM1\"),\n",
        "            left_on=[\"TEAM_SEASON1\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM1\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Merge features for TEAM2\n",
        "        game_logs = game_logs.merge(\n",
        "            team_features.add_suffix(\"_TEAM2\"),\n",
        "            left_on=[\"TEAM_SEASON2\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM2\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = [\"TEAM_SEASON_TEAM1\", \"TEAM_SEASON_TEAM2\", \"GAME_DATE\"]\n",
        "        game_logs.drop(columns=[col for col in columns_to_drop if col in game_logs.columns], inplace=True)\n",
        "\n",
        "        # Handle missing values\n",
        "        game_logs.fillna(0, inplace=True)\n",
        "\n",
        "        # Extract features and target\n",
        "        feature_columns = game_logs.select_dtypes(include=np.number).columns.difference([\"WL\"])\n",
        "        X = game_logs[feature_columns].to_numpy()\n",
        "        y = (game_logs[\"WL\"] == \"W\").astype(int).to_numpy()  # Convert \"W\"/\"L\" to 1/0\n",
        "\n",
        "        return X, y, feature_columns\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in prepare_dataset:\", e)\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "3viwY7byyMc2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "import joblib\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom accuracy metric to evaluate the model based on given conditions.\n",
        "    \"\"\"\n",
        "    condition_1 = K.cast(y_pred < 0.5, dtype=\"float32\") * K.cast(y_true == 0, dtype=\"float32\")\n",
        "    condition_2 = K.cast(y_pred >= 0.5, dtype=\"float32\") * K.cast(y_true == 1, dtype=\"float32\")\n",
        "    return K.mean(condition_1 + condition_2)\n",
        "\n",
        "\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using team-specific features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load game logs and team features\n",
        "        game_logs = pd.read_csv(game_logs_file, parse_dates=[\"GAME_DATE\"])\n",
        "        team_features = pd.read_csv(features_file)\n",
        "\n",
        "        # Normalize key columns\n",
        "        game_logs[\"TEAM_SEASON1\"] = game_logs[\"TEAM_SEASON1\"].str.strip().str.upper()\n",
        "        game_logs[\"TEAM_SEASON2\"] = game_logs[\"TEAM_SEASON2\"].str.strip().str.upper()\n",
        "\n",
        "        # Merge features for TEAM1 and TEAM2\n",
        "        game_logs = game_logs.merge(\n",
        "            team_features.add_suffix(\"_TEAM1\"),\n",
        "            left_on=[\"TEAM_SEASON1\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM1\"],\n",
        "            how=\"left\"\n",
        "        ).merge(\n",
        "            team_features.add_suffix(\"_TEAM2\"),\n",
        "            left_on=[\"TEAM_SEASON2\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM2\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = [\"TEAM_SEASON_TEAM1\", \"TEAM_SEASON_TEAM2\", \"GAME_DATE\"]\n",
        "        game_logs.drop(columns=[col for col in columns_to_drop if col in game_logs.columns], inplace=True)\n",
        "\n",
        "        # Handle missing values\n",
        "        game_logs.fillna(0, inplace=True)\n",
        "\n",
        "        # Extract features and target\n",
        "        feature_columns = game_logs.select_dtypes(include=np.number).columns.difference([\"WL\"])\n",
        "        X = game_logs[feature_columns].to_numpy()\n",
        "        y = (game_logs[\"WL\"] == \"W\").astype(int).to_numpy()\n",
        "\n",
        "        return X, y, feature_columns\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in prepare_dataset:\", e)\n",
        "        raise\n",
        "\n",
        "def build_neural_network(input_shape):\n",
        "    \"\"\"\n",
        "    Build a neural network model with added regularization.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(64, activation=\"relu\"),  # Added L2 regularization\n",
        "        Dropout(0.5),  # Increased dropout rate\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(16, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),  # Reduced learning rate\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[custom_accuracy])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(X, y, save_data_prefix=\"train_data\"):\n",
        "    \"\"\"\n",
        "    Train a neural network model and save the processed data.\n",
        "    \"\"\"\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Handle class imbalance\n",
        "    X_resampled, y_resampled = SMOTE().fit_resample(X_scaled, y)\n",
        "\n",
        "    # Save processed training data\n",
        "    pd.DataFrame(X_resampled).to_csv(f\"{save_data_prefix}_X.csv\", index=False, header=False)\n",
        "    pd.DataFrame(y_resampled).to_csv(f\"{save_data_prefix}_y.csv\", index=False, header=False)\n",
        "\n",
        "    # Debugging output\n",
        "    print(f\"Processed training data saved to {save_data_prefix}_X.csv and {save_data_prefix}_y.csv.\")\n",
        "    print(f\"Resampled Features Shape: {X_resampled.shape}, Resampled Target Shape: {y_resampled.shape}\")\n",
        "\n",
        "    # Build the model\n",
        "    model = build_neural_network(X_resampled.shape[1])\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=256, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_resampled, y_resampled,\n",
        "              epochs=64,\n",
        "              batch_size=1,\n",
        "              validation_split=0.2,\n",
        "              callbacks=[early_stopping])\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    game_logs_file = \"nba_game_logs.csv\"\n",
        "    features_file = \"nba_team_aggregated_data.csv\"\n",
        "\n",
        "    # Prepare the dataset\n",
        "    X, y, feature_columns = prepare_dataset(game_logs_file, features_file)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # Validate split\n",
        "    assert X_train.shape[0] == y_train.shape[0], \"Mismatch in training data sizes.\"\n",
        "    assert X_test.shape[0] == y_test.shape[0], \"Mismatch in test data sizes.\"\n",
        "\n",
        "    # Train neural network\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Save the model and scaler\n",
        "    model.save(\"model.keras\")\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    test_loss, test_custom_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "    print(f\"Test Loss: {test_loss}, Test Custom Accuracy: {test_custom_accuracy}\")\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred.flatten()}).to_csv(\"predictions.csv\", index=False)\n",
        "    print(\"Predictions saved to predictions.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cuhZRejGr7s",
        "outputId": "9049ff68-9388-4db6-8161-e6acc4f38bb1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed training data saved to train_data_X.csv and train_data_y.csv.\n",
            "Resampled Features Shape: (794, 33), Resampled Target Shape: (794,)\n",
            "Epoch 1/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.4928 - loss: 0.7233 - val_custom_accuracy: 0.6352 - val_loss: 0.6398\n",
            "Epoch 2/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - custom_accuracy: 0.6425 - loss: 0.6869 - val_custom_accuracy: 0.6289 - val_loss: 0.6395\n",
            "Epoch 3/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - custom_accuracy: 0.6396 - loss: 0.6381 - val_custom_accuracy: 0.6478 - val_loss: 0.6200\n",
            "Epoch 4/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - custom_accuracy: 0.6165 - loss: 0.7099 - val_custom_accuracy: 0.6604 - val_loss: 0.6345\n",
            "Epoch 5/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.6411 - loss: 0.6205 - val_custom_accuracy: 0.6918 - val_loss: 0.6106\n",
            "Epoch 6/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7143 - loss: 0.5910 - val_custom_accuracy: 0.6855 - val_loss: 0.6129\n",
            "Epoch 7/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.6584 - loss: 0.6301 - val_custom_accuracy: 0.6667 - val_loss: 0.6159\n",
            "Epoch 8/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.6667 - loss: 0.5902 - val_custom_accuracy: 0.6855 - val_loss: 0.6133\n",
            "Epoch 9/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.6275 - loss: 0.6259 - val_custom_accuracy: 0.6730 - val_loss: 0.6209\n",
            "Epoch 10/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.6116 - loss: 0.6768 - val_custom_accuracy: 0.6855 - val_loss: 0.6168\n",
            "Epoch 11/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.6848 - loss: 0.5966 - val_custom_accuracy: 0.6604 - val_loss: 0.6572\n",
            "Epoch 12/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7130 - loss: 0.5811 - val_custom_accuracy: 0.6855 - val_loss: 0.6088\n",
            "Epoch 13/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.6947 - loss: 0.5773 - val_custom_accuracy: 0.6981 - val_loss: 0.6049\n",
            "Epoch 14/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.6935 - loss: 0.5982 - val_custom_accuracy: 0.6730 - val_loss: 0.6271\n",
            "Epoch 15/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7230 - loss: 0.6395 - val_custom_accuracy: 0.6604 - val_loss: 0.6221\n",
            "Epoch 16/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7190 - loss: 0.5707 - val_custom_accuracy: 0.6667 - val_loss: 0.5984\n",
            "Epoch 17/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.6661 - loss: 0.6129 - val_custom_accuracy: 0.6855 - val_loss: 0.6024\n",
            "Epoch 18/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7012 - loss: 0.5627 - val_custom_accuracy: 0.6478 - val_loss: 0.6048\n",
            "Epoch 19/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7158 - loss: 0.5784 - val_custom_accuracy: 0.6667 - val_loss: 0.6050\n",
            "Epoch 20/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7135 - loss: 0.5722 - val_custom_accuracy: 0.6415 - val_loss: 0.6205\n",
            "Epoch 21/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7039 - loss: 0.5674 - val_custom_accuracy: 0.6604 - val_loss: 0.6129\n",
            "Epoch 22/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7233 - loss: 0.5490 - val_custom_accuracy: 0.6667 - val_loss: 0.6466\n",
            "Epoch 23/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7043 - loss: 0.5738 - val_custom_accuracy: 0.6918 - val_loss: 0.6157\n",
            "Epoch 24/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7462 - loss: 0.5783 - val_custom_accuracy: 0.6730 - val_loss: 0.6208\n",
            "Epoch 25/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7314 - loss: 0.5506 - val_custom_accuracy: 0.6730 - val_loss: 0.6085\n",
            "Epoch 26/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7357 - loss: 0.5676 - val_custom_accuracy: 0.6730 - val_loss: 0.6139\n",
            "Epoch 27/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7149 - loss: 0.5643 - val_custom_accuracy: 0.6918 - val_loss: 0.6036\n",
            "Epoch 28/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7383 - loss: 0.5441 - val_custom_accuracy: 0.6604 - val_loss: 0.6473\n",
            "Epoch 29/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7589 - loss: 0.5486 - val_custom_accuracy: 0.6918 - val_loss: 0.5985\n",
            "Epoch 30/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7291 - loss: 0.5416 - val_custom_accuracy: 0.6918 - val_loss: 0.6028\n",
            "Epoch 31/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7172 - loss: 0.5378 - val_custom_accuracy: 0.6918 - val_loss: 0.6221\n",
            "Epoch 32/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7123 - loss: 0.5356 - val_custom_accuracy: 0.6918 - val_loss: 0.6090\n",
            "Epoch 33/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7241 - loss: 0.5580 - val_custom_accuracy: 0.6730 - val_loss: 0.6015\n",
            "Epoch 34/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7189 - loss: 0.5697 - val_custom_accuracy: 0.6792 - val_loss: 0.6032\n",
            "Epoch 35/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7078 - loss: 0.5528 - val_custom_accuracy: 0.6792 - val_loss: 0.6083\n",
            "Epoch 36/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7371 - loss: 0.5433 - val_custom_accuracy: 0.6792 - val_loss: 0.5979\n",
            "Epoch 37/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7099 - loss: 0.5508 - val_custom_accuracy: 0.6792 - val_loss: 0.6096\n",
            "Epoch 38/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7470 - loss: 0.5509 - val_custom_accuracy: 0.6792 - val_loss: 0.6139\n",
            "Epoch 39/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7627 - loss: 0.5262 - val_custom_accuracy: 0.6855 - val_loss: 0.6047\n",
            "Epoch 40/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7642 - loss: 0.4990 - val_custom_accuracy: 0.6730 - val_loss: 0.6163\n",
            "Epoch 41/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7401 - loss: 0.5357 - val_custom_accuracy: 0.6792 - val_loss: 0.6306\n",
            "Epoch 42/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7043 - loss: 0.5805 - val_custom_accuracy: 0.6792 - val_loss: 0.6116\n",
            "Epoch 43/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7529 - loss: 0.5046 - val_custom_accuracy: 0.6667 - val_loss: 0.6075\n",
            "Epoch 44/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7526 - loss: 0.5584 - val_custom_accuracy: 0.6604 - val_loss: 0.6106\n",
            "Epoch 45/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7260 - loss: 0.5016 - val_custom_accuracy: 0.6792 - val_loss: 0.6073\n",
            "Epoch 46/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7238 - loss: 0.5167 - val_custom_accuracy: 0.6855 - val_loss: 0.6148\n",
            "Epoch 47/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7113 - loss: 0.5748 - val_custom_accuracy: 0.6918 - val_loss: 0.6145\n",
            "Epoch 48/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7562 - loss: 0.5067 - val_custom_accuracy: 0.6918 - val_loss: 0.6088\n",
            "Epoch 49/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7355 - loss: 0.5156 - val_custom_accuracy: 0.7044 - val_loss: 0.6111\n",
            "Epoch 50/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7218 - loss: 0.5162 - val_custom_accuracy: 0.6981 - val_loss: 0.6091\n",
            "Epoch 51/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7219 - loss: 0.5374 - val_custom_accuracy: 0.7358 - val_loss: 0.6216\n",
            "Epoch 52/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7507 - loss: 0.4874 - val_custom_accuracy: 0.7170 - val_loss: 0.6083\n",
            "Epoch 53/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7790 - loss: 0.4966 - val_custom_accuracy: 0.6855 - val_loss: 0.6148\n",
            "Epoch 54/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7323 - loss: 0.4952 - val_custom_accuracy: 0.6855 - val_loss: 0.6743\n",
            "Epoch 55/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7683 - loss: 0.4899 - val_custom_accuracy: 0.6855 - val_loss: 0.6327\n",
            "Epoch 56/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7773 - loss: 0.4598 - val_custom_accuracy: 0.6981 - val_loss: 0.6469\n",
            "Epoch 57/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7593 - loss: 0.4788 - val_custom_accuracy: 0.6667 - val_loss: 0.6567\n",
            "Epoch 58/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7603 - loss: 0.5979 - val_custom_accuracy: 0.6730 - val_loss: 0.6400\n",
            "Epoch 59/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.7711 - loss: 0.4736 - val_custom_accuracy: 0.6667 - val_loss: 0.6272\n",
            "Epoch 60/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7356 - loss: 0.4964 - val_custom_accuracy: 0.6918 - val_loss: 0.6182\n",
            "Epoch 61/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - custom_accuracy: 0.7911 - loss: 0.4565 - val_custom_accuracy: 0.6918 - val_loss: 0.6400\n",
            "Epoch 62/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - custom_accuracy: 0.7513 - loss: 0.4889 - val_custom_accuracy: 0.7233 - val_loss: 0.6440\n",
            "Epoch 63/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7531 - loss: 0.4742 - val_custom_accuracy: 0.7107 - val_loss: 0.6200\n",
            "Epoch 64/64\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - custom_accuracy: 0.7882 - loss: 0.4377 - val_custom_accuracy: 0.7107 - val_loss: 0.7147\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - custom_accuracy: 0.4997 - loss: 0.6517  \n",
            "Test Loss: 0.6566861271858215, Test Custom Accuracy: 0.5055022239685059\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Predictions saved to predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nba_api.stats.static import teams\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "def load_feature_names(feature_names_file=\"feature_names.pkl\"):\n",
        "    \"\"\"\n",
        "    Load saved feature names for feature alignment during prediction.\n",
        "    \"\"\"\n",
        "    feature_names = joblib.load(feature_names_file)\n",
        "\n",
        "    # Print feature names\n",
        "    print(\"Feature Names Used in the Model:\")\n",
        "    print(feature_names)\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "\n",
        "def get_team_id_by_abbreviation(team_abbreviation):\n",
        "    \"\"\"Retrieve the team ID by abbreviation.\"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        if team[\"abbreviation\"].lower() == team_abbreviation.lower():\n",
        "            return team[\"id\"]\n",
        "    raise ValueError(f\"Team '{team_abbreviation}' not found! Please enter a valid abbreviation.\")\n",
        "\n",
        "\n",
        "def fetch_team_features(team_abbreviation, features_file, feature_names):\n",
        "    \"\"\"\n",
        "    Retrieve the team-specific features for the given team abbreviation.\n",
        "\n",
        "    Args:\n",
        "        team_abbreviation: Abbreviation of the NBA team (e.g., 'LAL').\n",
        "        features_file: CSV file containing the aggregated team features.\n",
        "        feature_names: List of features expected by the model.\n",
        "\n",
        "    Returns:\n",
        "        numpy array of the team's features.\n",
        "    \"\"\"\n",
        "    team_features = pd.read_csv(features_file)\n",
        "    team_row = team_features[team_features[\"TEAM\"] == team_abbreviation.upper()]\n",
        "\n",
        "    if team_row.empty:\n",
        "        raise ValueError(f\"Features for team '{team_abbreviation}' not found in {features_file}.\")\n",
        "\n",
        "    # Ensure only numeric features are returned\n",
        "    numeric_features = team_row[feature_names].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    return numeric_features.to_numpy().flatten()\n",
        "\n",
        "\n",
        "def predict_matchup_win_probability(team1_abbreviation, team2_abbreviation, features_file, model_path=\"model.h5\", scaler_path=\"scaler.pkl\"):\n",
        "    \"\"\"\n",
        "    Predict the win probability for Team 1 in a matchup against Team 2.\n",
        "\n",
        "    Args:\n",
        "        team1_abbreviation: Abbreviation of Team 1 (e.g., 'LAL').\n",
        "        team2_abbreviation: Abbreviation of Team 2 (e.g., 'BOS').\n",
        "        features_file: CSV file containing aggregated team features.\n",
        "        model_path: Path to the trained neural network model file.\n",
        "        scaler_path: Path to the scaler file for feature normalization.\n",
        "    \"\"\"\n",
        "    # Load model, scaler, and feature names\n",
        "    model = load_model(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    feature_names = load_feature_names()\n",
        "\n",
        "    # Fetch features for both teams\n",
        "    team1_features = fetch_team_features(team1_abbreviation, features_file, feature_names)\n",
        "    team2_features = fetch_team_features(team2_abbreviation, features_file, feature_names)\n",
        "\n",
        "    # Create matchup feature differences and ratios\n",
        "    matchup_features = np.concatenate([\n",
        "        team1_features - team2_features,\n",
        "        team1_features / (team2_features + 1e-5)  # Avoid division by zero\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Scale the matchup features\n",
        "    matchup_features_scaled = scaler.transform(matchup_features)\n",
        "\n",
        "    # Predict win probability for Team 1\n",
        "    win_probability = model.predict(matchup_features_scaled)[0][0]\n",
        "\n",
        "    print(f\"\\nWin Probability for {team1_abbreviation} vs {team2_abbreviation}: {win_probability * 100:.2f}%\")\n",
        "\n",
        "\n",
        "def display_team_data():\n",
        "    \"\"\"\n",
        "    Display available team abbreviations and names for user reference.\n",
        "    \"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    print(\"Available NBA Teams:\")\n",
        "    for team in nba_teams:\n",
        "        print(f\"{team['abbreviation']} - {team['full_name']}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to handle user input and prediction.\"\"\"\n",
        "    features_file = \"features.csv\"  # Path to the features file\n",
        "\n",
        "    # Display team data before taking user input\n",
        "    display_team_data()\n",
        "\n",
        "    team1_abbreviation = input(\"Enter Team 1 abbreviation (e.g., 'LAL' for Los Angeles Lakers): \").strip()\n",
        "    team2_abbreviation = input(\"Enter Team 2 abbreviation (e.g., 'BOS' for Boston Celtics): \").strip()\n",
        "\n",
        "    try:\n",
        "        predict_matchup_win_probability(team1_abbreviation, team2_abbreviation, features_file)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "heGJjGPVGuLX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}