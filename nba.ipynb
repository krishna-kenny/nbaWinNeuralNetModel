{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkBUPHI/W/8WHBlX5eq+XH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "103ac1f8-3dcf-493a-b248-fa39ad61c633"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nba_api in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs, LeagueGameFinder, LeagueLeaders, PlayerCareerStats\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "# Maximum number of retries for each API call\n",
        "MAX_RETRIES = 3\n",
        "# Define the list of seasons\n",
        "# Generate all seasons from 2013 onwards\n",
        "start_year = 2024\n",
        "end_year = 2024  # Adjust to your desired year\n",
        "seasons = [f\"{year}-{(year + 1)%100}\" for year in range(start_year, end_year + 1)]\n",
        "\n",
        "# Printing seasons to verify\n",
        "print(seasons)\n"
      ],
      "metadata": {
        "id": "SJ2v4Yzgep72",
        "outputId": "815212ac-836d-47a7-afae-e77bd07b7ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2024-25']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt  # Exponential backoff\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "DV6BxvlTerjE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_info(seasons):\n",
        "    \"\"\"Fetches relevant team information for the specified seasons.\"\"\"\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for team in nba_teams:\n",
        "        team_info = fetch_with_retries(\n",
        "            TeamInfoCommon,\n",
        "            team_id=team[\"id\"],\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if team_info:\n",
        "            df_team = team_info.get_data_frames()[0]\n",
        "            df_team = df_team[[\"TEAM_ID\", \"TEAM_ABBREVIATION\"]]  # Only keep relevant features\n",
        "            team_data.append(df_team)\n",
        "            time.sleep(0.6)  # Delay to avoid API rate limits\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "# Run functions to save data to CSV files\n",
        "get_team_info(seasons)\n",
        "print(\"Team information data stored.\")"
      ],
      "metadata": {
        "id": "uflUFIR0et8B",
        "outputId": "b8233c5f-d3d1-4a8f-9746-9ca876e0244e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Team information data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_game_logs(seasons):\n",
        "    \"\"\"Fetches team game logs for the specified seasons and processes the MATCHUP column.\"\"\"\n",
        "    print(\"Fetching team game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            # Keep only relevant columns\n",
        "            df_game_logs = df_game_logs[[\"GAME_ID\", \"GAME_DATE\", \"MATCHUP\", \"WL\"]]\n",
        "            game_log_data.append(df_game_logs)\n",
        "            time.sleep(0.6)  # Delay to respect rate limits\n",
        "\n",
        "    if game_log_data:\n",
        "        # Concatenate all game logs\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "\n",
        "        # Process MATCHUP column to create team1 and team2 columns\n",
        "        matchups_split = df_all_game_logs['MATCHUP'].str.split(' @ | vs. ', expand=True)\n",
        "        df_all_game_logs['TEAM1'] = matchups_split[0]\n",
        "        df_all_game_logs['TEAM2'] = matchups_split[1]\n",
        "\n",
        "        # Drop the original MATCHUP column if no longer needed\n",
        "        df_all_game_logs.drop(columns=['MATCHUP'], inplace=True)\n",
        "\n",
        "        # Extract and add SEASON_YEAR\n",
        "        df_all_game_logs['SEASON_YEAR'] = pd.to_datetime(df_all_game_logs['GAME_DATE']).dt.year.astype(str)\n",
        "\n",
        "        # Create combined TEAM_SEASON columns\n",
        "        df_all_game_logs['TEAM_SEASON1'] = df_all_game_logs['TEAM1'] + ':' + df_all_game_logs['SEASON_YEAR']\n",
        "        df_all_game_logs['TEAM_SEASON2'] = df_all_game_logs['TEAM2'] + ':' + df_all_game_logs['SEASON_YEAR']\n",
        "\n",
        "        # Drop the original TEAM1, TEAM2, and SEASON_YEAR columns if no longer needed\n",
        "        df_all_game_logs.drop(columns=['TEAM1', 'TEAM2', 'SEASON_YEAR'], inplace=True)\n",
        "\n",
        "        # Save the processed DataFrame to a CSV file\n",
        "        df_all_game_logs.to_csv(\"nba_game_logs.csv\", index=False)\n",
        "        print(\"Processed game logs saved to 'nba_game_logs.csv'.\")\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "get_team_game_logs(seasons)\n",
        "print(\"Team game logs data stored.\")\n"
      ],
      "metadata": {
        "id": "8r9Z1Q8Hev9-",
        "outputId": "5f09be64-48dc-45ae-fd34-86c00deef39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team game logs...\n",
            "Processed game logs saved to 'nba_game_logs.csv'.\n",
            "Team game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_player_game_logs(seasons):\n",
        "    \"\"\"Fetches player game logs for the specified seasons.\"\"\"\n",
        "    print(\"Fetching player game logs...\")\n",
        "    player_game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        player_game_logs = fetch_with_retries(\n",
        "            PlayerGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if player_game_logs:\n",
        "            df_player_game_logs = player_game_logs.get_data_frames()[0]\n",
        "            # Keep only relevant columns\n",
        "            df_player_game_logs = df_player_game_logs[[\n",
        "                \"SEASON_YEAR\", \"GAME_ID\", \"TEAM_ID\", \"PLAYER_ID\", \"PLAYER_NAME\", \"PTS\", \"REB\", \"AST\", \"STL\", \"BLK\",\n",
        "                \"MIN\", \"FG_PCT\", \"FG3_PCT\", \"FT_PCT\", \"TOV\", \"PF\"\n",
        "            ]]\n",
        "            # Modify SEASON_YEAR to keep only the first 4 characters\n",
        "            df_player_game_logs[\"SEASON_YEAR\"] = df_player_game_logs[\"SEASON_YEAR\"].str[:4]\n",
        "            player_game_log_data.append(df_player_game_logs)\n",
        "            time.sleep(0.6)\n",
        "\n",
        "    if player_game_log_data:\n",
        "        df_all_player_game_logs = pd.concat(player_game_log_data, ignore_index=True)\n",
        "        df_all_player_game_logs.to_csv(\"nba_player_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player game log data fetched.\")\n",
        "\n",
        "# Call the function with the specified seasons\n",
        "get_player_game_logs(seasons)\n",
        "print(\"Player game logs data stored.\")"
      ],
      "metadata": {
        "id": "mX-tQtyvex59",
        "outputId": "4f6f5bc7-a551-4f7c-f3d1-52608cb9a682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching player game logs...\n",
            "Player game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_league_game_data():\n",
        "    \"\"\"Fetches league-wide game data with relevant features for a neural network.\"\"\"\n",
        "    print(\"Fetching league game data for NN...\")\n",
        "    game_data = fetch_with_retries(LeagueGameFinder, timeout=60)\n",
        "    if game_data:\n",
        "        df_game_data = game_data.get_data_frames()[0]\n",
        "        # Relevant columns for neural network input\n",
        "        relevant_columns = [\n",
        "            \"SEASON_ID\", \"TEAM_ID\", \"TEAM_ABBREVIATION\", \"TEAM_NAME\", \"GAME_ID\",\n",
        "            \"GAME_DATE\", \"MATCHUP\", \"WL\", \"MIN\", \"PTS\", \"FGM\", \"FGA\", \"FG_PCT\",\n",
        "            \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"OREB\", \"DREB\",\n",
        "            \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PLUS_MINUS\"\n",
        "        ]\n",
        "        df_nn_data = df_game_data[relevant_columns]\n",
        "        df_nn_data.to_csv(\"nba_league_game.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No league game data fetched.\")\n",
        "\n",
        "get_league_game_data()\n",
        "print(\"League game data stored.\")"
      ],
      "metadata": {
        "id": "AQfHSeRyezmh",
        "outputId": "dce76206-3283-4c0f-a6d1-bdd45ef2423f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching league game data for NN...\n",
            "League game data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_league_leaders():\n",
        "    \"\"\"Fetches league leaders data with relevant columns for analysis.\"\"\"\n",
        "    print(\"Fetching league leaders data...\")\n",
        "    leaders_data = fetch_with_retries(LeagueLeaders, timeout=60)\n",
        "    if leaders_data:\n",
        "        df_leaders = leaders_data.get_data_frames()[0]\n",
        "        # Select only relevant columns\n",
        "        relevant_columns = [\n",
        "            \"PLAYER_ID\", \"PLAYER\", \"TEAM_ID\", \"TEAM\", \"GP\", \"MIN\", \"FGM\", \"FGA\",\n",
        "            \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"OREB\",\n",
        "            \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PTS\", \"EFF\"\n",
        "        ]\n",
        "        df_relevant_leaders = df_leaders[relevant_columns]\n",
        "        df_relevant_leaders.to_csv(\"nba_league_leaders_relevant.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No league leaders data fetched.\")\n",
        "\n",
        "get_league_leaders()\n",
        "print(\"League leaders data stored.\")"
      ],
      "metadata": {
        "id": "nemrfltYe1I-",
        "outputId": "c150d1dc-4c1a-40da-da9c-d438a52ce56f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching league leaders data...\n",
            "League leaders data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_player_career_stats():\n",
        "    \"\"\"Fetches career stats for players.\"\"\"\n",
        "    print(\"Fetching player career stats...\")\n",
        "    career_stats_data = []\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        players = team.get(\"players\", [])\n",
        "        for player in players:\n",
        "            career_stats = fetch_with_retries(PlayerCareerStats, player_id=player[\"id\"], timeout=60)\n",
        "            if career_stats:\n",
        "                df_career_stats = career_stats.get_data_frames()[0]\n",
        "                # Keep only relevant columns\n",
        "                df_career_stats = df_career_stats[[\n",
        "                    \"PLAYER_ID\", \"PLAYER_NAME\", \"GP\", \"PTS\", \"REB\", \"AST\", \"FG_PCT\", \"FG3_PCT\", \"FT_PCT\"\n",
        "                ]]\n",
        "                career_stats_data.append(df_career_stats)\n",
        "                time.sleep(0.6)\n",
        "\n",
        "    if career_stats_data:\n",
        "        df_all_career_stats = pd.concat(career_stats_data, ignore_index=True)\n",
        "        df_all_career_stats.to_csv(\"nba_player_career_stats.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player career stats data fetched.\")\n",
        "\n",
        "get_player_career_stats()\n",
        "print(\"Player career stats data stored.\")"
      ],
      "metadata": {
        "id": "MBwUaa-We2i8",
        "outputId": "c046667d-bd79-4ddf-9a37-5d9c348bdb5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching player career stats...\n",
            "No player career stats data fetched.\n",
            "Player career stats data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load player game logs from the CSV file\n",
        "file_path = \"nba_player_game_logs.csv\"  # Update with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Exclude non-numerical columns explicitly\n",
        "no_aggregate_columns = ['PLAYER_ID', 'SEASON_YEAR', 'PLAYER_NAME', 'TEAM_ID', 'GAME_ID']  # Adjust as necessary\n",
        "numerical_columns = [col for col in df.columns if col not in no_aggregate_columns]\n",
        "\n",
        "# Group by PLAYER_ID and SEASON_YEAR\n",
        "grouped = df.groupby(['PLAYER_ID', 'SEASON_YEAR'])\n",
        "\n",
        "# Aggregate numerical columns with mean, std, median, and variance\n",
        "aggregated_data = grouped[numerical_columns].agg(['mean', 'std', 'median', 'var']).reset_index()\n",
        "\n",
        "# Flatten multi-level columns\n",
        "aggregated_data.columns = ['_'.join(col).strip('_') for col in aggregated_data.columns]\n",
        "\n",
        "# Add coefficient of variation (CV) separately\n",
        "for col in numerical_columns:\n",
        "    col_mean = f\"{col}_mean\"\n",
        "    col_std = f\"{col}_std\"\n",
        "    col_cv = f\"{col}_cv\"\n",
        "    aggregated_data[col_cv] = aggregated_data[col_std] / aggregated_data[col_mean]\n",
        "    aggregated_data[col_cv] = aggregated_data[col_cv].replace([float('inf'), -float('inf')], None)  # Handle division by zero\n",
        "\n",
        "# Add non-numerical columns using the first value in the group (like TEAM_ID)\n",
        "aggregated_data['TEAM_ID'] = grouped['TEAM_ID'].first().values\n",
        "\n",
        "# Add games played as a new column\n",
        "aggregated_data['GAMES_PLAYED'] = grouped.size().values\n",
        "\n",
        "# Save the aggregated data for further use\n",
        "output_path = \"nba_player_aggregated_data.csv\"\n",
        "aggregated_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Aggregated data saved to '{output_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dTm6jXFeV36",
        "outputId": "13d85c5f-1179-451c-c25a-e1abb3cc0abf"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data saved to 'nba_player_aggregated_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load aggregated player data\n",
        "player_aggregated_file = \"nba_player_aggregated_data.csv\"  # Update with your actual file path\n",
        "team_abbreviation_file = \"nba_team_data.csv\"  # File containing TEAM_ID to TEAM_ABBREVIATION mapping\n",
        "\n",
        "# Load player data and team abbreviation mapping\n",
        "player_df = pd.read_csv(player_aggregated_file)\n",
        "team_data_df = pd.read_csv(team_abbreviation_file)\n",
        "\n",
        "# Ensure 'MIN_mean' column exists\n",
        "if 'MIN_mean' not in player_df.columns:\n",
        "    raise KeyError(\"The 'MIN_mean' column is missing from the player data. Please verify the input file.\")\n",
        "\n",
        "# Merge team abbreviations into player data\n",
        "player_df = player_df.merge(team_data_df[['TEAM_ID', 'TEAM_ABBREVIATION']], on=\"TEAM_ID\", how=\"left\")\n",
        "\n",
        "# Combine TEAM_ABBREVIATION and SEASON_YEAR into a new column\n",
        "player_df['TEAM_SEASON'] = player_df['TEAM_ABBREVIATION'] + \":\" + player_df['SEASON_YEAR'].astype(str)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['TEAM_ABBREVIATION', 'SEASON_YEAR']\n",
        "if 'GAME_ID' in player_df.columns:  # Check if 'GAME_ID' exists\n",
        "    columns_to_drop.append('GAME_ID')\n",
        "\n",
        "player_df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Define non-numerical columns to exclude\n",
        "no_aggregate_columns = ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_SEASON']\n",
        "numerical_columns = [col for col in player_df.columns if col not in no_aggregate_columns]\n",
        "\n",
        "# Weight each player's stats by their 'MIN_mean'\n",
        "for col in numerical_columns:\n",
        "    if col != 'MIN_mean':  # Avoid weighting the 'MIN_mean' column by itself\n",
        "        player_df[f\"{col}_WEIGHTED\"] = player_df[col] * player_df['MIN_mean']\n",
        "\n",
        "# Group by TEAM_SEASON\n",
        "grouped = player_df.groupby(['TEAM_SEASON'])\n",
        "\n",
        "# Compute team-level weighted averages\n",
        "weighted_stats = grouped[[f\"{col}_WEIGHTED\" for col in numerical_columns if col != 'MIN_mean']].sum()\n",
        "total_minutes = grouped['MIN_mean'].sum()\n",
        "\n",
        "# Calculate weighted averages by dividing the sum of weighted stats by total minutes\n",
        "team_weighted_avg = weighted_stats.div(total_minutes, axis=0)\n",
        "\n",
        "# Rename columns back to their original names\n",
        "team_weighted_avg.columns = [col.replace('_WEIGHTED', '') for col in team_weighted_avg.columns]\n",
        "\n",
        "# Add additional columns\n",
        "team_weighted_avg['TOTAL_MIN'] = total_minutes\n",
        "team_weighted_avg['TEAM_GAMES_PLAYED'] = grouped['GAMES_PLAYED'].sum()\n",
        "\n",
        "# Reset index to flatten the DataFrame\n",
        "team_weighted_avg.reset_index(inplace=True)\n",
        "\n",
        "# Save the aggregated data for further use\n",
        "output_path = \"nba_team_aggregated_data.csv\"\n",
        "team_weighted_avg.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Team aggregated data saved to '{output_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojGa1FcvBwsO",
        "outputId": "991023ad-0f62-40ce-d238-2d74203606f5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team aggregated data saved to 'nba_team_aggregated_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_season_year(game_date):\n",
        "    \"\"\"\n",
        "    Extracts the year from a game date in the format 'YYYY-MM-DDTHH:MM:SS'.\n",
        "\n",
        "    Args:\n",
        "        game_date (str): The game date string.\n",
        "\n",
        "    Returns:\n",
        "        str: The year as a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pd.to_datetime(game_date).year\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date '{game_date}': {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "cydqZx_RvyGo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom accuracy metric to evaluate the model based on given conditions.\n",
        "    \"\"\"\n",
        "    condition_1 = K.cast(y_pred < 0.5, dtype=\"float32\") * K.cast(y_true == 0, dtype=\"float32\")\n",
        "    condition_2 = K.cast(y_pred >= 0.5, dtype=\"float32\") * K.cast(y_true == 1, dtype=\"float32\")\n",
        "    return K.mean(condition_1 + condition_2)\n",
        "\n",
        "\n",
        "def build_neural_network(input_shape):\n",
        "    \"\"\"\n",
        "    Build a neural network model with added regularization and improved architecture.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.5),  # Slight reduction in dropout rate\n",
        "        Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.4),  # Slight reduction in dropout rate\n",
        "        Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[custom_accuracy])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"\n",
        "    Train a neural network model.\n",
        "    \"\"\"\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build the model\n",
        "    model = build_neural_network(X_train.shape[1])\n",
        "\n",
        "    # Early stopping and learning rate scheduler\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=80, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train,\n",
        "              epochs=150,\n",
        "              batch_size=32,  # Reduced batch size\n",
        "              validation_data=(X_val, y_val),\n",
        "              callbacks=[early_stopping, lr_scheduler])\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "# Modify prepare_dataset for improved feature scaling and transformations\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using transformed features (difference and ratio).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load game logs and team features\n",
        "        game_logs = pd.read_csv(game_logs_file, parse_dates=[\"GAME_DATE\"])\n",
        "        team_features = pd.read_csv(features_file)\n",
        "\n",
        "        # Normalize key columns\n",
        "        game_logs[\"TEAM_SEASON1\"] = game_logs[\"TEAM_SEASON1\"].str.strip().str.upper()\n",
        "        game_logs[\"TEAM_SEASON2\"] = game_logs[\"TEAM_SEASON2\"].str.strip().str.upper()\n",
        "\n",
        "        # Merge features for TEAM1 and TEAM2\n",
        "        game_logs = game_logs.merge(\n",
        "            team_features.add_suffix(\"_TEAM1\"),\n",
        "            left_on=[\"TEAM_SEASON1\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM1\"],\n",
        "            how=\"left\"\n",
        "        ).merge(\n",
        "            team_features.add_suffix(\"_TEAM2\"),\n",
        "            left_on=[\"TEAM_SEASON2\"],\n",
        "            right_on=[\"TEAM_SEASON_TEAM2\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = [\"TEAM_SEASON_TEAM1\", \"TEAM_SEASON_TEAM2\", \"GAME_DATE\"]\n",
        "        game_logs.drop(columns=[col for col in columns_to_drop if col in game_logs.columns], inplace=True)\n",
        "\n",
        "        # Handle missing values\n",
        "        game_logs.fillna(0, inplace=True)\n",
        "\n",
        "        # Extract features and target\n",
        "        feature_columns_team1 = [col for col in game_logs.columns if col.endswith(\"_TEAM1\")]\n",
        "        feature_columns_team2 = [col.replace(\"_TEAM1\", \"_TEAM2\") for col in feature_columns_team1]\n",
        "\n",
        "        # Ensure column alignment\n",
        "        feature_columns_team2 = [col for col in feature_columns_team2 if col in game_logs.columns]\n",
        "\n",
        "        # Standardize team features before transformations\n",
        "        scaler = StandardScaler()\n",
        "        team_features_team1 = scaler.fit_transform(game_logs[feature_columns_team1])\n",
        "        team_features_team2 = scaler.fit_transform(game_logs[feature_columns_team2])\n",
        "\n",
        "        # Compute transformed features\n",
        "        X = (team_features_team1 - team_features_team2)  # Simpler transformation\n",
        "        y = (game_logs[\"WL\"] == \"W\").astype(int).to_numpy()\n",
        "\n",
        "        return X, y, feature_columns_team1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in prepare_dataset:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "def main():\n",
        "    game_logs_file = \"nba_game_logs.csv\"\n",
        "    features_file = \"nba_team_aggregated_data.csv\"\n",
        "\n",
        "    # Prepare the dataset\n",
        "    X, y, feature_columns = prepare_dataset(game_logs_file, features_file)\n",
        "\n",
        "    # Train the model\n",
        "    model, scaler = train_model(X, y)\n",
        "\n",
        "    # Evaluate the model\n",
        "    X_scaled = scaler.transform(X)\n",
        "    loss, accuracy = model.evaluate(X_scaled, y)\n",
        "    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_cuhZRejGr7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from nba_api.stats.static import teams\n",
        "from keras.models import load_model\n",
        "import os\n",
        "\n",
        "\n",
        "def prepare_features(team_season1, team_season2, features_file, scaler, feature_columns):\n",
        "    \"\"\"\n",
        "    Prepare input features for prediction by combining team-specific features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load team features\n",
        "        team_features = pd.read_csv(features_file)\n",
        "\n",
        "        # Normalize team names\n",
        "        team_features[\"TEAM_SEASON\"] = team_features[\"TEAM_SEASON\"].str.strip().str.upper()\n",
        "        team_season1 = team_season1.strip().upper()\n",
        "        team_season2 = team_season2.strip().upper()\n",
        "\n",
        "        # Extract features for the two teams\n",
        "        features_team1 = team_features[team_features[\"TEAM_SEASON\"] == team_season1].add_suffix(\"_TEAM1\")\n",
        "        features_team2 = team_features[team_features[\"TEAM_SEASON\"] == team_season2].add_suffix(\"_TEAM2\")\n",
        "\n",
        "        if features_team1.empty or features_team2.empty:\n",
        "            raise ValueError(f\"Features for {team_season1} or {team_season2} not found in the file.\")\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = pd.concat([features_team1.reset_index(drop=True),\n",
        "                                        features_team2.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        # Align with feature_columns and fill missing values with 0\n",
        "        combined_features = combined_features.reindex(columns=feature_columns, fill_value=0)\n",
        "\n",
        "        # Scale features\n",
        "        X = combined_features.to_numpy()\n",
        "        X_scaled = scaler.transform(X)\n",
        "\n",
        "        return X_scaled\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in prepare_features:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "def predict(team_season1, team_season2, model_path, scaler_path, features_file, feature_columns):\n",
        "    \"\"\"\n",
        "    Predict the probability of Team 1 beating Team 2.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate file paths\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n",
        "        if not os.path.exists(scaler_path):\n",
        "            raise FileNotFoundError(f\"Scaler file '{scaler_path}' not found.\")\n",
        "\n",
        "        # Load the model and scaler\n",
        "        model = load_model(model_path, custom_objects={\"custom_accuracy\": custom_accuracy})\n",
        "        scaler = joblib.load(scaler_path)\n",
        "\n",
        "        # Prepare the input features\n",
        "        X_scaled = prepare_features(team_season1, team_season2, features_file, scaler, feature_columns)\n",
        "\n",
        "        # Make predictions\n",
        "        probability = model.predict(X_scaled).flatten()[0]\n",
        "\n",
        "        print(f\"Probability of {team_season1} beating {team_season2}: {probability:.2%}\")\n",
        "        return probability\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in predict:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom accuracy metric to evaluate the model based on given conditions.\n",
        "    \"\"\"\n",
        "    import tensorflow.keras.backend as K\n",
        "    condition_1 = K.cast(y_pred < 0.5, dtype=\"float32\") * K.cast(y_true == 0, dtype=\"float32\")\n",
        "    condition_2 = K.cast(y_pred >= 0.5, dtype=\"float32\") * K.cast(y_true == 1, dtype=\"float32\")\n",
        "    return K.mean(condition_1 + condition_2)\n",
        "\n",
        "\n",
        "def display_team_data(features_file):\n",
        "    \"\"\"\n",
        "    Display available team-season combinations for user reference.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        team_features = pd.read_csv(features_file)\n",
        "        print(\"Available TEAM_SEASON combinations:\")\n",
        "        for team_season in team_features[\"TEAM_SEASON\"].unique():\n",
        "            print(team_season)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in display_team_data:\", e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    features_file = \"nba_team_aggregated_data.csv\"\n",
        "    model_path = \"model.keras\"\n",
        "    scaler_path = \"scaler.pkl\"\n",
        "\n",
        "    # Load feature columns from the training process\n",
        "    try:\n",
        "        feature_columns = pd.read_csv(\"train_data_X.csv\", nrows=0).columns.tolist()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Feature column file 'train_data_X.csv' not found.\")\n",
        "        exit(1)\n",
        "\n",
        "    display_team_data(features_file)\n",
        "\n",
        "    # Example usage\n",
        "    team_season1 = input(\"Enter TEAM_SEASON like team:season (e.g., GSW:2024): \")\n",
        "    team_season2 = input(\"Enter TEAM_SEASON like team:season (e.g., PHI:2024): \")\n",
        "\n",
        "    predict(team_season1, team_season2, model_path, scaler_path, features_file, feature_columns)\n"
      ],
      "metadata": {
        "id": "_ilaXJR5Ud-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}