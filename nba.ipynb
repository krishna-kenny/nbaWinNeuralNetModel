{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh0AeaYnCkzJkyH184TWIn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "840b72db-fbd8-4159-c817-9699806fa407"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nba_api\n",
            "  Downloading nba_api-1.7.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.11/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2025.1.31)\n",
            "Downloading nba_api-1.7.0-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m280.2/280.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nba_api\n",
            "Successfully installed nba_api-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs, LeagueGameFinder, LeagueLeaders, PlayerCareerStats\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "MAX_RETRIES = 3\n",
        "seasons = [\n",
        "    \"2015-16\", \"2016-17\", \"2017-18\", \"2018-19\", \"2019-20\",\n",
        "    \"2020-21\", \"2021-22\", \"2022-23\", \"2023-24\", \"2024-25\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "SJ2v4Yzgep72"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "DV6BxvlTerjE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_info(seasons):\n",
        "    \"\"\"Fetches relevant team information for the specified seasons.\"\"\"\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for team in nba_teams:\n",
        "        team_info = fetch_with_retries(\n",
        "            TeamInfoCommon,\n",
        "            team_id=team[\"id\"],\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if team_info:\n",
        "            df_team = team_info.get_data_frames()[0]\n",
        "            team_data.append(df_team)\n",
        "            time.sleep(0.6)\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "get_team_info(seasons[-1])\n",
        "print(\"Team information data stored.\")"
      ],
      "metadata": {
        "id": "uflUFIR0et8B",
        "outputId": "4c158610-cdfb-438e-f5a6-a4fc1acdf9d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Team information data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_game_logs(seasons):\n",
        "    \"\"\"Fetches team game logs for the specified seasons and processes the MATCHUP column.\"\"\"\n",
        "    print(\"Fetching team game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            game_log_data.append(df_game_logs)\n",
        "\n",
        "    if game_log_data:\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "        matchups_split = df_all_game_logs['MATCHUP'].str.split(' @ | vs. ', expand=True)\n",
        "        df_all_game_logs['TEAM1'] = matchups_split[0]\n",
        "        df_all_game_logs['TEAM2'] = matchups_split[1]\n",
        "        df_all_game_logs.drop(columns=['MATCHUP'], inplace=True)\n",
        "\n",
        "        df_all_game_logs['SEASON_YEAR'] = pd.to_datetime(df_all_game_logs['GAME_DATE']).dt.year.astype(str)\n",
        "\n",
        "        df_all_game_logs.to_csv(\"nba_game_logs.csv\", index=False)\n",
        "        print(\"Processed game logs saved to 'nba_game_logs.csv'.\")\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "get_team_game_logs(seasons)\n",
        "print(\"Team game logs data stored.\")"
      ],
      "metadata": {
        "id": "AQfHSeRyezmh",
        "outputId": "5e5e8076-a2bd-418c-dfce-8bb8c528fe14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team game logs...\n",
            "Processed game logs saved to 'nba_game_logs.csv'.\n",
            "Team game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_game_logs():\n",
        "    file_path = \"nba_game_logs.csv\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"])\n",
        "\n",
        "    current_season = df[\"SEASON_YEAR\"].max()\n",
        "    df = df[df[\"SEASON_YEAR\"] <= current_season]\n",
        "\n",
        "    aggregation_rules = {\n",
        "        \"GAME_ID\": \"count\",\n",
        "        \"WL\": lambda x: (x == \"W\").sum(),\n",
        "        \"PTS\": \"sum\",\n",
        "        \"FGM\": \"sum\", \"FGA\": \"sum\", \"FG_PCT\": \"mean\",\n",
        "        \"FG3M\": \"sum\", \"FG3A\": \"sum\", \"FG3_PCT\": \"mean\",\n",
        "        \"FTM\": \"sum\", \"FTA\": \"sum\", \"FT_PCT\": \"mean\",\n",
        "        \"OREB\": \"sum\", \"DREB\": \"sum\", \"REB\": \"sum\",\n",
        "        \"AST\": \"sum\", \"TOV\": \"sum\", \"STL\": \"sum\", \"BLK\": \"sum\", \"BLKA\": \"sum\",\n",
        "        \"PF\": \"sum\", \"PFD\": \"sum\", \"PLUS_MINUS\": \"sum\"\n",
        "    }\n",
        "\n",
        "    aggregated_df = df.groupby([\"SEASON_YEAR\", \"TEAM_ID\", \"TEAM_ABBREVIATION\"]).agg(aggregation_rules)\n",
        "\n",
        "    aggregated_df.rename(columns={\"GAME_ID\": \"GP\", \"WL\": \"W\"}, inplace=True)\n",
        "\n",
        "    aggregated_df[\"L\"] = aggregated_df[\"GP\"] - aggregated_df[\"W\"]\n",
        "    aggregated_df[\"W_PCT\"] = aggregated_df[\"W\"] / aggregated_df[\"GP\"]\n",
        "\n",
        "    aggregated_df.reset_index(inplace=True)\n",
        "\n",
        "    output_file = \"nba_aggregated_data.csv\"\n",
        "    aggregated_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Aggregated data saved to {output_file}\")\n",
        "\n",
        "process_game_logs()\n",
        "print(\"Aggregated data stored.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79J47RZ79zPY",
        "outputId": "9cce084c-2957-47a7-de25-cd165675324f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data saved to nba_aggregated_data.csv\n",
            "Aggregated data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "import joblib\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "@register_keras_serializable()\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, dtype=\"float32\")\n",
        "    return K.mean(K.equal(K.round(y_pred), y_true))\n",
        "\n",
        "def prepare_training_data(game_logs_file, aggregated_file):\n",
        "    df_logs = pd.read_csv(game_logs_file, parse_dates=[\"GAME_DATE\"])\n",
        "    df_agg = pd.read_csv(aggregated_file)\n",
        "    df_logs[\"TEAM1\"] = df_logs[\"TEAM1\"].str.strip().str.upper()\n",
        "    df_logs[\"TEAM2\"] = df_logs[\"TEAM2\"].str.strip().str.upper()\n",
        "    df_agg[\"TEAM_ABBREVIATION\"] = df_agg[\"TEAM_ABBREVIATION\"].str.strip().str.upper()\n",
        "    df_logs[\"SEASON_YEAR\"] = df_logs[\"SEASON_YEAR\"].astype(str).str.strip()\n",
        "    df_agg[\"SEASON_YEAR\"] = df_agg[\"SEASON_YEAR\"].astype(str).str.strip()\n",
        "    # Merge aggregated stats for TEAM1 and TEAM2\n",
        "    df_train = df_logs.merge(\n",
        "        df_agg.add_suffix(\"_TEAM1\"),\n",
        "        left_on=[\"SEASON_YEAR\", \"TEAM1\"],\n",
        "        right_on=[\"SEASON_YEAR_TEAM1\", \"TEAM_ABBREVIATION_TEAM1\"],\n",
        "        how=\"left\"\n",
        "    ).merge(\n",
        "        df_agg.add_suffix(\"_TEAM2\"),\n",
        "        left_on=[\"SEASON_YEAR\", \"TEAM2\"],\n",
        "        right_on=[\"SEASON_YEAR_TEAM2\", \"TEAM_ABBREVIATION_TEAM2\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "    df_train.dropna(inplace=True)\n",
        "    df_train[\"WL\"] = df_train[\"WL\"].astype(str).str.strip().str.upper()\n",
        "    y = (df_train[\"WL\"] == \"W\").astype(int).to_numpy()\n",
        "    # Define base features: use only numeric columns from aggregated data (discard TEAM_ID, TEAM_ABBREVIATION, SEASON_YEAR)\n",
        "    base_cols = [col for col in df_agg.columns if col not in [\"TEAM_ID\", \"TEAM_ABBREVIATION\", \"SEASON_YEAR\"]]\n",
        "    feature_cols = [f\"{col}_TEAM1\" for col in base_cols] + [f\"{col}_TEAM2\" for col in base_cols]\n",
        "    X = df_train[feature_cols].to_numpy()\n",
        "    joblib.dump(feature_cols, \"feature_names.pkl\")\n",
        "    return X, y, feature_cols\n",
        "\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[custom_accuracy])\n",
        "    return model\n",
        "\n",
        "def train_model(X, y):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "    model = build_model(X_train.shape[1])\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_val, y_val))\n",
        "    return model, scaler\n",
        "\n",
        "def main():\n",
        "    game_logs_file = \"nba_game_logs.csv\"\n",
        "    aggregated_file = \"nba_aggregated_data.csv\"\n",
        "    X, y, _ = prepare_training_data(game_logs_file, aggregated_file)\n",
        "    model, scaler = train_model(X, y)\n",
        "    model.save(\"model.keras\")\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_cuhZRejGr7s",
        "outputId": "c4c1ae34-d539-4e1f-a87c-17dce92f9c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3ms/step - custom_accuracy: 0.6320 - loss: 0.6503 - val_custom_accuracy: 0.6534 - val_loss: 0.6260\n",
            "Epoch 2/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3ms/step - custom_accuracy: 0.6593 - loss: 0.6264 - val_custom_accuracy: 0.6521 - val_loss: 0.6281\n",
            "Epoch 3/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - custom_accuracy: 0.6625 - loss: 0.6217 - val_custom_accuracy: 0.6622 - val_loss: 0.6241\n",
            "Epoch 4/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 3ms/step - custom_accuracy: 0.6555 - loss: 0.6273 - val_custom_accuracy: 0.6570 - val_loss: 0.6556\n",
            "Epoch 5/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - custom_accuracy: 0.6547 - loss: 0.6263 - val_custom_accuracy: 0.6575 - val_loss: 0.6217\n",
            "Epoch 6/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - custom_accuracy: 0.6569 - loss: 0.6240 - val_custom_accuracy: 0.6596 - val_loss: 0.6241\n",
            "Epoch 7/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - custom_accuracy: 0.6596 - loss: 0.6207 - val_custom_accuracy: 0.6586 - val_loss: 0.6206\n",
            "Epoch 8/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - custom_accuracy: 0.6577 - loss: 0.6233 - val_custom_accuracy: 0.6530 - val_loss: 0.6238\n",
            "Epoch 9/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3ms/step - custom_accuracy: 0.6607 - loss: 0.6249 - val_custom_accuracy: 0.6568 - val_loss: 0.6202\n",
            "Epoch 10/10\n",
            "\u001b[1m18564/18564\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3ms/step - custom_accuracy: 0.6621 - loss: 0.6255 - val_custom_accuracy: 0.6605 - val_loss: 0.6311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.keras.backend as K\n",
        "from nba_api.stats.static import teams\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "@register_keras_serializable()\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, dtype=\"float32\")\n",
        "    return K.mean(K.equal(K.round(y_pred), y_true))\n",
        "\n",
        "def load_feature_names(feature_names_file=\"feature_names.pkl\"):\n",
        "    return joblib.load(feature_names_file)\n",
        "\n",
        "def fetch_team_stats(team_abbr, season_year, aggregated_file, base_features):\n",
        "    df = pd.read_csv(aggregated_file)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    df[\"TEAM_ABBREVIATION\"] = df[\"TEAM_ABBREVIATION\"].str.strip().str.upper()\n",
        "    df[\"SEASON_YEAR\"] = df[\"SEASON_YEAR\"].astype(str).str.strip()\n",
        "    row = df[(df[\"TEAM_ABBREVIATION\"] == team_abbr.upper()) & (df[\"SEASON_YEAR\"] == str(season_year))]\n",
        "    if row.empty:\n",
        "        raise ValueError(f\"‚ùå Features for team '{team_abbr}' in {season_year} not found.\")\n",
        "    return row[base_features].iloc[0]\n",
        "\n",
        "def predict_matchup_win_probability(team1_abbr, team2_abbr, season_year, aggregated_file,\n",
        "                                    model_path=\"model.keras\", scaler_path=\"scaler.pkl\", feature_names_file=\"feature_names.pkl\"):\n",
        "    model = load_model(model_path, custom_objects={\"custom_accuracy\": custom_accuracy})\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    # Load expected feature names (e.g. ['GP_TEAM1', 'W_TEAM1', ... 'GP_TEAM2', 'W_TEAM2', ...])\n",
        "    expected_feature_names = load_feature_names(feature_names_file)\n",
        "    # Determine base features from expected_feature_names (remove suffixes)\n",
        "    base_features = sorted({name.rsplit(\"_\", 1)[0] for name in expected_feature_names})\n",
        "    df = pd.read_csv(aggregated_file)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    # For team1 and team2, fetch base aggregated stats\n",
        "    team1_stats = fetch_team_stats(team1_abbr, season_year, aggregated_file, base_features)\n",
        "    team2_stats = fetch_team_stats(team2_abbr, season_year, aggregated_file, base_features)\n",
        "    team1_stats.index = [f\"{col}_TEAM1\" for col in team1_stats.index]\n",
        "    team2_stats.index = [f\"{col}_TEAM2\" for col in team2_stats.index]\n",
        "    merged = pd.concat([team1_stats, team2_stats])\n",
        "    merged = merged.reindex(expected_feature_names, fill_value=0)\n",
        "    X_input = merged.values.reshape(1, -1)\n",
        "    if X_input.shape[1] != scaler.mean_.shape[0]:\n",
        "        raise ValueError(f\"‚ùå Feature size mismatch! Model expects {scaler.mean_.shape[0]}, got {X_input.shape[1]}.\")\n",
        "    X_scaled = scaler.transform(X_input)\n",
        "    prob = model.predict(X_scaled)[0][0]\n",
        "    print(f\"\\nüèÄ Win Probability for {team1_abbr} vs {team2_abbr} in {season_year}: {prob*100:.2f}%\")\n",
        "\n",
        "def display_team_data():\n",
        "    nba = teams.get_teams()\n",
        "    for t in nba:\n",
        "        print(f\"{t['abbreviation']} - {t['full_name']}\")\n",
        "\n",
        "def main():\n",
        "    aggregated_file = \"nba_aggregated_data.csv\"\n",
        "    display_team_data()\n",
        "    team1 = input(\"Enter Team 1 abbreviation: \").strip().upper()\n",
        "    team2 = input(\"Enter Team 2 abbreviation: \").strip().upper()\n",
        "    season_year = int(input(\"Enter season year: \"))\n",
        "    try:\n",
        "        predict_matchup_win_probability(team1, team2, season_year, aggregated_file)\n",
        "    except ValueError as e:\n",
        "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_ilaXJR5Ud-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f3ffcb-4ad3-4b49-baee-3826a788917f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ATL - Atlanta Hawks\n",
            "BOS - Boston Celtics\n",
            "CLE - Cleveland Cavaliers\n",
            "NOP - New Orleans Pelicans\n",
            "CHI - Chicago Bulls\n",
            "DAL - Dallas Mavericks\n",
            "DEN - Denver Nuggets\n",
            "GSW - Golden State Warriors\n",
            "HOU - Houston Rockets\n",
            "LAC - Los Angeles Clippers\n",
            "LAL - Los Angeles Lakers\n",
            "MIA - Miami Heat\n",
            "MIL - Milwaukee Bucks\n",
            "MIN - Minnesota Timberwolves\n",
            "BKN - Brooklyn Nets\n",
            "NYK - New York Knicks\n",
            "ORL - Orlando Magic\n",
            "IND - Indiana Pacers\n",
            "PHI - Philadelphia 76ers\n",
            "PHX - Phoenix Suns\n",
            "POR - Portland Trail Blazers\n",
            "SAC - Sacramento Kings\n",
            "SAS - San Antonio Spurs\n",
            "OKC - Oklahoma City Thunder\n",
            "TOR - Toronto Raptors\n",
            "UTA - Utah Jazz\n",
            "MEM - Memphis Grizzlies\n",
            "WAS - Washington Wizards\n",
            "DET - Detroit Pistons\n",
            "CHA - Charlotte Hornets\n",
            "Enter Team 1 abbreviation: CHI\n",
            "Enter Team 2 abbreviation: PHI\n",
            "Enter season year: 2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a9af4603c40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\n",
            "üèÄ Win Probability for CHI vs PHI in 2025: 49.85%\n"
          ]
        }
      ]
    }
  ]
}