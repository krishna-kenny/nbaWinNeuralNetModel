{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm1IB6I4IvudD4k9VDBJgv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "a03575bb-e314-4c59-ac8f-cf1e42066c6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nba_api in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugZUl0n8I8qO",
        "outputId": "905b6bd9-c6d5-4b01-fac1-15687b995f84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODhh0g7xGfMv",
        "outputId": "cb8addba-683d-4865-fb53-8a4b43bb41ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Error: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=60). Retrying in 1 seconds...\n",
            "Team information data stored.\n",
            "Fetching game logs...\n",
            "Game logs data stored.\n",
            "Fetching player game logs...\n",
            "Player game logs data stored.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "# Maximum number of retries for each API call\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "\n",
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt  # Exponential backoff\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_team_info(seasons):\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        for team in nba_teams:\n",
        "            team_info = fetch_with_retries(\n",
        "                TeamInfoCommon,\n",
        "                team_id=team[\"id\"],\n",
        "                season_type_nullable=\"Regular Season\",\n",
        "                timeout=60,\n",
        "            )\n",
        "            if team_info:\n",
        "                df_team = team_info.get_data_frames()[0]\n",
        "                team_data.append(df_team)\n",
        "                time.sleep(0.6)  # Delay to avoid API rate limits\n",
        "            else:\n",
        "                print(\n",
        "                    f\"Skipping team {team['full_name']} for season {season} after failed attempts.\"\n",
        "                )\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"data/raw/nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "\n",
        "def get_game_logs(seasons):\n",
        "    print(\"Fetching game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            game_log_data.append(df_game_logs)\n",
        "            time.sleep(0.6)\n",
        "        else:\n",
        "            print(f\"Skipping game logs for season {season} after failed attempts.\")\n",
        "\n",
        "    if game_log_data:\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "        df_all_game_logs.to_csv(\"data/raw/nba_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "\n",
        "def get_player_game_logs(seasons):\n",
        "    print(\"Fetching player game logs...\")\n",
        "    player_game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        player_game_logs = fetch_with_retries(\n",
        "            PlayerGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if player_game_logs:\n",
        "            df_player_game_logs = player_game_logs.get_data_frames()[0]\n",
        "            player_game_log_data.append(df_player_game_logs)\n",
        "            time.sleep(0.6)\n",
        "        else:\n",
        "            print(\n",
        "                f\"Skipping player game logs for season {season} after failed attempts.\"\n",
        "            )\n",
        "\n",
        "    if player_game_log_data:\n",
        "        df_all_player_game_logs = pd.concat(player_game_log_data, ignore_index=True)\n",
        "        df_all_player_game_logs.to_csv(\"data/raw/nba_player_game_logs.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No player game log data fetched.\")\n",
        "\n",
        "\n",
        "# Define the list of seasons\n",
        "seasons = [\"2023-24\", \"2024-25\"]\n",
        "\n",
        "# Run functions to save data to CSV files\n",
        "get_team_info(seasons[-1])\n",
        "print(\"Team information data stored.\")\n",
        "\n",
        "get_game_logs(seasons)\n",
        "print(\"Game logs data stored.\")\n",
        "\n",
        "get_player_game_logs(seasons[-2:])  # Fetching for the most recent two seasons only\n",
        "print(\"Player game logs data stored.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to preprocess team data\n",
        "def preprocess_team_data(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save team data by removing unnecessary columns.\"\"\"\n",
        "    team_data = pd.read_csv(input_file).dropna()\n",
        "    columns_to_drop = [\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_CITY\",\n",
        "        \"SEASON_YEAR\",\n",
        "        \"TEAM_CODE\",\n",
        "        \"TEAM_DIVISION\",\n",
        "        \"MIN_YEAR\",\n",
        "        \"MAX_YEAR\",\n",
        "    ]\n",
        "    team_data = team_data.drop(columns=columns_to_drop)\n",
        "    team_data = team_data.drop_duplicates()\n",
        "    team_data.to_csv(output_file, index=False)\n",
        "    print(f\"Team data preprocessed and saved. rows: {team_data.shape}\")\n",
        "\n",
        "\n",
        "# Function to preprocess game logs\n",
        "def preprocess_game_logs(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save game log data with specific transformations.\"\"\"\n",
        "    game_logs = pd.read_csv(input_file).dropna()\n",
        "\n",
        "    # Convert 'GAME_DATE' to datetime and sort by date\n",
        "    game_logs[\"GAME_DATE\"] = pd.to_datetime(game_logs[\"GAME_DATE\"])\n",
        "    game_logs_sorted = game_logs.sort_values(by=\"GAME_DATE\", ascending=True)\n",
        "\n",
        "    # Extract 'TEAM1' and 'TEAM2' from 'MATCHUP' column\n",
        "    game_logs_sorted[\"TEAM1\"] = game_logs_sorted[\"MATCHUP\"].str.split().str[0]\n",
        "    game_logs_sorted[\"TEAM2\"] = game_logs_sorted[\"MATCHUP\"].str.split().str[2]\n",
        "\n",
        "    # Drop columns that are not required\n",
        "    columns_to_drop = [\n",
        "        \"MATCHUP\",\n",
        "        \"AVAILABLE_FLAG\",\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_ABBREVIATION\",\n",
        "        \"GAME_ID\",\n",
        "    ]\n",
        "    game_logs_cleaned = game_logs_sorted.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Convert 'WL' to binary format: 'W' becomes 1, 'L' becomes 0\n",
        "    game_logs_cleaned[\"WL\"] = game_logs_cleaned[\"WL\"].apply(\n",
        "        lambda result: 1 if result == \"W\" else 0\n",
        "    )\n",
        "\n",
        "    # Convert 'SEASON_YEAR' to integer format, using only the starting year\n",
        "    game_logs_cleaned[\"SEASON_YEAR\"] = game_logs_cleaned[\"SEASON_YEAR\"].apply(\n",
        "        lambda year: int(year[:4])\n",
        "    )\n",
        "\n",
        "    game_logs_cleaned.to_csv(output_file, index=False)\n",
        "    print(f\"Game logs preprocessed and saved. rows: {game_logs_cleaned.shape}\")\n",
        "\n",
        "\n",
        "# Function to preprocess player game logs\n",
        "def preprocess_player_game_logs(input_file, output_file):\n",
        "    \"\"\"Load, clean, and save player game log data with specific transformations.\"\"\"\n",
        "    player_game_logs = pd.read_csv(input_file).dropna()\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = [\n",
        "        \"PLAYER_NAME\",\n",
        "        \"NICKNAME\",\n",
        "        \"TEAM_NAME\",\n",
        "        \"TEAM_ABBREVIATION\",\n",
        "        \"MATCHUP\",\n",
        "        \"GAME_ID\",\n",
        "        \"MIN_SEC\",\n",
        "    ]\n",
        "    player_game_logs = player_game_logs.drop(columns=columns_to_drop)\n",
        "    player_game_logs[\"WL\"] = player_game_logs[\"WL\"].apply(\n",
        "        lambda x: 1 if x == \"W\" else 0\n",
        "    )\n",
        "    player_game_logs[\"SEASON_YEAR\"] = player_game_logs[\"SEASON_YEAR\"].apply(\n",
        "        lambda x: x[:4]\n",
        "    )\n",
        "\n",
        "    player_game_logs.to_csv(output_file, index=False)\n",
        "    print(f\"Player game logs preprocessed and saved. rows: {player_game_logs.shape}\")\n",
        "\n",
        "\n",
        "# Function to compute weighted averages\n",
        "def compute_weighted_avg(player_id, df):\n",
        "    \"\"\"Compute weighted averages for a given player.\"\"\"\n",
        "    player_rows = df[df[\"PLAYER_ID\"] == player_id].copy()\n",
        "\n",
        "    # Retain TEAM_ID\n",
        "    team_id = player_rows[\"TEAM_ID\"].iloc[0]\n",
        "\n",
        "    # Convert GAME_DATE to a timestamp for weighting\n",
        "    player_rows[\"GAME_TIMESTAMP\"] = pd.to_datetime(player_rows[\"GAME_DATE\"]).apply(\n",
        "        lambda x: x.timestamp()\n",
        "    )\n",
        "    max_timestamp = player_rows[\"GAME_TIMESTAMP\"].max()\n",
        "\n",
        "    # Calculate weights based on recency\n",
        "    player_rows[\"WEIGHTS\"] = np.exp(\n",
        "        (player_rows[\"GAME_TIMESTAMP\"] - max_timestamp) / 1e7\n",
        "    )\n",
        "    player_rows[\"WEIGHTS\"] /= player_rows[\"WEIGHTS\"].sum()\n",
        "\n",
        "    # Compute weighted average for all columns after WL\n",
        "    weighted_avg = (\n",
        "        player_rows.iloc[:, df.columns.get_loc(\"WL\") + 1 :]  # Select columns after WL\n",
        "        .mul(player_rows[\"WEIGHTS\"], axis=0)  # Multiply each column by weights\n",
        "        .sum()  # Sum the weighted values for each column\n",
        "    )\n",
        "    weighted_avg[\"TEAM_ID\"] = team_id  # Include TEAM_ID in the output\n",
        "    return weighted_avg\n",
        "\n",
        "\n",
        "# Function to create feature data\n",
        "def create_feature_data(input_csv, output_csv):\n",
        "    \"\"\"Generate feature data where each player is represented by a single row.\"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_csv)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ensure GAME_DATE is parsed correctly\n",
        "    df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"])\n",
        "\n",
        "    # Get unique players\n",
        "    unique_players = df[\"PLAYER_ID\"].unique()\n",
        "\n",
        "    # Create a new dataframe to store the features\n",
        "    feature_data = []\n",
        "\n",
        "    for player_id in unique_players:\n",
        "        weighted_avg = compute_weighted_avg(player_id, df)\n",
        "        weighted_avg[\"PLAYER_ID\"] = player_id  # Retain the player ID\n",
        "        feature_data.append(weighted_avg)\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    feature_df = pd.DataFrame(feature_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    feature_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Feature data saved to {output_csv}. rows: {feature_df.shape}\")\n",
        "\n",
        "\n",
        "# Function to compute team-level aggregated features\n",
        "def create_team_features(player_features_file, team_features_output):\n",
        "    \"\"\"\n",
        "    Generate aggregated features for each team by averaging player statistics.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load the player features data\n",
        "    player_data = pd.read_csv(player_features_file)\n",
        "\n",
        "    # Compute team features\n",
        "    team_features = player_data.groupby(\"TEAM_ID\").mean().reset_index()\n",
        "\n",
        "    # Load mapping of TEAM_ID to TEAM_ABBREVIATION and additional features\n",
        "    preprocessed_nba_team_data = pd.read_csv(\n",
        "        \"data/processed/preprocessed_nba_team_data.csv\"\n",
        "    )\n",
        "    id_to_abbr_map = preprocessed_nba_team_data.set_index(\"TEAM_ID\")[\n",
        "        \"TEAM_ABBREVIATION\"\n",
        "    ].to_dict()\n",
        "\n",
        "    # Apply a function to map TEAM_ID to ABBR\n",
        "    team_features[\"TEAM_ID\"] = team_features[\"TEAM_ID\"].map(id_to_abbr_map)\n",
        "\n",
        "    # Rename the column\n",
        "    team_features.rename(columns={\"TEAM_ID\": \"TEAM_ABBREVIATION\"}, inplace=True)\n",
        "\n",
        "    # Merge additional team features\n",
        "    additional_features = preprocessed_nba_team_data.drop(\n",
        "        columns=[\"TEAM_ID\", \"TEAM_CONFERENCE\", \"TEAM_SLUG\"]\n",
        "    )\n",
        "    team_features = team_features.merge(\n",
        "        additional_features,\n",
        "        left_on=\"TEAM_ABBREVIATION\",\n",
        "        right_on=\"TEAM_ABBREVIATION\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # Save the resulting team features to a CSV file\n",
        "    team_features.to_csv(team_features_output, index=False)\n",
        "    print(\n",
        "        f\"Team feature data saved to {team_features_output}. rows: {team_features.shape}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# File paths for input and output data\n",
        "team_data_file = \"data/raw/nba_team_data.csv\"\n",
        "team_data_output = \"data/processed/preprocessed_nba_team_data.csv\"\n",
        "\n",
        "game_data_file = \"data/raw/nba_game_logs.csv\"\n",
        "game_data_output = \"data/processed/preprocessed_nba_game_logs.csv\"\n",
        "\n",
        "player_game_logs_file = \"data/raw/nba_player_game_logs.csv\"\n",
        "player_game_logs_output = \"data/processed/preprocessed_nba_player_game_logs.csv\"\n",
        "\n",
        "# Run the preprocessing functions\n",
        "preprocess_team_data(team_data_file, team_data_output)\n",
        "preprocess_game_logs(game_data_file, game_data_output)\n",
        "preprocess_player_game_logs(player_game_logs_file, player_game_logs_output)\n",
        "create_feature_data(player_game_logs_output, \"data/nba_player_features.csv\")\n",
        "\n",
        "# File paths for player features and team output\n",
        "player_features_file = \"data/nba_player_features.csv\"\n",
        "team_features_output = \"data/nba_team_features.csv\"\n",
        "\n",
        "# Run the team feature creation function\n",
        "create_team_features(player_features_file, team_features_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSorjKxGouB",
        "outputId": "f620701c-462f-4214-dae7-01dc9d312ccd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team data preprocessed and saved. rows: (30, 9)\n",
            "Game logs preprocessed and saved. rows: (3238, 54)\n",
            "Player game logs preprocessed and saved. rows: (34803, 62)\n",
            "Feature data saved to data/nba_player_features.csv. rows: (660, 61)\n",
            "Team feature data saved to data/nba_team_features.csv. rows: (30, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # To save and load the scaler\n",
        "\n",
        "\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using team-specific features.\n",
        "\n",
        "    Args:\n",
        "        game_logs_file: CSV file containing game logs with TEAM1, TEAM2, and WL columns.\n",
        "        features_file: CSV file to save aggregated team features.\n",
        "\n",
        "    Returns:\n",
        "        X: Feature matrix for training.\n",
        "        y: Target vector (win/loss).\n",
        "    \"\"\"\n",
        "    # Load game logs\n",
        "    game_logs = pd.read_csv(game_logs_file)\n",
        "\n",
        "    # Aggregate features by team\n",
        "    team_features = (\n",
        "        game_logs.groupby(\"TEAM1\")\n",
        "        .mean(numeric_only=True)\n",
        "        .reset_index()\n",
        "        .rename(columns={\"TEAM1\": \"TEAM\"})\n",
        "    )\n",
        "\n",
        "    # Save the aggregated features to features_file\n",
        "    team_features.to_csv(features_file, index=False)\n",
        "\n",
        "    # Merge team features for TEAM1 and TEAM2\n",
        "    game_logs = game_logs.merge(\n",
        "        team_features,\n",
        "        how=\"left\",\n",
        "        left_on=\"TEAM1\",\n",
        "        right_on=\"TEAM\",\n",
        "        suffixes=(\"\", \"_TEAM1\"),\n",
        "    ).merge(\n",
        "        team_features,\n",
        "        how=\"left\",\n",
        "        left_on=\"TEAM2\",\n",
        "        right_on=\"TEAM\",\n",
        "        suffixes=(\"\", \"_TEAM2\"),\n",
        "    )\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = [\n",
        "        \"TEAM\",\n",
        "        \"TEAM_TEAM2\",\n",
        "        \"TEAM_CONFERENCE\",\n",
        "        \"TEAM_SLUG\",\n",
        "        \"TEAM_CONFERENCE_TEAM2\",\n",
        "        \"TEAM_SLUG_TEAM2\",\n",
        "        \"PLAYER_ID\",\n",
        "        \"PLAYER_ID_TEAM2\",\n",
        "        \"AVAILABLE_FLAG\",\n",
        "        \"AVAILABLE_FLAG_TEAM2\",\n",
        "        \"GAME_TIMESTAMP\",\n",
        "        \"GAME_TIMESTAMP_TEAM2\",\n",
        "    ]\n",
        "    game_logs.drop(\n",
        "        columns=[col for col in columns_to_drop if col in game_logs.columns],\n",
        "        inplace=True,\n",
        "    )\n",
        "\n",
        "    # Feature engineering: Create new features for differences and ratios\n",
        "    numeric_columns = game_logs.filter(regex=\"_TEAM1$\").columns\n",
        "    diff_features = {}\n",
        "    ratio_features = {}\n",
        "    for col in numeric_columns:\n",
        "        base_col = col.replace(\"_TEAM1\", \"\")\n",
        "        diff_features[f\"{base_col}_DIFF\"] = (\n",
        "            game_logs[f\"{base_col}_TEAM1\"] - game_logs[f\"{base_col}_TEAM2\"]\n",
        "        )\n",
        "        ratio_features[f\"{base_col}_RATIO\"] = game_logs[f\"{base_col}_TEAM1\"] / (\n",
        "            game_logs[f\"{base_col}_TEAM2\"] + 1e-5\n",
        "        )\n",
        "\n",
        "    # Add all new features at once to optimize performance\n",
        "    new_features = pd.concat(\n",
        "        [pd.DataFrame(diff_features), pd.DataFrame(ratio_features)], axis=1\n",
        "    )\n",
        "    game_logs = pd.concat([game_logs, new_features], axis=1)\n",
        "\n",
        "    # Handle missing values\n",
        "    game_logs.fillna(0, inplace=True)\n",
        "\n",
        "    # Extract features and target\n",
        "    feature_columns = game_logs.select_dtypes(include=np.number).columns.difference(\n",
        "        [\"WL\"]\n",
        "    )\n",
        "    X = game_logs[feature_columns].to_numpy()\n",
        "    y = game_logs[\"WL\"].astype(int).to_numpy()\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"\n",
        "    Train a neural network model on the given features and labels.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix for training.\n",
        "        y: Target vector (win/loss).\n",
        "\n",
        "    Returns:\n",
        "        model: Trained neural network model.\n",
        "        scaler: Fitted scaler for feature normalization.\n",
        "    \"\"\"\n",
        "    # Normalize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Handle class imbalance\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "    # Define the neural network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation=\"relu\", input_shape=(X_resampled.shape[1],)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation=\"relu\"))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_resampled, y_resampled, epochs=16, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "def load_trained_model(\n",
        "    model_path=\"saved_model/model.h5\", scaler_path=\"saved_model/scaler.pkl\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Load the pre-trained neural network model and the scaler.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model file.\n",
        "        scaler_path: Path to the saved scaler file.\n",
        "\n",
        "    Returns:\n",
        "        model: The pre-trained model.\n",
        "        scaler: The scaler used for feature normalization.\n",
        "    \"\"\"\n",
        "    # Load the trained model\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Load the scaler\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    game_logs_file = \"data/processed/preprocessed_nba_game_logs.csv\"\n",
        "    features_file = \"data/features.csv\"\n",
        "    model_save_path = \"saved_model/model.h5\"\n",
        "    scaler_save_path = \"saved_model/scaler.pkl\"\n",
        "\n",
        "    # Prepare the dataset\n",
        "    X, y = prepare_dataset(game_logs_file, features_file)\n",
        "\n",
        "    # Check if data is valid for training\n",
        "    if X.size == 0 or y.size == 0:\n",
        "        print(\"No data available to train the model.\")\n",
        "    else:\n",
        "        # Split the dataset into training and test sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Train the neural network\n",
        "        model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "        # Save the trained model\n",
        "        model.save(model_save_path)\n",
        "        print(f\"Neural network model saved to {model_save_path}\")\n",
        "\n",
        "        # Save the scaler\n",
        "        joblib.dump(scaler, scaler_save_path)\n",
        "        print(f\"Scaler saved to {scaler_save_path}\")\n",
        "\n",
        "        # Evaluate the neural network\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "        print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "        # Train and evaluate a Random Forest for comparison\n",
        "        rf = RandomForestClassifier(random_state=42)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred_rf = rf.predict(X_test)\n",
        "        rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "        print(f\"Random Forest Accuracy: {rf_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cuhZRejGr7s",
        "outputId": "35c2a216-5d24-4a66-e237-17f08ffad560"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7058 - loss: 0.5275 - val_accuracy: 0.9675 - val_loss: 0.0899\n",
            "Epoch 2/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9738 - loss: 0.0705 - val_accuracy: 0.9790 - val_loss: 0.0450\n",
            "Epoch 3/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9828 - loss: 0.0399 - val_accuracy: 0.9809 - val_loss: 0.0537\n",
            "Epoch 4/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9900 - loss: 0.0260 - val_accuracy: 0.9790 - val_loss: 0.0535\n",
            "Epoch 5/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9905 - loss: 0.0274 - val_accuracy: 0.9943 - val_loss: 0.0160\n",
            "Epoch 6/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9903 - loss: 0.0297 - val_accuracy: 0.9924 - val_loss: 0.0347\n",
            "Epoch 7/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9890 - loss: 0.0209 - val_accuracy: 0.9924 - val_loss: 0.0174\n",
            "Epoch 8/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0097 - val_accuracy: 0.9962 - val_loss: 0.0119\n",
            "Epoch 9/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9955 - loss: 0.0096 - val_accuracy: 0.9981 - val_loss: 0.0049\n",
            "Epoch 10/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0056 - val_accuracy: 0.9924 - val_loss: 0.0202\n",
            "Epoch 11/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9972 - loss: 0.0146 - val_accuracy: 0.9943 - val_loss: 0.0087\n",
            "Epoch 12/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9962 - val_loss: 0.0072\n",
            "Epoch 13/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
            "Epoch 14/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 15/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9915 - loss: 0.0347 - val_accuracy: 0.9924 - val_loss: 0.0171\n",
            "Epoch 16/16\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9859 - loss: 0.0460 - val_accuracy: 0.9981 - val_loss: 0.0054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network model saved to saved_model/model.h5\n",
            "Scaler saved to saved_model/scaler.pkl\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0114     \n",
            "Test Loss: 0.01557663083076477, Test Accuracy: 0.9938271641731262\n",
            "Random Forest Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nba_api.stats.static import teams\n",
        "from nbaModel import (\n",
        "    load_trained_model,\n",
        ")  # Ensure this is correctly implemented in your nbaModel.py\n",
        "\n",
        "\n",
        "# Get Team ID based on abbreviation\n",
        "def get_team_id_by_abbreviation(team_abbreviation):\n",
        "    \"\"\"Retrieve the team ID by abbreviation.\"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        if team[\"abbreviation\"].lower() == team_abbreviation.lower():\n",
        "            return team[\"id\"]\n",
        "    raise ValueError(\n",
        "        f\"Team '{team_abbreviation}' not found! Please enter a valid abbreviation.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Fetch team-specific features\n",
        "def fetch_team_features(team_abbreviation, features_file):\n",
        "    \"\"\"\n",
        "    Retrieve the team-specific features for the given team abbreviation.\n",
        "    Args:\n",
        "        team_abbreviation: Abbreviation of the NBA team (e.g., 'LAL').\n",
        "        features_file: CSV file containing the aggregated team features.\n",
        "    Returns:\n",
        "        numpy array of the team's features.\n",
        "    \"\"\"\n",
        "    team_features = pd.read_csv(features_file)\n",
        "    team_row = team_features[team_features[\"TEAM\"] == team_abbreviation.upper()]\n",
        "    if team_row.empty:\n",
        "        raise ValueError(\n",
        "            f\"Features for team '{team_abbreviation}' not found in {features_file}.\"\n",
        "        )\n",
        "    return team_row.drop(columns=[\"TEAM\"]).to_numpy().flatten()\n",
        "\n",
        "\n",
        "# Predict win probability\n",
        "def predict_matchup_win_probability(\n",
        "    team1_abbreviation, team2_abbreviation, features_file\n",
        "):\n",
        "    \"\"\"\n",
        "    Predict the win probability of Team 1 beating Team 2.\n",
        "    Args:\n",
        "        team1_abbreviation: Abbreviation of Team 1 (e.g., 'LAL').\n",
        "        team2_abbreviation: Abbreviation of Team 2 (e.g., 'BOS').\n",
        "        features_file: Path to the CSV file containing aggregated team features.\n",
        "    \"\"\"\n",
        "    # Load the trained model and scaler\n",
        "    model, scaler = load_trained_model()\n",
        "\n",
        "    # Fetch features for both teams\n",
        "    team1_features = fetch_team_features(team1_abbreviation, features_file)\n",
        "    team2_features = fetch_team_features(team2_abbreviation, features_file)\n",
        "\n",
        "    # Calculate difference and ratio features\n",
        "    diff_features = team1_features - team2_features\n",
        "    ratio_features = team1_features / (team2_features + 1e-5)\n",
        "\n",
        "    # Combine features for the model\n",
        "    matchup_features = np.concatenate([diff_features, ratio_features]).reshape(1, -1)\n",
        "\n",
        "    # Debug: Check input shape\n",
        "    print(f\"Matchup features shape: {matchup_features.shape}\")\n",
        "    print(f\"Scaler expects: {scaler.n_features_in_}\")\n",
        "\n",
        "    # Ensure consistent feature count\n",
        "    if matchup_features.shape[1] != scaler.n_features_in_:\n",
        "        raise ValueError(\n",
        "            f\"Feature count mismatch. Got {matchup_features.shape[1]} features, \"\n",
        "            f\"but scaler expects {scaler.n_features_in_}. Check feature engineering consistency.\"\n",
        "        )\n",
        "\n",
        "    # Scale the features\n",
        "    scaled_features = scaler.transform(matchup_features)\n",
        "\n",
        "    # Predict win probability for Team 1\n",
        "    win_probability = model.predict(scaled_features)[0][0]\n",
        "    print(\n",
        "        f\"Predicted probability of {team1_abbreviation} beating {team2_abbreviation}: {win_probability * 100:.2f}%\"\n",
        "    )\n",
        "    return win_probability\n",
        "\n",
        "\n",
        "def display_team_data():\n",
        "    \"\"\"Display team names, abbreviations, and IDs in a 2D array.\"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = np.array(\n",
        "        [[team[\"full_name\"], team[\"abbreviation\"], team[\"id\"]] for team in nba_teams]\n",
        "    )\n",
        "    print(\"\\nAvailable Teams:\")\n",
        "    print(pd.DataFrame(team_data, columns=[\"Team Name\", \"Abbreviation\", \"Team ID\"]))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to handle user input and prediction.\"\"\"\n",
        "    features_file = \"data/features.csv\"  # Path to the features file\n",
        "\n",
        "    # Display team data before taking user input\n",
        "    display_team_data()\n",
        "\n",
        "    team1_abbreviation = input(\n",
        "        \"Enter Team 1 abbreviation (e.g., 'LAL' for Los Angeles Lakers): \"\n",
        "    ).strip()\n",
        "    team2_abbreviation = input(\n",
        "        \"Enter Team 2 abbreviation (e.g., 'BOS' for Boston Celtics): \"\n",
        "    ).strip()\n",
        "\n",
        "    try:\n",
        "        predict_matchup_win_probability(\n",
        "            team1_abbreviation, team2_abbreviation, features_file\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "heGJjGPVGuLX",
        "outputId": "70da90c9-81cd-4c29-956a-a476af66a54d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nbaModel'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-11728ef6f810>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnba_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mteams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from nbaModel import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mload_trained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )  # Ensure this is correctly implemented in your nbaModel.py\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nbaModel'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}