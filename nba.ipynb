{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp04TW9eDMCLNbgL1Actjr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-kenny/nbaWinNeuralNetModel/blob/main/nba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nba_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfOs3SYHN8u",
        "outputId": "97da8606-680a-4b74-8645-73ecd02198fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nba_api in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /usr/local/lib/python3.11/dist-packages (from nba_api) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from nba_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from nba_api.stats.endpoints import TeamInfoCommon, TeamGameLogs, PlayerGameLogs, LeagueGameFinder, LeagueLeaders, PlayerCareerStats\n",
        "from nba_api.stats.static import teams\n",
        "\n",
        "# Maximum number of retries for each API call\n",
        "MAX_RETRIES = 3\n",
        "# Define the list of seasons\n",
        "seasons = [\"2024-25\"]"
      ],
      "metadata": {
        "id": "SJ2v4Yzgep72"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_with_retries(func, *args, **kwargs):\n",
        "    \"\"\"Attempts a function call up to MAX_RETRIES with exponential backoff.\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            wait_time = 2**attempt  # Exponential backoff\n",
        "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"Failed after {MAX_RETRIES} attempts.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "DV6BxvlTerjE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_info(seasons):\n",
        "    \"\"\"Fetches relevant team information for the specified seasons.\"\"\"\n",
        "    print(\"Fetching team information...\")\n",
        "    nba_teams = teams.get_teams()\n",
        "    team_data = []\n",
        "\n",
        "    for team in nba_teams:\n",
        "        team_info = fetch_with_retries(\n",
        "            TeamInfoCommon,\n",
        "            team_id=team[\"id\"],\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if team_info:\n",
        "            df_team = team_info.get_data_frames()[0]\n",
        "            df_team = df_team[[\"TEAM_ID\", \"TEAM_ABBREVIATION\"]]  # Only keep relevant features\n",
        "            team_data.append(df_team)\n",
        "            time.sleep(0.6)  # Delay to avoid API rate limits\n",
        "\n",
        "    if team_data:\n",
        "        df_teams = pd.concat(team_data, ignore_index=True)\n",
        "        df_teams.to_csv(\"nba_team_data.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No team data fetched.\")\n",
        "\n",
        "# Run functions to save data to CSV files\n",
        "get_team_info(seasons)\n",
        "print(\"Team information data stored.\")"
      ],
      "metadata": {
        "id": "uflUFIR0et8B",
        "outputId": "b8bf0639-2577-4dce-e195-f230af0b7b03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team information...\n",
            "Team information data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_game_logs(seasons):\n",
        "    \"\"\"Fetches team game logs for the specified seasons and processes the MATCHUP column.\"\"\"\n",
        "    print(\"Fetching team game logs...\")\n",
        "    game_log_data = []\n",
        "\n",
        "    for season in seasons:\n",
        "        game_logs = fetch_with_retries(\n",
        "            TeamGameLogs,\n",
        "            season_nullable=season,\n",
        "            season_type_nullable=\"Regular Season\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        if game_logs:\n",
        "            df_game_logs = game_logs.get_data_frames()[0]\n",
        "            # Keep only relevant columns\n",
        "            df_game_logs = df_game_logs[[\"GAME_ID\", \"GAME_DATE\", \"MATCHUP\", \"WL\"]]\n",
        "            game_log_data.append(df_game_logs)\n",
        "            time.sleep(0.6)  # Delay to respect rate limits\n",
        "\n",
        "    if game_log_data:\n",
        "        # Concatenate all game logs\n",
        "        df_all_game_logs = pd.concat(game_log_data, ignore_index=True)\n",
        "\n",
        "        # Process MATCHUP column to create team1 and team2 columns\n",
        "        matchups_split = df_all_game_logs['MATCHUP'].str.split(' @ | vs. ', expand=True)\n",
        "        df_all_game_logs['TEAM1'] = matchups_split[0]\n",
        "        df_all_game_logs['TEAM2'] = matchups_split[1]\n",
        "\n",
        "        # Drop the original MATCHUP column if no longer needed\n",
        "        df_all_game_logs.drop(columns=['MATCHUP'], inplace=True)\n",
        "\n",
        "        # Extract and add SEASON_YEAR\n",
        "        df_all_game_logs['SEASON_YEAR'] = pd.to_datetime(df_all_game_logs['GAME_DATE']).dt.year.astype(str)\n",
        "\n",
        "        # Save the processed DataFrame to a CSV file\n",
        "        df_all_game_logs.to_csv(\"nba_game_logs.csv\", index=False)\n",
        "        print(\"Processed game logs saved to 'nba_game_logs.csv'.\")\n",
        "    else:\n",
        "        print(\"No game log data fetched.\")\n",
        "\n",
        "get_team_game_logs(seasons)\n",
        "print(\"Team game logs data stored.\")"
      ],
      "metadata": {
        "id": "AQfHSeRyezmh",
        "outputId": "3187222f-1fe9-4c91-f991-b1a9d2df8534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching team game logs...\n",
            "Processed game logs saved to 'nba_game_logs.csv'.\n",
            "Team game logs data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_league_leaders():\n",
        "    \"\"\"Fetches league leaders data with relevant columns for analysis.\"\"\"\n",
        "    print(\"Fetching league leaders data...\")\n",
        "    leaders_data = fetch_with_retries(LeagueLeaders, timeout=60)\n",
        "    if leaders_data:\n",
        "        df_leaders = leaders_data.get_data_frames()[0]\n",
        "        # Select only relevant columns\n",
        "        relevant_columns = [\n",
        "            \"RANK\", \"PLAYER_ID\", \"PLAYER\", \"TEAM_ID\", \"TEAM\", \"GP\", \"MIN\", \"FGM\", \"FGA\",\n",
        "            \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"OREB\",\n",
        "            \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PTS\", \"EFF\"\n",
        "        ]\n",
        "        df_relevant_leaders = df_leaders[relevant_columns]\n",
        "        df_relevant_leaders.to_csv(\"nba_league_leaders_relevant.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No league leaders data fetched.\")\n",
        "\n",
        "get_league_leaders()\n",
        "print(\"League leaders data stored.\")"
      ],
      "metadata": {
        "id": "30UFb9BVZLdW",
        "outputId": "ebd83bf3-03d9-42d6-c2f6-73d9a20f01d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching league leaders data...\n",
            "League leaders data stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load aggregated player data\n",
        "player_aggregated_file = \"nba_league_leaders_relevant.csv\"  # Update with actual file path\n",
        "team_abbreviation_file = \"nba_team_data.csv\"  # File containing TEAM_ID to TEAM_ABBREVIATION mapping\n",
        "\n",
        "# Load player data and team abbreviation mapping\n",
        "player_df = pd.read_csv(player_aggregated_file)\n",
        "team_data_df = pd.read_csv(team_abbreviation_file)\n",
        "\n",
        "# Merge team abbreviations into player data\n",
        "player_df = player_df.merge(team_data_df, on=\"TEAM_ID\", how=\"left\")\n",
        "\n",
        "# Convert numerical columns to float (force errors='coerce' to handle bad data)\n",
        "player_df = player_df.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Ensure 'MIN' is numeric and replace any missing values with 1 to avoid division errors\n",
        "player_df['MIN'] = pd.to_numeric(player_df['MIN'], errors='coerce').fillna(1)\n",
        "\n",
        "# Define non-numerical columns to exclude\n",
        "no_aggregate_columns = ['RANK', 'PLAYER_ID', 'PLAYER', 'TEAM_ID', 'TEAM', 'SEASON_YEAR', 'MIN']\n",
        "numerical_columns = [col for col in player_df.columns if col not in no_aggregate_columns]\n",
        "\n",
        "# Multiply each player's stats by their 'MIN' to weight the statistics\n",
        "for col in numerical_columns:\n",
        "    player_df[f\"{col}_WEIGHTED\"] = pd.to_numeric(player_df[col], errors='coerce').fillna(0) * player_df['MIN']\n",
        "\n",
        "# Group by TEAM and sum the weighted stats & total minutes\n",
        "grouped = player_df.groupby('TEAM')\n",
        "\n",
        "team_aggregated_data = grouped[[f\"{col}_WEIGHTED\" for col in numerical_columns]].sum()\n",
        "\n",
        "# Compute total minutes played by the team\n",
        "team_aggregated_data['TOTAL_MIN'] = grouped['MIN'].sum()\n",
        "\n",
        "# Normalize weighted stats by dividing by TOTAL_MIN (avoid division by zero)\n",
        "for col in numerical_columns:\n",
        "    team_aggregated_data[col] = team_aggregated_data[f\"{col}_WEIGHTED\"] / team_aggregated_data['TOTAL_MIN'].replace(0, 1)\n",
        "\n",
        "# Drop weighted stat columns (no longer needed)\n",
        "team_aggregated_data.drop(columns=[f\"{col}_WEIGHTED\" for col in numerical_columns], inplace=True)\n",
        "\n",
        "# Add total games played per team (if column exists)\n",
        "if 'GAMES_PLAYED' in player_df.columns:\n",
        "    team_aggregated_data['TEAM_GAMES_PLAYED'] = grouped['GAMES_PLAYED'].sum()\n",
        "\n",
        "# Reset index to flatten the DataFrame\n",
        "team_aggregated_data.reset_index(inplace=True)\n",
        "\n",
        "# Save the aggregated data\n",
        "output_path = \"nba_team_aggregated_data.csv\"\n",
        "team_aggregated_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Team aggregated data saved to '{output_path}'.\")\n"
      ],
      "metadata": {
        "id": "nemrfltYe1I-",
        "outputId": "c413fb52-6a5d-462f-984f-ae1753f16644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team aggregated data saved to 'nba_team_aggregated_data.csv'.\n",
            "  TEAM  TOTAL_MIN         GP         FGM         FGA    FG_PCT        FG3M  \\\n",
            "0  ATL      13565  40.193660  185.250498  400.713011  0.466796   55.378769   \n",
            "1  BKN      11390  36.854609  143.696488  329.696576  0.440982   51.908780   \n",
            "2  BOS      12465  43.771360  233.465142  510.812435  0.464372  100.862174   \n",
            "3  CHA      11345  32.745615  127.698017  296.251388  0.436675   51.794006   \n",
            "4  CHI      12847  43.279521  180.176539  387.355180  0.453470   63.430451   \n",
            "\n",
            "         FG3A   FG3_PCT        FTM  ...        DREB         REB         AST  \\\n",
            "0  158.818282  0.321223  79.232584  ...  142.496351  194.230225  149.787615   \n",
            "1  151.999298  0.283867  63.801756  ...  119.290606  156.827305   95.399385   \n",
            "2  273.385560  0.322537  93.864180  ...  175.957240  225.476615  149.999438   \n",
            "3  149.697929  0.325982  49.754782  ...  104.434817  138.826972   69.495549   \n",
            "4  177.487273  0.351750  58.167043  ...  160.409512  204.802444  132.919514   \n",
            "\n",
            "         STL        BLK        TOV         PF         PTS         EFF  \\\n",
            "0  46.100184  21.682271  72.522374  80.893181  505.112348  604.506082   \n",
            "1  27.981036  15.833011  52.578227  77.242845  403.103512  444.107287   \n",
            "2  38.403129  26.329001  65.238989  80.429122  661.656639  735.229282   \n",
            "3  26.172763  11.820714  44.134597  63.485853  356.944822  377.191803   \n",
            "4  32.728653  20.386783  60.155056  77.197322  481.950572  591.637581   \n",
            "\n",
            "   TEAM_ABBREVIATION  \n",
            "0                0.0  \n",
            "1                0.0  \n",
            "2                0.0  \n",
            "3                0.0  \n",
            "4                0.0  \n",
            "\n",
            "[5 rows x 23 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-b5539238e2bd>:15: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  player_df = player_df.apply(pd.to_numeric, errors='ignore')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_season_year(game_date):\n",
        "    \"\"\"\n",
        "    Extracts the year from a game date in the format 'YYYY-MM-DDTHH:MM:SS'.\n",
        "\n",
        "    Args:\n",
        "        game_date (str): The game date string.\n",
        "\n",
        "    Returns:\n",
        "        str: The year as a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pd.to_datetime(game_date).year\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date '{game_date}': {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "sKpwA7ybWy3E"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "import joblib\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom accuracy metric to evaluate the model based on given conditions.\n",
        "    \"\"\"\n",
        "    condition_1 = K.cast(y_pred < 0.5, dtype=\"float32\") * K.cast(y_true == 0, dtype=\"float32\")\n",
        "    condition_2 = K.cast(y_pred >= 0.5, dtype=\"float32\") * K.cast(y_true == 1, dtype=\"float32\")\n",
        "    return K.mean(condition_1 + condition_2)\n",
        "\n",
        "\n",
        "def prepare_dataset(game_logs_file, features_file):\n",
        "    \"\"\"\n",
        "    Prepare dataset for training using team-specific features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load game logs and team features\n",
        "        game_logs = pd.read_csv(game_logs_file, parse_dates=[\"GAME_DATE\"])\n",
        "        team_features = pd.read_csv(features_file)\n",
        "\n",
        "        # Normalize key columns\n",
        "        game_logs[\"TEAM1\"] = game_logs[\"TEAM1\"].str.strip().str.upper()\n",
        "        game_logs[\"TEAM2\"] = game_logs[\"TEAM2\"].str.strip().str.upper()\n",
        "        team_features[\"TEAM\"] = team_features[\"TEAM\"].str.strip().str.upper()\n",
        "\n",
        "        # Merge features for TEAM1 and TEAM2\n",
        "        game_logs = game_logs.merge(\n",
        "            team_features.add_suffix(\"_TEAM1\"),\n",
        "            left_on=\"TEAM1\",\n",
        "            right_on=\"TEAM_TEAM1\",\n",
        "            how=\"left\"\n",
        "        ).merge(\n",
        "            team_features.add_suffix(\"_TEAM2\"),\n",
        "            left_on=\"TEAM2\",\n",
        "            right_on=\"TEAM_TEAM2\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = [\"TEAM_TEAM1\", \"TEAM_TEAM2\", \"GAME_DATE\"]\n",
        "        game_logs.drop(columns=[col for col in columns_to_drop if col in game_logs.columns], inplace=True)\n",
        "\n",
        "        # Handle missing values\n",
        "        game_logs.fillna(0, inplace=True)\n",
        "\n",
        "        # Extract features and target\n",
        "        feature_columns = game_logs.select_dtypes(include=np.number).columns.difference([\"WL\"])\n",
        "        X = game_logs[feature_columns].to_numpy()\n",
        "        y = (game_logs[\"WL\"] == \"W\").astype(int).to_numpy()\n",
        "\n",
        "        # Debugging and validation\n",
        "        print(\"Features Shape:\", X.shape)\n",
        "        assert X.shape[0] == y.shape[0], \"Mismatch between features and target sizes.\"\n",
        "        assert not np.any(pd.isnull(X)), \"Missing values found in features.\"\n",
        "\n",
        "        return X, y, feature_columns\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in prepare_dataset:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "def build_neural_network(input_shape):\n",
        "    \"\"\"\n",
        "    Build a neural network model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(256, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[custom_accuracy])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(X, y, save_data_prefix=\"train_data\"):\n",
        "    \"\"\"\n",
        "    Train a neural network model and save the processed data.\n",
        "    \"\"\"\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Validate scaling\n",
        "    assert not np.any(np.isnan(X_scaled)), \"NaN values found after scaling.\"\n",
        "\n",
        "    # Handle class imbalance\n",
        "    X_resampled, y_resampled = SMOTE().fit_resample(X_scaled, y)\n",
        "\n",
        "    # Save processed training data\n",
        "    pd.DataFrame(X_resampled).to_csv(f\"{save_data_prefix}_X.csv\", index=False, header=False)\n",
        "    pd.DataFrame(y_resampled).to_csv(f\"{save_data_prefix}_y.csv\", index=False, header=False)\n",
        "\n",
        "    # Debugging output\n",
        "    print(f\"Processed training data saved to {save_data_prefix}_X.csv and {save_data_prefix}_y.csv.\")\n",
        "    print(f\"Resampled Features Shape: {X_resampled.shape}, Resampled Target Shape: {y_resampled.shape}\")\n",
        "\n",
        "    # Build and train the model\n",
        "    model = build_neural_network(X_resampled.shape[1])\n",
        "    model.fit(X_resampled, y_resampled, epochs=1024, batch_size=1, validation_split=0.2)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "def main():\n",
        "    game_logs_file = \"nba_game_logs.csv\"\n",
        "    features_file = \"nba_team_aggregated_data.csv\"\n",
        "\n",
        "    # Prepare the dataset\n",
        "    X, y, feature_columns = prepare_dataset(game_logs_file, features_file)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # Validate split\n",
        "    assert X_train.shape[0] == y_train.shape[0], \"Mismatch in training data sizes.\"\n",
        "    assert X_test.shape[0] == y_test.shape[0], \"Mismatch in test data sizes.\"\n",
        "\n",
        "    # Train neural network\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Save the model and scaler\n",
        "    model.save(\"model.keras\")\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    test_loss, test_custom_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "    print(f\"Test Loss: {test_loss}, Test Custom Accuracy: {test_custom_accuracy}\")\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred.flatten()}).to_csv(\"predictions.csv\", index=False)\n",
        "    print(\"Predictions saved to predictions.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_cuhZRejGr7s",
        "outputId": "6271ae61-15a7-4d85-d8c1-4005f5afe629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features Shape: (1518, 46)\n",
            "Sample Features:\n",
            "     AST_TEAM1   AST_TEAM2  BLK_TEAM1  BLK_TEAM2  DREB_TEAM1  DREB_TEAM2  \\\n",
            "0  122.357556  142.972402  24.168861  31.540799  165.477598  201.083832   \n",
            "1  149.990104  119.067859  14.942702  18.326316  140.009203  130.714359   \n",
            "2  105.393782  149.999438  32.745596  26.329001  161.319615  175.957240   \n",
            "3  145.536784  117.869221  25.227535  20.287572  158.583937  160.331531   \n",
            "4   94.105082  181.381717  23.967896  25.013990  118.038838  189.441812   \n",
            "\n",
            "    EFF_TEAM1   EFF_TEAM2  FG3A_TEAM1  FG3A_TEAM2  ...   REB_TEAM2  \\\n",
            "0  654.936888  773.220308  185.085033  225.491346  ...  267.904363   \n",
            "1  604.447204  525.334900  161.457694  168.998263  ...  177.798263   \n",
            "2  652.992820  735.229282  147.147002  273.385560  ...  225.476615   \n",
            "3  684.130267  612.540433  174.761685  156.127566  ...  208.597555   \n",
            "4  463.006186  857.705803  139.954271  176.569475  ...  249.646184   \n",
            "\n",
            "   SEASON_YEAR  STL_TEAM1  STL_TEAM2  TEAM_ABBREVIATION_TEAM1  \\\n",
            "0       2025.0  44.442990  44.972242                      0.0   \n",
            "1       2025.0  30.968629  31.960245                      0.0   \n",
            "2       2025.0  36.975278  38.403129                      0.0   \n",
            "3       2025.0  38.734282  41.048974                      0.0   \n",
            "4       2025.0  37.252134  47.727027                      0.0   \n",
            "\n",
            "   TEAM_ABBREVIATION_TEAM2  TOTAL_MIN_TEAM1  TOTAL_MIN_TEAM2  TOV_TEAM1  \\\n",
            "0                      0.0          12454.0          12537.0  67.566886   \n",
            "1                      0.0          10105.0           9785.0  65.729342   \n",
            "2                      0.0          13510.0          12465.0  60.276758   \n",
            "3                      0.0          12152.0          12762.0  61.264154   \n",
            "4                      0.0          12771.0          12580.0  53.536841   \n",
            "\n",
            "   TOV_TEAM2  \n",
            "0  85.206190  \n",
            "1  55.065713  \n",
            "2  65.238989  \n",
            "3  72.233898  \n",
            "4  75.503736  \n",
            "\n",
            "[5 rows x 46 columns]\n",
            "Processed training data saved to train_data_X.csv and train_data_y.csv.\n",
            "Resampled Features Shape: (1250, 46), Resampled Target Shape: (1250,)\n",
            "Epoch 1/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - custom_accuracy: 0.5638 - loss: 0.7101 - val_custom_accuracy: 0.6480 - val_loss: 0.6391\n",
            "Epoch 2/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.5874 - loss: 0.6845 - val_custom_accuracy: 0.6560 - val_loss: 0.6067\n",
            "Epoch 3/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - custom_accuracy: 0.6360 - loss: 0.6339 - val_custom_accuracy: 0.6280 - val_loss: 0.6265\n",
            "Epoch 4/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - custom_accuracy: 0.6414 - loss: 0.6345 - val_custom_accuracy: 0.6240 - val_loss: 0.6272\n",
            "Epoch 5/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - custom_accuracy: 0.6534 - loss: 0.6284 - val_custom_accuracy: 0.6720 - val_loss: 0.6283\n",
            "Epoch 6/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - custom_accuracy: 0.6476 - loss: 0.6410 - val_custom_accuracy: 0.6720 - val_loss: 0.6516\n",
            "Epoch 7/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - custom_accuracy: 0.6319 - loss: 0.6243 - val_custom_accuracy: 0.6240 - val_loss: 0.6086\n",
            "Epoch 8/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - custom_accuracy: 0.6349 - loss: 0.6127 - val_custom_accuracy: 0.6560 - val_loss: 0.6229\n",
            "Epoch 9/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.6730 - loss: 0.5858 - val_custom_accuracy: 0.6480 - val_loss: 0.5992\n",
            "Epoch 10/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - custom_accuracy: 0.6791 - loss: 0.5866 - val_custom_accuracy: 0.6960 - val_loss: 0.6115\n",
            "Epoch 11/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - custom_accuracy: 0.6879 - loss: 0.5806 - val_custom_accuracy: 0.6200 - val_loss: 0.6295\n",
            "Epoch 12/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.6687 - loss: 0.5860 - val_custom_accuracy: 0.6400 - val_loss: 0.6303\n",
            "Epoch 13/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - custom_accuracy: 0.6978 - loss: 0.5937 - val_custom_accuracy: 0.6240 - val_loss: 0.6435\n",
            "Epoch 14/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.6962 - loss: 0.5775 - val_custom_accuracy: 0.6480 - val_loss: 0.6279\n",
            "Epoch 15/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - custom_accuracy: 0.6791 - loss: 0.5752 - val_custom_accuracy: 0.6720 - val_loss: 0.6182\n",
            "Epoch 16/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - custom_accuracy: 0.6919 - loss: 0.5738 - val_custom_accuracy: 0.6720 - val_loss: 0.6442\n",
            "Epoch 17/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - custom_accuracy: 0.7108 - loss: 0.5442 - val_custom_accuracy: 0.6560 - val_loss: 0.6463\n",
            "Epoch 18/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.6981 - loss: 0.5479 - val_custom_accuracy: 0.6560 - val_loss: 0.6771\n",
            "Epoch 19/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - custom_accuracy: 0.7058 - loss: 0.5287 - val_custom_accuracy: 0.6280 - val_loss: 0.7168\n",
            "Epoch 20/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - custom_accuracy: 0.7095 - loss: 0.5458 - val_custom_accuracy: 0.6400 - val_loss: 0.7128\n",
            "Epoch 21/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.6885 - loss: 0.5606 - val_custom_accuracy: 0.6440 - val_loss: 0.6890\n",
            "Epoch 22/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - custom_accuracy: 0.7101 - loss: 0.5376 - val_custom_accuracy: 0.6480 - val_loss: 0.7513\n",
            "Epoch 23/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - custom_accuracy: 0.6902 - loss: 0.5491 - val_custom_accuracy: 0.6560 - val_loss: 0.7167\n",
            "Epoch 24/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - custom_accuracy: 0.7307 - loss: 0.5364 - val_custom_accuracy: 0.6800 - val_loss: 0.7041\n",
            "Epoch 25/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - custom_accuracy: 0.7448 - loss: 0.4864 - val_custom_accuracy: 0.6600 - val_loss: 0.6558\n",
            "Epoch 26/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - custom_accuracy: 0.7077 - loss: 0.5218 - val_custom_accuracy: 0.6600 - val_loss: 0.6876\n",
            "Epoch 27/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7060 - loss: 0.5135 - val_custom_accuracy: 0.6800 - val_loss: 0.7631\n",
            "Epoch 28/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - custom_accuracy: 0.7398 - loss: 0.4890 - val_custom_accuracy: 0.6880 - val_loss: 0.7857\n",
            "Epoch 29/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - custom_accuracy: 0.6852 - loss: 0.5142 - val_custom_accuracy: 0.6560 - val_loss: 0.9347\n",
            "Epoch 30/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - custom_accuracy: 0.7093 - loss: 0.5377 - val_custom_accuracy: 0.6760 - val_loss: 0.9606\n",
            "Epoch 31/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - custom_accuracy: 0.7208 - loss: 0.4878 - val_custom_accuracy: 0.6680 - val_loss: 0.7442\n",
            "Epoch 32/1024\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - custom_accuracy: 0.7287 - loss: 0.5016 - val_custom_accuracy: 0.6760 - val_loss: 0.8580\n",
            "Epoch 33/1024\n",
            "\u001b[1m 981/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - custom_accuracy: 0.6918 - loss: 0.5006"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nba_api.stats.static import teams\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "def load_feature_names(feature_names_file=\"feature_names.pkl\"):\n",
        "    \"\"\"\n",
        "    Load saved feature names for feature alignment during prediction.\n",
        "    \"\"\"\n",
        "    feature_names = joblib.load(feature_names_file)\n",
        "\n",
        "    # Print feature names\n",
        "    print(\"Feature Names Used in the Model:\")\n",
        "    print(feature_names)\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "\n",
        "def get_team_id_by_abbreviation(team_abbreviation):\n",
        "    \"\"\"Retrieve the team ID by abbreviation.\"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    for team in nba_teams:\n",
        "        if team[\"abbreviation\"].lower() == team_abbreviation.lower():\n",
        "            return team[\"id\"]\n",
        "    raise ValueError(f\"Team '{team_abbreviation}' not found! Please enter a valid abbreviation.\")\n",
        "\n",
        "\n",
        "def fetch_team_features(team_abbreviation, features_file, feature_names):\n",
        "    \"\"\"\n",
        "    Retrieve the team-specific features for the given team abbreviation.\n",
        "\n",
        "    Args:\n",
        "        team_abbreviation: Abbreviation of the NBA team (e.g., 'LAL').\n",
        "        features_file: CSV file containing the aggregated team features.\n",
        "        feature_names: List of features expected by the model.\n",
        "\n",
        "    Returns:\n",
        "        numpy array of the team's features.\n",
        "    \"\"\"\n",
        "    team_features = pd.read_csv(features_file)\n",
        "    team_row = team_features[team_features[\"TEAM\"] == team_abbreviation.upper()]\n",
        "\n",
        "    if team_row.empty:\n",
        "        raise ValueError(f\"Features for team '{team_abbreviation}' not found in {features_file}.\")\n",
        "\n",
        "    # Ensure only numeric features are returned\n",
        "    numeric_features = team_row[feature_names].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    return numeric_features.to_numpy().flatten()\n",
        "\n",
        "\n",
        "def predict_matchup_win_probability(team1_abbreviation, team2_abbreviation, features_file, model_path=\"model.h5\", scaler_path=\"scaler.pkl\"):\n",
        "    \"\"\"\n",
        "    Predict the win probability for Team 1 in a matchup against Team 2.\n",
        "\n",
        "    Args:\n",
        "        team1_abbreviation: Abbreviation of Team 1 (e.g., 'LAL').\n",
        "        team2_abbreviation: Abbreviation of Team 2 (e.g., 'BOS').\n",
        "        features_file: CSV file containing aggregated team features.\n",
        "        model_path: Path to the trained neural network model file.\n",
        "        scaler_path: Path to the scaler file for feature normalization.\n",
        "    \"\"\"\n",
        "    # Load model, scaler, and feature names\n",
        "    model = load_model(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    feature_names = load_feature_names()\n",
        "\n",
        "    # Fetch features for both teams\n",
        "    team1_features = fetch_team_features(team1_abbreviation, features_file, feature_names)\n",
        "    team2_features = fetch_team_features(team2_abbreviation, features_file, feature_names)\n",
        "\n",
        "    # Create matchup feature differences and ratios\n",
        "    matchup_features = np.concatenate([\n",
        "        team1_features - team2_features,\n",
        "        team1_features / (team2_features + 1e-5)  # Avoid division by zero\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Scale the matchup features\n",
        "    matchup_features_scaled = scaler.transform(matchup_features)\n",
        "\n",
        "    # Predict win probability for Team 1\n",
        "    win_probability = model.predict(matchup_features_scaled)[0][0]\n",
        "\n",
        "    print(f\"\\nWin Probability for {team1_abbreviation} vs {team2_abbreviation}: {win_probability * 100:.2f}%\")\n",
        "\n",
        "\n",
        "def display_team_data():\n",
        "    \"\"\"\n",
        "    Display available team abbreviations and names for user reference.\n",
        "    \"\"\"\n",
        "    nba_teams = teams.get_teams()\n",
        "    print(\"Available NBA Teams:\")\n",
        "    for team in nba_teams:\n",
        "        print(f\"{team['abbreviation']} - {team['full_name']}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to handle user input and prediction.\"\"\"\n",
        "    features_file = \"features.csv\"  # Path to the features file\n",
        "\n",
        "    # Display team data before taking user input\n",
        "    display_team_data()\n",
        "\n",
        "    team1_abbreviation = input(\"Enter Team 1 abbreviation (e.g., 'LAL' for Los Angeles Lakers): \").strip()\n",
        "    team2_abbreviation = input(\"Enter Team 2 abbreviation (e.g., 'BOS' for Boston Celtics): \").strip()\n",
        "\n",
        "    try:\n",
        "        predict_matchup_win_probability(team1_abbreviation, team2_abbreviation, features_file)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_ilaXJR5Ud-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}